_entity1_ _C_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ _P_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity4 entity1
This paper proposes a _entity1_ Hidden Markov Model ( HMM ) _/entity1_ and an _entity2_ HMM-based chunk tagger _/entity2_ , from which a _entity3_ named entity ( NE ) recognition ( NER ) system _/entity3_ is built to recognize and classify _entity4_ names _/entity4_ , _entity5_ times and numerical quantities _/entity5_ . Through the _entity6_ HMM _/entity6_ , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the _entity7_ words _/entity7_ , such as _entity8_ capitalization _/entity8_ and digitalization ; 2 ) _entity9_ internal semantic feature _/entity9_ of important triggers ; 3 ) _entity10_ internal gazetteer feature _/entity10_ ; 4 ) _entity11_ external macro context feature _/entity11_ . In this way , the _entity12_ _P_ NER problem _/entity12_ can be resolved effectively . Evaluation of our _entity13_ _C_ system _/entity13_ on _entity14_ MUC-6 and MUC-7 English NE tasks _/entity14_ achieves _entity15_ F-measures _/entity15_ of 96.6 % and 94.1 % respectively . It shows that the performance is significantly better than reported by any other _entity16_ machine-learning system _/entity16_ . Moreover , the _entity17_ performance _/entity17_ is even consistently better than those based on _entity18_ handcrafted rules _/entity18_ .	NONE entity12 entity13
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ _P_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ _C_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity14 entity16
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ feature structure-directed generation _/entity2_ , developed for a _entity3_ _P_ dialogue translation system _/entity3_ . The system utilizes _entity4_ _C_ typed feature structures _/entity4_ to control the _entity5_ top-down derivation _/entity5_ in a declarative way . This _entity6_ generation system _/entity6_ also uses _entity7_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ derivation tree _/entity8_ . The _entity9_ grammar _/entity9_ for this _entity10_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	NONE entity3 entity4
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ _C_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ _P_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity12 entity9
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ _P_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ _C_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity12 entity13
We present a _entity1_ syntax-based constraint _/entity1_ for _entity2_ _P_ word alignment _/entity2_ , known as the _entity3_ cohesion constraint _/entity3_ . It requires disjoint _entity4_ English phrases _/entity4_ to be mapped to non-overlapping intervals in the _entity5_ _C_ French sentence _/entity5_ . We evaluate the utility of this _entity6_ constraint _/entity6_ in two different algorithms . The results show that it can provide a significant improvement in _entity7_ alignment quality _/entity7_ .	NONE entity2 entity5
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ _C_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ _P_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity9 entity7
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ _P_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ _C_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity5 entity7
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ _C_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ _P_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ high-quality data _/entity7_ that allow the comparison of both _entity8_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ automatic metrics _/entity11_ used within _entity12_ MT _/entity12_ are also discussed in this regard .	NONE entity5 entity3
It is often assumed that when _entity1_ natural language processing _/entity1_ meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned . However , our experience with _entity2_ TACITUS _/entity2_ ; especially in the _entity3_ MUC-3 evaluation _/entity3_ , has shown that principled techniques for _entity4_ syntactic and pragmatic analysis _/entity4_ can be bolstered with methods for achieving robustness . We describe three techniques for making _entity5_ syntactic analysis _/entity5_ more robust -- -an _entity6_ agenda-based scheduling parser _/entity6_ , a _entity7_ _P_ recovery technique for failed parses _/entity7_ , and a new technique called _entity8_ _C_ terminal substring parsing _/entity8_ . For _entity9_ pragmatics processing _/entity9_ , we describe how the method of _entity10_ abductive inference _/entity10_ is inherently robust , in that an interpretation is always possible , so that in the absence of the required _entity11_ world knowledge _/entity11_ , performance degrades gracefully . Each of these techniques have been evaluated and the results of the evaluations are presented .	NONE entity7 entity8
This paper presents a _entity1_ machine learning approach _/entity1_ to _entity2_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ corpus-based sample _/entity5_ and formulate them as _entity6_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ clauses _/entity7_ to create a set of _entity8_ domain independent features _/entity8_ to annotate an _entity9_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ _C_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ _P_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity13 entity12
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ _P_ adverbial constructs _/entity20_ . The _entity21_ _C_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity20 entity21
Multimodal interfaces require effective _entity1_ parsing _/entity1_ and understanding of _entity2_ _C_ utterances _/entity2_ whose content is distributed across multiple input modes . Johnston 1998 presents an approach in which strategies for _entity3_ _P_ multimodal integration _/entity3_ are stated declaratively using a _entity4_ unification-based grammar _/entity4_ that is used by a _entity5_ multidimensional chart parser _/entity5_ to compose inputs . This approach is highly expressive and supports a broad class of _entity6_ interfaces _/entity6_ , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input . In this paper , we present an alternative approach in which _entity7_ multimodal parsing and understanding _/entity7_ are achieved using a _entity8_ weighted finite-state device _/entity8_ which takes _entity9_ speech and gesture streams _/entity9_ as inputs and outputs their joint interpretation . This approach is significantly more efficient , enables tight-coupling of multimodal understanding with _entity10_ speech recognition _/entity10_ , and provides a general probabilistic framework for _entity11_ multimodal ambiguity resolution _/entity11_ .	NONE entity3 entity2
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ _C_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ _P_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity7 entity5
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ _C_ constituent structure trees _/entity13_ are used to guide _entity14_ _P_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity14 entity13
This paper describes a method of _entity1_ interactively visualizing and directing the process _/entity1_ of _entity2_ translating a sentence _/entity2_ . The method allows a _entity3_ user _/entity3_ to explore a _entity4_ model _/entity4_ of _entity5_ syntax-based statistical machine translation ( MT ) _/entity5_ , to understand the _entity6_ model _/entity6_ 's strengths and weaknesses , and to compare it to other _entity7_ MT systems _/entity7_ . Using this _entity8_ visualization method _/entity8_ , we can find and address conceptual and practical problems in an _entity9_ _C_ MT system _/entity9_ . In our demonstration at _entity10_ ACL _/entity10_ , new _entity11_ users _/entity11_ of our tool will drive a _entity12_ _P_ syntax-based decoder _/entity12_ for themselves .	NONE entity12 entity9
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ _C_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ _P_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity14 entity12
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ _C_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ _P_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity5 entity2
This paper presents a _entity1_ machine learning approach _/entity1_ to _entity2_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ corpus-based sample _/entity5_ and formulate them as _entity6_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ clauses _/entity7_ to create a set of _entity8_ _P_ domain independent features _/entity8_ to annotate an _entity9_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ _C_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity8 entity11
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ _C_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ _P_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity9 entity7
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ _C_ genre _/entity13_ . Examples and results will be given for _entity14_ _P_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity14 entity13
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ _C_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ _P_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity6 entity4
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ _C_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ _P_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity21 entity20
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ _C_ French _/entity3_ as our _entity4_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ _P_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity6 entity3
This paper presents necessary and sufficient conditions for the use of _entity1_ demonstrative expressions _/entity1_ in _entity2_ English _/entity2_ and discusses implications for current _entity3_ discourse processing algorithms _/entity3_ . We examine a broad range of _entity4_ texts _/entity4_ to show how the distribution of _entity5_ _C_ demonstrative forms and functions _/entity5_ is _entity6_ _P_ genre dependent _/entity6_ . This research is part of a larger study of _entity7_ anaphoric expressions _/entity7_ , the results of which will be incorporated into a _entity8_ natural language generation system _/entity8_ .	NONE entity6 entity5
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ _C_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ _P_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity11 entity9
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ _P_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ _C_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity7 entity10
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ context-free parsing _/entity2_ is presented . The _entity3_ parser _/entity3_ computes a compact _entity4_ parse forest representation _/entity4_ of the complete set of possible _entity5_ analyses for large treebank grammars _/entity5_ and long _entity6_ input sentences _/entity6_ . The _entity7_ parser _/entity7_ uses _entity8_ _C_ bit-vector operations _/entity8_ to parallelise the _entity9_ _P_ basic parsing operations _/entity9_ . The _entity10_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	NONE entity9 entity8
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ language independent modules _/entity3_ of the _entity4_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ _P_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ _C_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity6 entity8
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ question phrases _/entity5_ in _entity6_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ _C_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ _P_ MRR _/entity14_ .	NONE entity14 entity13
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ _C_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ _P_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity9 entity7
This paper proposes a generic _entity1_ mathematical formalism _/entity1_ for the combination of various _entity2_ structures _/entity2_ : _entity3_ strings _/entity3_ , _entity4_ trees _/entity4_ , _entity5_ dags _/entity5_ , _entity6_ graphs _/entity6_ , and products of them . The _entity7_ polarization _/entity7_ of the objects of the _entity8_ elementary structures _/entity8_ controls the _entity9_ saturation _/entity9_ of the final _entity10_ structure _/entity10_ . This formalism is both elementary and powerful enough to strongly simulate many _entity11_ _C_ grammar formalisms _/entity11_ , such as _entity12_ _P_ rewriting systems _/entity12_ , _entity13_ dependency grammars _/entity13_ , _entity14_ TAG _/entity14_ , _entity15_ HPSG _/entity15_ and _entity16_ LFG _/entity16_ .	NONE entity12 entity11
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ _P_ stories _/entity8_ , many _entity9_ _C_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity8 entity9
An attempt has been made to use an _entity1_ Augmented Transition Network _/entity1_ as a procedural _entity2_ dialog model _/entity2_ . The development of such a _entity3_ model _/entity3_ appears to be important in several respects : as a device to represent and to use different _entity4_ dialog schemata _/entity4_ proposed in empirical _entity5_ conversation analysis _/entity5_ ; as a device to represent and to use _entity6_ models of verbal interaction _/entity6_ ; as a device combining knowledge about _entity7_ dialog schemata _/entity7_ and about _entity8_ verbal interaction _/entity8_ with knowledge about _entity9_ task-oriented and goal-directed dialogs _/entity9_ . A standard _entity10_ ATN _/entity10_ should be further developed in order to account for the _entity11_ _P_ verbal interactions _/entity11_ of _entity12_ _C_ task-oriented dialogs _/entity12_ .	PART_WHOLE entity11 entity12
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ _P_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ _C_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity17 entity19
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ _P_ broad semantic classes _/entity5_ and _entity6_ _C_ LCS meaning components _/entity6_ . Our _entity7_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity5 entity6
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ _C_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ _P_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity19 entity18
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ _P_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ _C_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity11 entity13
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ traditional MT architectural designs _/entity4_ . A new _entity5_ semantic representation _/entity5_ is proposed that uses _entity6_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ _P_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ interlingua _/entity9_ within a new _entity10_ _C_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ direct approaches _/entity12_ into a single system .	NONE entity8 entity10
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ _P_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ _C_ language independent modules _/entity3_ of the _entity4_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity2 entity3
_entity1_ Pipelined Natural Language Generation ( NLG ) systems _/entity1_ have grown increasingly complex as _entity2_ architectural modules _/entity2_ were added to support _entity3_ language functionalities _/entity3_ such as _entity4_ referring expressions _/entity4_ , _entity5_ lexical choice _/entity5_ , and _entity6_ _P_ revision _/entity6_ . This has given rise to discussions about the relative placement of these new _entity7_ _C_ modules _/entity7_ in the overall _entity8_ architecture _/entity8_ . Recent work on another aspect of _entity9_ multi-paragraph text _/entity9_ , _entity10_ discourse markers _/entity10_ , indicates it is time to consider where a _entity11_ discourse marker insertion algorithm _/entity11_ fits in . We present examples which suggest that in a _entity12_ pipelined NLG architecture _/entity12_ , the best approach is to strongly tie it to a _entity13_ revision component _/entity13_ . Finally , we evaluate the approach in a working _entity14_ multi-page system _/entity14_ .	NONE entity6 entity7
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ _P_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ _C_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity11 entity14
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ _P_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ _C_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity6 entity8
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ _C_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ _P_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity20 entity17
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ _C_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ _P_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity13 entity10
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ morphological and syntactic information _/entity3_ . While the _entity4_ model _/entity4_ is more complex than those which have been employed for _entity5_ unsupervised learning _/entity5_ of _entity6_ POS tags in English _/entity6_ , which use only _entity7_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ _C_ morphology _/entity9_ as well . In many _entity10_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ _P_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ Bulgarian _/entity15_ , a _entity16_ Slavic language _/entity16_ with relatively _entity17_ free word order _/entity17_ and _entity18_ rich morphology _/entity18_ .	NONE entity12 entity9
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ _C_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ _P_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity5 entity2
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ _C_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ _P_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity11 entity8
In this paper , we describe a _entity1_ _P_ phrase-based unigram model _/entity1_ for _entity2_ _C_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	USAGE entity1 entity2
This paper introduces a method for _entity1_ computational analysis of move structures _/entity1_ in _entity2_ abstracts _/entity2_ of _entity3_ research articles _/entity3_ . In our approach , _entity4_ sentences _/entity4_ in a given _entity5_ abstract _/entity5_ are analyzed and labeled with a specific _entity6_ _P_ move _/entity6_ in light of various _entity7_ rhetorical functions _/entity7_ . The method involves automatically gathering a large number of _entity8_ _C_ abstracts _/entity8_ from the _entity9_ Web _/entity9_ and building a _entity10_ language model _/entity10_ of _entity11_ abstract moves _/entity11_ . We also present a prototype _entity12_ concordancer _/entity12_ , _entity13_ CARE _/entity13_ , which exploits the _entity14_ move-tagged abstracts _/entity14_ for _entity15_ digital learning _/entity15_ . This system provides a promising approach to _entity16_ Web-based computer-assisted academic writing _/entity16_ .	NONE entity6 entity8
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ _C_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ _P_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity7 entity4
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ _C_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ _P_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity10 entity9
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ _C_ Korean-English translation _/entity16_ . Through experiments with _entity17_ _P_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity17 entity16
_entity1_ _C_ Determiners _/entity1_ play an important role in conveying the _entity2_ meaning _/entity2_ of an _entity3_ _P_ utterance _/entity3_ , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the _entity4_ global meaning _/entity4_ of a _entity5_ sentence _/entity5_ , even if not in a precise way . Another problem with _entity6_ determiners _/entity6_ is their inherent _entity7_ ambiguity _/entity7_ . In this paper we propose a _entity8_ logical formalism _/entity8_ , which , among other things , is suitable for representing _entity9_ determiners _/entity9_ without forcing a particular _entity10_ interpretation _/entity10_ when their _entity11_ meaning _/entity11_ is still not clear .	NONE entity3 entity1
This paper presents a novel _entity1_ ensemble learning approach _/entity1_ to resolving _entity2_ German pronouns _/entity2_ . _entity3_ Boosting _/entity3_ , the method in question , combines the moderately accurate _entity4_ hypotheses _/entity4_ of several _entity5_ classifiers _/entity5_ to form a highly accurate one . Experiments show that this approach is superior to a single _entity6_ decision-tree classifier _/entity6_ . Furthermore , we present a _entity7_ standalone system _/entity7_ that resolves _entity8_ pronouns _/entity8_ in _entity9_ _P_ unannotated text _/entity9_ by using a fully automatic sequence of _entity10_ _C_ preprocessing modules _/entity10_ that mimics the _entity11_ manual annotation process _/entity11_ . Although the system performs well within a limited _entity12_ textual domain _/entity12_ , further research is needed to make it effective for _entity13_ open-domain question answering _/entity13_ and _entity14_ text summarisation _/entity14_ .	NONE entity9 entity10
We give an analysis of _entity1_ ellipsis resolution _/entity1_ in terms of a straightforward _entity2_ discourse copying algorithm _/entity2_ that correctly predicts a wide range of phenomena . The treatment does not suffer from problems inherent in _entity3_ _P_ identity-of-relations analyses _/entity3_ . Furthermore , in contrast to the approach of Dalrymple et al . [ 1991 ] , the treatment directly encodes the intuitive distinction between _entity4_ full NPs _/entity4_ and the _entity5_ _C_ referential elements _/entity5_ that corefer with them through what we term _entity6_ role linking _/entity6_ . The correct _entity7_ predictions _/entity7_ for several problematic examples of _entity8_ ellipsis _/entity8_ naturally result . Finally , the analysis extends directly to other _entity9_ discourse copying phenomena _/entity9_ .	NONE entity3 entity5
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ _C_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ _P_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity13 entity10
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ _P_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ _C_ phrase _/entity17_ length .	NONE entity15 entity17
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ _C_ laboratory study _/entity9_ using the _entity10_ _P_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	USAGE entity10 entity9
_entity1_ Statistical machine translation ( SMT ) _/entity1_ is currently one of the hot spots in _entity2_ natural language processing _/entity2_ . Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that _entity3_ SMT _/entity3_ gives competitive results to _entity4_ rule-based translation systems _/entity4_ , requiring significantly less development time . This is particularly important when building _entity5_ translation systems _/entity5_ for new _entity6_ _C_ language pairs _/entity6_ or new _entity7_ domains _/entity7_ . This workshop is intended to give an introduction to _entity8_ _P_ statistical machine translation _/entity8_ with a focus on practical considerations . Participants should be able , after attending this workshop , to set out building an _entity9_ SMT system _/entity9_ themselves and achieving good _entity10_ baseline results _/entity10_ in a short time . The tutorial will cover the basics of _entity11_ SMT _/entity11_ : Theory will be put into practice . _entity12_ STTK _/entity12_ , a _entity13_ statistical machine translation tool kit _/entity13_ , will be introduced and used to build a working _entity14_ translation system _/entity14_ . _entity15_ STTK _/entity15_ has been developed by the presenter and co-workers over a number of years and is currently used as the basis of _entity16_ CMU 's SMT system _/entity16_ . It has also successfully been coupled with _entity17_ rule-based and example based machine translation modules _/entity17_ to build a _entity18_ multi engine machine translation system _/entity18_ . The _entity19_ source code _/entity19_ of the _entity20_ tool kit _/entity20_ will be made available .	NONE entity8 entity6
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ _C_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ _P_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity24 entity22
This paper proposes that _entity1_ sentence analysis _/entity1_ should be treated as _entity2_ defeasible reasoning _/entity2_ , and presents such a treatment for _entity3_ Japanese sentence analyses _/entity3_ using an _entity4_ _C_ argumentation system _/entity4_ by Konolige , which is a _entity5_ formalization _/entity5_ of _entity6_ defeasible reasoning _/entity6_ , that includes _entity7_ _P_ arguments _/entity7_ and _entity8_ defeat rules _/entity8_ that capture _entity9_ defeasibility _/entity9_ .	NONE entity7 entity4
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ _P_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ _C_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity12 entity14
A research program is described in which a particular _entity1_ representational format for meaning _/entity1_ is tested as broadly as possible . In this format , developed by the LNR research group at The University of California at San Diego , _entity2_ verbs _/entity2_ are represented as interconnected sets of _entity3_ subpredicates _/entity3_ . These _entity4_ _P_ subpredicates _/entity4_ may be thought of as the almost inevitable _entity5_ inferences _/entity5_ that a _entity6_ listener _/entity6_ makes when a _entity7_ _C_ verb _/entity7_ is used in a _entity8_ sentence _/entity8_ . They confer a _entity9_ meaning structure _/entity9_ on the _entity10_ sentence _/entity10_ in which the _entity11_ verb _/entity11_ is used .	NONE entity4 entity7
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ _C_ computational semantic analyses _/entity11_ of _entity12_ _P_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity12 entity11
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ _C_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ _P_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	NONE entity14 entity12
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ HPSG _/entity2_ into _entity3_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ HPSG _/entity5_ has a more elaborated _entity6_ _P_ principle-based theory _/entity6_ of possible _entity7_ phrase structures _/entity7_ , _entity8_ TAG _/entity8_ provides the means to represent _entity9_ _C_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity6 entity9
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ _C_ language modeling _/entity14_ . The recognizer uses a _entity15_ _P_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity15 entity14
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ _P_ model _/entity2_ that a _entity3_ _C_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity2 entity3
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ _P_ out-degree _/entity9_ greater than one . We create a _entity10_ _C_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity9 entity10
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ ungrammatical input _/entity8_ . While _entity9_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ passives _/entity11_ , _entity12_ relative clauses _/entity12_ and _entity13_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ _P_ Plume _/entity16_ to handle _entity17_ passives _/entity17_ , _entity18_ relative clauses _/entity18_ , and _entity19_ _C_ interrogatives _/entity19_ in a general manner .	NONE entity16 entity19
This paper presents a _entity1_ _C_ critical discussion _/entity1_ of the various _entity2_ _P_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity2 entity1
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ _C_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ _P_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity8 entity6
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ _C_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ _P_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity5 entity2
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ _C_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ _P_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity20 entity18
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ _C_ Path-based inference rules _/entity9_ may be written using a _entity10_ _P_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	USAGE entity10 entity9
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ _P_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ _C_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity3 entity6
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ _P_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ _C_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity3 entity6
In this paper , we present an _entity1_ unlexicalized parser _/entity1_ for _entity2_ German _/entity2_ which employs _entity3_ smoothing _/entity3_ and _entity4_ suffix analysis _/entity4_ to achieve a _entity5_ labelled bracket F-score _/entity5_ of 76.2 , higher than previously reported results on the _entity6_ NEGRA corpus _/entity6_ . In addition to the high _entity7_ _P_ accuracy _/entity7_ of the model , the use of _entity8_ smoothing _/entity8_ in an _entity9_ unlexicalized parser _/entity9_ allows us to better examine the interplay between _entity10_ _C_ smoothing _/entity10_ and _entity11_ parsing _/entity11_ results .	NONE entity7 entity10
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ _P_ grammatical gender _/entity10_ in _entity11_ _C_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity10 entity11
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ _P_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ _C_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity4 entity7
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ _P_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ _C_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity8 entity11
In order to meet the needs of a publication of papers in English , many systems to run off texts have been developed . In this paper , we report a system _entity1_ FROFF _/entity1_ which can make a fair copy of not only texts but also graphs and tables indispensable to our papers . Its selection of _entity2_ fonts _/entity2_ , specification of _entity3_ character _/entity3_ size are dynamically changeable , and the _entity4_ _C_ typing location _/entity4_ can be also changed in lateral or longitudinal directions . Each _entity5_ _P_ character _/entity5_ has its own width and a line length is counted by the sum of each _entity6_ character _/entity6_ . By using commands or _entity7_ rules _/entity7_ which are defined to facilitate the construction of format expected or some _entity8_ mathematical expressions _/entity8_ , elaborate and pretty documents can be successfully obtained .	NONE entity5 entity4
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ _C_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ _P_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity11 entity8
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ _P_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ _C_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity3 entity4
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ _P_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ _C_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity16 entity19
_entity1_ Link detection _/entity1_ has been regarded as a core technology for the _entity2_ Topic Detection and Tracking tasks _/entity2_ of _entity3_ new event detection _/entity3_ . In this paper we formulate _entity4_ _C_ story link detection _/entity4_ and _entity5_ _P_ new event detection _/entity5_ as _entity6_ information retrieval task _/entity6_ and hypothesize on the impact of _entity7_ precision _/entity7_ and _entity8_ recall _/entity8_ on both systems . Motivated by these arguments , we introduce a number of new performance enhancing techniques including _entity9_ part of speech tagging _/entity9_ , new _entity10_ similarity measures _/entity10_ and expanded _entity11_ stop lists _/entity11_ . Experimental results validate our hypothesis .	NONE entity5 entity4
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ _P_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ _C_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity7 entity9
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ _C_ human annotated text _/entity22_ , in addition to an _entity23_ _P_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity23 entity22
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ _P_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ _C_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity7 entity10
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ performance _/entity5_ of using _entity6_ ASR output _/entity6_ as opposed to _entity7_ human transcription _/entity7_ . Examination of the effect of _entity8_ features _/entity8_ shows that _entity9_ _C_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ _P_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity10 entity9
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ _C_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ _P_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity7 entity5
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ _C_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ _P_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity5 entity2
A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature , practical applications rather than `` toy '' implementations . Evaluation is also crucial to assessing competing claims and identifying promising technical approaches . While work in _entity1_ speech recognition ( SR ) _/entity1_ has a history of evaluation methodologies that permit comparison among various systems , until recently no methodology existed for either developers of _entity2_ natural language ( NL ) interfaces _/entity2_ or researchers in _entity3_ speech understanding ( SU ) _/entity3_ to evaluate and compare the systems they developed . Recently considerable progress has been made by a number of groups involved in the _entity4_ DARPA Spoken Language Systems ( SLS ) program _/entity4_ to agree on a methodology for comparative evaluation of _entity5_ _C_ SLS systems _/entity5_ , and that methodology has been put into practice several times in comparative tests of several _entity6_ _P_ SLS systems _/entity6_ . These evaluations are probably the only _entity7_ NL evaluations _/entity7_ other than the series of _entity8_ Message Understanding Conferences _/entity8_ ( Sundheim , 1989 ; Sundheim , 1991 ) to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems ( Palmer et al. , 1989 ; Neal et al. , 1991 ) . This paper describes a practical _entity9_ `` black-box '' methodology _/entity9_ for automatic evaluation of _entity10_ question-answering NL systems _/entity10_ . While each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and it can be used with either _entity11_ speech or text input _/entity11_ . The particular characteristics of the approach are described in the following section : subsequent sections present its implementation in the _entity12_ DARPA SLS community _/entity12_ , and some problems and directions for future development .	NONE entity6 entity5
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ _P_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ _C_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity15 entity17
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ _P_ features _/entity14_ for _entity15_ _C_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity14 entity15
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ _C_ HPSG _/entity2_ into _entity3_ _P_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ HPSG _/entity5_ has a more elaborated _entity6_ principle-based theory _/entity6_ of possible _entity7_ phrase structures _/entity7_ , _entity8_ TAG _/entity8_ provides the means to represent _entity9_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity3 entity2
In this paper I will argue for a _entity1_ model of grammatical processing _/entity1_ that is based on _entity2_ uniform processing _/entity2_ and _entity3_ _C_ knowledge sources _/entity3_ . The main _entity4_ feature _/entity4_ of this model is to view _entity5_ _P_ parsing _/entity5_ and _entity6_ generation _/entity6_ as two strongly interleaved tasks performed by a single _entity7_ parametrized deduction _/entity7_ process . It will be shown that this view supports flexible and efficient _entity8_ natural language processing _/entity8_ .	NONE entity5 entity3
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ _C_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ _P_ transliteration process _/entity16_ .	NONE entity16 entity13
This paper proposes a generic _entity1_ mathematical formalism _/entity1_ for the combination of various _entity2_ structures _/entity2_ : _entity3_ strings _/entity3_ , _entity4_ trees _/entity4_ , _entity5_ dags _/entity5_ , _entity6_ graphs _/entity6_ , and products of them . The _entity7_ polarization _/entity7_ of the objects of the _entity8_ elementary structures _/entity8_ controls the _entity9_ saturation _/entity9_ of the final _entity10_ structure _/entity10_ . This formalism is both elementary and powerful enough to strongly simulate many _entity11_ grammar formalisms _/entity11_ , such as _entity12_ rewriting systems _/entity12_ , _entity13_ _C_ dependency grammars _/entity13_ , _entity14_ _P_ TAG _/entity14_ , _entity15_ HPSG _/entity15_ and _entity16_ LFG _/entity16_ .	NONE entity14 entity13
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ _P_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ _C_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity10 entity13
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ _P_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ _C_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity7 entity9
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ _P_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ _C_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity13 entity16
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ _C_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ _P_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity24 entity22
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ _C_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ _P_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity10 entity7
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ _C_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ _P_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity8 entity6
This paper presents a novel _entity1_ ensemble learning approach _/entity1_ to resolving _entity2_ _P_ German pronouns _/entity2_ . _entity3_ Boosting _/entity3_ , the method in question , combines the moderately accurate _entity4_ hypotheses _/entity4_ of several _entity5_ _C_ classifiers _/entity5_ to form a highly accurate one . Experiments show that this approach is superior to a single _entity6_ decision-tree classifier _/entity6_ . Furthermore , we present a _entity7_ standalone system _/entity7_ that resolves _entity8_ pronouns _/entity8_ in _entity9_ unannotated text _/entity9_ by using a fully automatic sequence of _entity10_ preprocessing modules _/entity10_ that mimics the _entity11_ manual annotation process _/entity11_ . Although the system performs well within a limited _entity12_ textual domain _/entity12_ , further research is needed to make it effective for _entity13_ open-domain question answering _/entity13_ and _entity14_ text summarisation _/entity14_ .	NONE entity2 entity5
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ _P_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ _C_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity9 entity11
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ _P_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ _C_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity8 entity10
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ _P_ parsing _/entity4_ . We compare two wide-coverage _entity5_ lexicalized grammars of English _/entity5_ , _entity6_ _C_ LEXSYS _/entity6_ and _entity7_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity4 entity6
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ parsing _/entity4_ . We compare two wide-coverage _entity5_ _C_ lexicalized grammars of English _/entity5_ , _entity6_ _P_ LEXSYS _/entity6_ and _entity7_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity6 entity5
The applicability of many current _entity1_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ _C_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ _P_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ supervised methods _/entity13_ on 50 _entity14_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity7 entity5
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ _C_ intentional structure _/entity15_ captures the _entity16_ _P_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity16 entity15
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ _P_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ _C_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity16 entity18
_entity1_ Chart parsing _/entity1_ is _entity2_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ _C_ chart _/entity3_ that works outward from _entity4_ islands _/entity4_ and makes sense of as much of the _entity5_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ _P_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ fragments _/entity7_ occur in the _entity8_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ islands _/entity9_ , until possibly completely missing _entity10_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity6 entity3
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ _P_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ _C_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity15 entity17
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ _P_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ _C_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity9 entity12
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ _P_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ _C_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity5 entity8
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ _P_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ _C_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity23 entity26
We present a _entity1_ _P_ text mining method _/entity1_ for finding _entity2_ _C_ synonymous expressions _/entity2_ based on the _entity3_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity1 entity2
We examine the relationship between the two _entity1_ grammatical formalisms _/entity1_ : _entity2_ _C_ Tree Adjoining Grammars _/entity2_ and _entity3_ _P_ Head Grammars _/entity3_ . We briefly investigate the weak _entity4_ equivalence _/entity4_ of the two _entity5_ formalisms _/entity5_ . We then turn to a discussion comparing the _entity6_ linguistic expressiveness _/entity6_ of the two _entity7_ formalisms _/entity7_ .	NONE entity3 entity2
The multiplicative fragment of _entity1_ linear logic _/entity1_ has found a number of applications in _entity2_ computational linguistics _/entity2_ : in the _entity3_ _P_ `` glue language '' _/entity3_ approach to _entity4_ LFG semantics _/entity4_ , and in the formulation and _entity5_ parsing _/entity5_ of various _entity6_ _C_ categorial grammars _/entity6_ . These applications call for efficient deduction methods . Although a number of deduction methods for _entity7_ multiplicative linear logic _/entity7_ are known , none of them are tabular methods , which bring a substantial efficiency gain by avoiding redundant computation ( cf . chart methods in _entity8_ CFG parsing _/entity8_ ) : this paper presents such a method , and discusses its use in relation to the above applications .	NONE entity3 entity6
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ _C_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ _P_ electronic discussions _/entity5_ that influence the _entity6_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity5 entity2
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ _P_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ _C_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity9 entity11
In this paper we present a novel , customizable : _entity1_ _P_ IE paradigm _/entity1_ that takes advantage of _entity2_ predicate-argument structures _/entity2_ . We also introduce a new way of automatically identifying _entity3_ predicate argument structures _/entity3_ , which is central to our _entity4_ _C_ IE paradigm _/entity4_ . It is based on : ( 1 ) an extended set of _entity5_ features _/entity5_ ; and ( 2 ) _entity6_ inductive decision tree learning _/entity6_ . The experimental results prove our claim that accurate _entity7_ predicate-argument structures _/entity7_ enable high quality _entity8_ IE _/entity8_ results .	NONE entity1 entity4
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ question phrases _/entity5_ in _entity6_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ _C_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ _P_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ MRR _/entity14_ .	NONE entity11 entity10
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ _P_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ _C_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity19 entity21
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ _P_ corpus text _/entity29_ and lemmatize them as complete _entity30_ _C_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity29 entity30
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ _P_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ _C_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity5 entity7
_entity1_ _P_ Sentence boundary detection _/entity1_ in _entity2_ _C_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity1 entity2
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ _C_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ _P_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity18 entity15
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ _P_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ _C_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity2 entity5
We discuss _entity1_ maximum a posteriori estimation _/entity1_ of _entity2_ _C_ continuous density hidden Markov models ( CDHMM ) _/entity2_ . The classical _entity3_ _P_ MLE reestimation algorithms _/entity3_ , namely the _entity4_ forward-backward algorithm _/entity4_ and the _entity5_ segmental k-means algorithm _/entity5_ , are expanded and _entity6_ reestimation formulas _/entity6_ are given for _entity7_ HMM with Gaussian mixture observation densities _/entity7_ . Because of its adaptive nature , _entity8_ Bayesian learning _/entity8_ serves as a unified approach for the following four _entity9_ speech recognition _/entity9_ applications , namely _entity10_ parameter smoothing _/entity10_ , _entity11_ speaker adaptation _/entity11_ , _entity12_ speaker group modeling _/entity12_ and _entity13_ corrective training _/entity13_ . New experimental results on all four applications are provided to show the effectiveness of the _entity14_ MAP estimation approach _/entity14_ .	NONE entity3 entity2
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ _C_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ _P_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity11 entity9
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ _C_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ _P_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity14 entity13
We present an operable definition of _entity1_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ _P_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ _C_ discourse model _/entity3_ and _entity4_ knowledge store _/entity4_ is introduced enabling the _entity5_ decomposition _/entity5_ and _entity6_ formal representation _/entity6_ of its _entity7_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity2 entity3
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ _C_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ _P_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity10 entity7
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ _C_ semantic coherence _/entity6_ . We conducted an _entity7_ _P_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity7 entity6
This paper describes a method of _entity1_ interactively visualizing and directing the process _/entity1_ of _entity2_ translating a sentence _/entity2_ . The method allows a _entity3_ user _/entity3_ to explore a _entity4_ model _/entity4_ of _entity5_ syntax-based statistical machine translation ( MT ) _/entity5_ , to understand the _entity6_ model _/entity6_ 's strengths and weaknesses , and to compare it to other _entity7_ _C_ MT systems _/entity7_ . Using this _entity8_ visualization method _/entity8_ , we can find and address conceptual and practical problems in an _entity9_ MT system _/entity9_ . In our demonstration at _entity10_ _P_ ACL _/entity10_ , new _entity11_ users _/entity11_ of our tool will drive a _entity12_ syntax-based decoder _/entity12_ for themselves .	NONE entity10 entity7
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ _C_ completeness _/entity34_ and _entity35_ _P_ coherence _/entity35_ .	NONE entity35 entity34
To support engaging human users in robust , _entity1_ mixed-initiative speech dialogue interactions _/entity1_ which reach beyond current capabilities in _entity2_ dialogue systems _/entity2_ , the _entity3_ _P_ DARPA Communicator program _/entity3_ [ 1 ] is funding the development of a _entity4_ _C_ distributed message-passing infrastructure _/entity4_ for _entity5_ dialogue systems _/entity5_ which all _entity6_ Communicator _/entity6_ participants are using . In this presentation , we describe the features of and _entity7_ requirements _/entity7_ for a genuinely useful _entity8_ software infrastructure _/entity8_ for this purpose .	NONE entity3 entity4
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ _C_ high-quality data _/entity7_ that allow the comparison of both _entity8_ _P_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ automatic metrics _/entity11_ used within _entity12_ MT _/entity12_ are also discussed in this regard .	NONE entity8 entity7
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ _P_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ _C_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity4 entity6
`` To explain complex phenomena , an _entity1_ _P_ explanation system _/entity1_ must be able to select information from a formal representation of _entity2_ _C_ domain knowledge _/entity2_ , organize the selected information into _entity3_ multisentential discourse plans _/entity3_ , and realize the _entity4_ discourse plans _/entity4_ in text . Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for _entity5_ explanation _/entity5_ , empirical results have been limited . This paper reports on a seven-year effort to empirically study _entity6_ explanation generation _/entity6_ from _entity7_ semantically rich , large-scale knowledge bases _/entity7_ . In particular , it describes a _entity8_ robust explanation system _/entity8_ that constructs _entity9_ multisentential and multi-paragraph explanations _/entity9_ from the a _entity10_ large-scale knowledge base _/entity10_ in the domain of botanical anatomy , physiology , and development . We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system . In this evaluation , scored within `` '' half a grade '' '' of domain experts , and its performance exceeded that of one of the domain experts . ''	NONE entity1 entity2
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ statistical ranking _/entity6_ than a previous approach to _entity7_ statistical generation _/entity7_ . An efficient _entity8_ _C_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ _P_ lattice-based approach _/entity9_ .	NONE entity9 entity8
We present a _entity1_ Czech-English statistical machine translation system _/entity1_ which performs _entity2_ tree-to-tree translation _/entity2_ of _entity3_ dependency structures _/entity3_ . The only _entity4_ _P_ bilingual resource _/entity4_ required is a _entity5_ sentence-aligned parallel corpus _/entity5_ . All other _entity6_ resources _/entity6_ are _entity7_ _C_ monolingual _/entity7_ . We also refer to an _entity8_ evaluation method _/entity8_ and plan to compare our _entity9_ system 's output _/entity9_ with a _entity10_ benchmark system _/entity10_ .	NONE entity4 entity7
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ _C_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ _P_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity4 entity2
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ _P_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ _C_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	RESULT entity6 entity8
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ formatted weather data _/entity2_ . Such _entity3_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ stereotyped text structure _/entity5_ . _entity6_ RAREAS _/entity6_ draws on several kinds of _entity7_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ _P_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ _C_ bilingual or multi-lingual texts _/entity9_ .	NONE entity8 entity9
This paper proposes _entity1_ document oriented preference sets ( DoPS ) _/entity1_ for the disambiguation of the _entity2_ dependency structure _/entity2_ of _entity3_ sentences _/entity3_ . The _entity4_ DoPS system _/entity4_ extracts preference knowledge from a _entity5_ target document _/entity5_ or other _entity6_ documents _/entity6_ automatically . _entity7_ Sentence ambiguities _/entity7_ can be resolved by using domain targeted preference knowledge without using complicated large _entity8_ _C_ knowledgebases _/entity8_ . _entity9_ Implementation _/entity9_ and _entity10_ empirical results _/entity10_ are described for the the analysis of _entity11_ _P_ dependency structures _/entity11_ of _entity12_ Japanese patent claim sentences _/entity12_ .	NONE entity11 entity8
In order to boost the _entity1_ translation quality _/entity1_ of _entity2_ EBMT _/entity2_ based on a small-sized _entity3_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ language model _/entity5_ of an in-domain _entity6_ _P_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ _C_ BLEU score _/entity9_ and the _entity10_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity6 entity9
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ _C_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ _P_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity19 entity17
This paper describes a recently collected _entity1_ spoken language corpus _/entity1_ for the _entity2_ ATIS ( Air Travel Information System ) domain _/entity2_ . This data collection effort has been co-ordinated by _entity3_ MADCOW ( Multi-site ATIS Data COllection Working group ) _/entity3_ . We summarize the motivation for this effort , the goals , the implementation of a _entity4_ multi-site data collection paradigm _/entity4_ , and the accomplishments of _entity5_ MADCOW _/entity5_ in monitoring the _entity6_ _P_ collection _/entity6_ and distribution of 12,000 _entity7_ utterances _/entity7_ of _entity8_ _C_ spontaneous speech _/entity8_ from five sites for use in a _entity9_ multi-site common evaluation of speech , natural language and spoken language _/entity9_ .	NONE entity6 entity8
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ _P_ pattern-matching language _/entity13_ to classify _entity14_ _C_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity13 entity14
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ _C_ compositional approaches _/entity24_ , we present a _entity25_ _P_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity25 entity24
The multiplicative fragment of _entity1_ linear logic _/entity1_ has found a number of applications in _entity2_ computational linguistics _/entity2_ : in the _entity3_ `` glue language '' _/entity3_ approach to _entity4_ LFG semantics _/entity4_ , and in the formulation and _entity5_ parsing _/entity5_ of various _entity6_ _P_ categorial grammars _/entity6_ . These applications call for efficient deduction methods . Although a number of deduction methods for _entity7_ multiplicative linear logic _/entity7_ are known , none of them are tabular methods , which bring a substantial efficiency gain by avoiding redundant computation ( cf . chart methods in _entity8_ _C_ CFG parsing _/entity8_ ) : this paper presents such a method , and discusses its use in relation to the above applications .	NONE entity6 entity8
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ _P_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ _C_ fields _/entity19_ and 87 % for multi-field records .	NONE entity16 entity19
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ _P_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ _C_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ LFG _/entity4_ and _entity5_ UDRS _/entity5_ formalisms . It provides a _entity6_ model theoretic interpretation _/entity6_ and an _entity7_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ f-structures _/entity9_ through the _entity10_ translation images _/entity10_ of _entity11_ f-structures _/entity11_ as _entity12_ UDRSs _/entity12_ .	NONE entity2 entity3
We describe a novel approach to _entity1_ _P_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ _C_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity1 entity4
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ _P_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ _C_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity15 entity18
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ _P_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ _C_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity6 entity7
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ _P_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ _C_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity5 entity7
Although every _entity1_ natural language system _/entity1_ needs a _entity2_ computational lexicon _/entity2_ , each system puts different amounts and types of information into its _entity3_ lexicon _/entity3_ according to its individual needs . However , some of the information needed across systems is shared or identical information . This paper presents our experience in planning and building _entity4_ _C_ COMPLEX _/entity4_ , a _entity5_ _P_ computational lexicon _/entity5_ designed to be a repository of _entity6_ shared lexical information _/entity6_ for use by _entity7_ Natural Language Processing ( NLP ) systems _/entity7_ . We have drawn primarily on explicit and implicit information from _entity8_ machine-readable dictionaries ( MRD 's ) _/entity8_ to create a _entity9_ broad coverage lexicon _/entity9_ .	NONE entity5 entity4
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ _P_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ _C_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity6 entity8
_entity1_ Pipelined Natural Language Generation ( NLG ) systems _/entity1_ have grown increasingly complex as _entity2_ _C_ architectural modules _/entity2_ were added to support _entity3_ language functionalities _/entity3_ such as _entity4_ _P_ referring expressions _/entity4_ , _entity5_ lexical choice _/entity5_ , and _entity6_ revision _/entity6_ . This has given rise to discussions about the relative placement of these new _entity7_ modules _/entity7_ in the overall _entity8_ architecture _/entity8_ . Recent work on another aspect of _entity9_ multi-paragraph text _/entity9_ , _entity10_ discourse markers _/entity10_ , indicates it is time to consider where a _entity11_ discourse marker insertion algorithm _/entity11_ fits in . We present examples which suggest that in a _entity12_ pipelined NLG architecture _/entity12_ , the best approach is to strongly tie it to a _entity13_ revision component _/entity13_ . Finally , we evaluate the approach in a working _entity14_ multi-page system _/entity14_ .	NONE entity4 entity2
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ _C_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ _P_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity9 entity7
In this paper , we propose a novel _entity1_ Cooperative Model _/entity1_ for _entity2_ natural language understanding _/entity2_ in a _entity3_ dialogue system _/entity3_ . We build this based on both _entity4_ _P_ Finite State Model ( FSM ) _/entity4_ and _entity5_ Statistical Learning Model ( SLM ) _/entity5_ . _entity6_ FSM _/entity6_ provides two strategies for _entity7_ _C_ language understanding _/entity7_ and have a high accuracy but little robustness and flexibility . _entity8_ Statistical approach _/entity8_ is much more robust but less accurate . _entity9_ Cooperative Model _/entity9_ incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies .	NONE entity4 entity7
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ _C_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ _P_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity6 entity3
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ _C_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ _P_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity6 entity5
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ _P_ part of speech information _/entity9_ of the _entity10_ _C_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	MODEL-FEATURE entity9 entity10
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ _C_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ _P_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity7 entity5
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ statistical approaches _/entity6_ , which resolve _entity7_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ _C_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ _P_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity12 entity10
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ _C_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ _P_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity12 entity11
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ _C_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ _P_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity12 entity10
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ _P_ human annotated text _/entity22_ , in addition to an _entity23_ _C_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity22 entity23
This paper shows that it is very often possible to identify the _entity1_ source language _/entity1_ of medium-length speeches in the _entity2_ EUROPARL corpus _/entity2_ on the basis of _entity3_ frequency counts _/entity3_ of _entity4_ word n-grams _/entity4_ ( 87.2 % -96.7 % _entity5_ _C_ accuracy _/entity5_ depending on _entity6_ classification method _/entity6_ ) . The paper also examines in detail which _entity7_ _P_ positive markers _/entity7_ are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones .	NONE entity7 entity5
This paper shows that it is very often possible to identify the _entity1_ source language _/entity1_ of medium-length speeches in the _entity2_ EUROPARL corpus _/entity2_ on the basis of _entity3_ _P_ frequency counts _/entity3_ of _entity4_ _C_ word n-grams _/entity4_ ( 87.2 % -96.7 % _entity5_ accuracy _/entity5_ depending on _entity6_ classification method _/entity6_ ) . The paper also examines in detail which _entity7_ positive markers _/entity7_ are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones .	MODEL-FEATURE entity3 entity4
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ parsing _/entity4_ . We compare two wide-coverage _entity5_ _P_ lexicalized grammars of English _/entity5_ , _entity6_ LEXSYS _/entity6_ and _entity7_ XTAG _/entity7_ , finding that the two _entity8_ _C_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity5 entity8
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ _C_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ _P_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity8 entity6
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ _C_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ _P_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity17 entity15
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ _C_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ _P_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity5 entity3
How to obtain _entity1_ hierarchical relations _/entity1_ ( e.g . _entity2_ superordinate -hyponym relation _/entity2_ , _entity3_ synonym relation _/entity3_ ) is one of the most important problems for _entity4_ thesaurus construction _/entity4_ . A pilot system for extracting these _entity5_ relations _/entity5_ automatically from an ordinary _entity6_ _P_ Japanese language dictionary _/entity6_ ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given . The features of the _entity7_ definition sentences _/entity7_ in the _entity8_ _C_ dictionary _/entity8_ , the mechanical extraction of the _entity9_ hierarchical relations _/entity9_ and the estimation of the results are discussed .	NONE entity6 entity8
We present a new HMM tagger that exploits _entity1_ context _/entity1_ on both sides of a _entity2_ word _/entity2_ to be tagged , and evaluate it in both the _entity3_ unsupervised and supervised case _/entity3_ . Along the way , we present the first comprehensive comparison of _entity4_ unsupervised methods for part-of-speech tagging _/entity4_ , noting that published results to date have not been comparable across _entity5_ corpora _/entity5_ or _entity6_ lexicons _/entity6_ . Observing that the _entity7_ _P_ quality _/entity7_ of the _entity8_ lexicon _/entity8_ greatly impacts the _entity9_ _C_ accuracy _/entity9_ that can be achieved by the _entity10_ algorithms _/entity10_ , we present a method of _entity11_ HMM training _/entity11_ that improves _entity12_ accuracy _/entity12_ when _entity13_ training _/entity13_ of _entity14_ lexical probabilities _/entity14_ is unstable . Finally , we show how this new tagger achieves state-of-the-art results in a _entity15_ supervised , non-training intensive framework _/entity15_ .	RESULT entity7 entity9
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ CFG parsers _/entity3_ running in _entity4_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ grammar G _/entity5_ and a _entity6_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ _C_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ _P_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ CFG parsing _/entity11_ : a fast , practical _entity12_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity9 entity8
_entity1_ _C_ Information extraction techniques _/entity1_ automatically create _entity2_ _P_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity2 entity1
We present a framework for _entity1_ word alignment _/entity1_ based on _entity2_ log-linear models _/entity2_ . All _entity3_ knowledge sources _/entity3_ are treated as _entity4_ feature functions _/entity4_ , which depend on the _entity5_ source langauge sentence _/entity5_ , the _entity6_ target language sentence _/entity6_ and possible additional variables . _entity7_ Log-linear models _/entity7_ allow _entity8_ statistical alignment models _/entity8_ to be easily extended by incorporating _entity9_ syntactic information _/entity9_ . In this paper , we use _entity10_ _C_ IBM Model 3 alignment probabilities _/entity10_ , _entity11_ _P_ POS correspondence _/entity11_ , and _entity12_ bilingual dictionary coverage _/entity12_ as _entity13_ features _/entity13_ . Our experiments show that _entity14_ log-linear models _/entity14_ significantly outperform _entity15_ IBM translation models _/entity15_ .	NONE entity11 entity10
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ _P_ paraphrases _/entity7_ in one _entity8_ _C_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity7 entity8
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ corpus of sentences _/entity3_ in the domain of the _entity4_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ _P_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ _C_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity5 entity7
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ _C_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ _P_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity17 entity14
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ events _/entity2_ are defined as _entity3_ event terms _/entity3_ and _entity4_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ _P_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ _C_ documents _/entity9_ . Experimental results are encouraging .	NONE entity6 entity9
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ _P_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ _C_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity15 entity18
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ _P_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ _C_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity23 entity25
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ _P_ subtrees _/entity11_ in the _entity12_ _C_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	PART_WHOLE entity11 entity12
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ _P_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ _C_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity9 entity11
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ _P_ weight _/entity9_ on errors from the viewpoint of _entity10_ _C_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity9 entity10
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ _P_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ _C_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity5 entity8
We present results on _entity1_ addressee identification _/entity1_ in _entity2_ four-participants face-to-face meetings _/entity2_ using _entity3_ Bayesian Network _/entity3_ and _entity4_ Naive Bayes classifiers _/entity4_ . First , we investigate how well the _entity5_ addressee _/entity5_ of a _entity6_ dialogue act _/entity6_ can be predicted based on _entity7_ gaze _/entity7_ , _entity8_ utterance _/entity8_ and _entity9_ conversational context features _/entity9_ . Then , we explore whether information about _entity10_ meeting context _/entity10_ can aid _entity11_ _P_ classifiers _/entity11_ ' _entity12_ performances _/entity12_ . Both _entity13_ _C_ classifiers _/entity13_ perform the best when _entity14_ conversational context _/entity14_ and _entity15_ utterance features _/entity15_ are combined with _entity16_ speaker 's gaze information _/entity16_ . The _entity17_ classifiers _/entity17_ show little _entity18_ gain _/entity18_ from information about _entity19_ meeting context _/entity19_ .	NONE entity11 entity13
Currently several _entity1_ _C_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ _P_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity4 entity1
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ _C_ song lyrics _/entity5_ actually contribute little to _entity6_ _P_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity6 entity5
We present results on _entity1_ addressee identification _/entity1_ in _entity2_ four-participants face-to-face meetings _/entity2_ using _entity3_ Bayesian Network _/entity3_ and _entity4_ Naive Bayes classifiers _/entity4_ . First , we investigate how well the _entity5_ addressee _/entity5_ of a _entity6_ dialogue act _/entity6_ can be predicted based on _entity7_ gaze _/entity7_ , _entity8_ _C_ utterance _/entity8_ and _entity9_ _P_ conversational context features _/entity9_ . Then , we explore whether information about _entity10_ meeting context _/entity10_ can aid _entity11_ classifiers _/entity11_ ' _entity12_ performances _/entity12_ . Both _entity13_ classifiers _/entity13_ perform the best when _entity14_ conversational context _/entity14_ and _entity15_ utterance features _/entity15_ are combined with _entity16_ speaker 's gaze information _/entity16_ . The _entity17_ classifiers _/entity17_ show little _entity18_ gain _/entity18_ from information about _entity19_ meeting context _/entity19_ .	NONE entity9 entity8
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ _C_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ _P_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity6 entity4
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ _P_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ _C_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ TAGs _/entity14_ to be used beyond their role in _entity15_ syntax proper _/entity15_ . We discuss the application of _entity16_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity10 entity13
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ _P_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ _C_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity3 entity6
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ _C_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ _P_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity13 entity10
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ _P_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ _C_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity17 entity20
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ _C_ generative model _/entity16_ which takes these _entity17_ _P_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	USAGE entity17 entity16
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ _P_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ _C_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity9 entity11
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ _C_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ _P_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity11 entity8
A method for _entity1_ error correction _/entity1_ of _entity2_ ill-formed input _/entity2_ is described that acquires _entity3_ dialogue patterns _/entity3_ in typical usage and uses these _entity4_ patterns _/entity4_ to predict new inputs . _entity5_ Error correction _/entity5_ is done by strongly biasing _entity6_ parsing _/entity6_ toward expected _entity7_ meanings _/entity7_ unless clear evidence from the input shows the current _entity8_ _P_ sentence _/entity8_ is not expected . A _entity9_ dialogue acquisition and tracking algorithm _/entity9_ is presented along with a description of its _entity10_ _C_ implementation _/entity10_ in a _entity11_ voice interactive system _/entity11_ . A series of tests are described that show the power of the _entity12_ error correction methodology _/entity12_ when _entity13_ stereotypic dialogue _/entity13_ occurs .	NONE entity8 entity10
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ _C_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ _P_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity4 entity2
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ _P_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ _C_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity7 entity10
We consider the problem of computing the _entity1_ Kullback-Leibler distance _/entity1_ , also called the _entity2_ _C_ relative entropy _/entity2_ , between a _entity3_ probabilistic context-free grammar _/entity3_ and a _entity4_ _P_ probabilistic finite automaton _/entity4_ . We show that there is a _entity5_ closed-form ( analytical ) solution _/entity5_ for one part of the _entity6_ Kullback-Leibler distance _/entity6_ , viz . the _entity7_ cross-entropy _/entity7_ . We discuss several applications of the result to the problem of _entity8_ distributional approximation _/entity8_ of _entity9_ probabilistic context-free grammars _/entity9_ by means of _entity10_ probabilistic finite automata _/entity10_ .	NONE entity4 entity2
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ _P_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ _C_ original document _/entity23_ .	NONE entity21 entity23
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ _P_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ _C_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity2 entity4
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ _P_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ _C_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity3 entity6
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ corpus of sentences _/entity3_ in the domain of the _entity4_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ cross-validation _/entity8_ against the _entity9_ _C_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ _P_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity11 entity9
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ _P_ latent variables _/entity7_ . Finegrained _entity8_ _C_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity7 entity8
To verify _entity1_ _P_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ _C_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity1 entity3
We have developed a _entity1_ computational model _/entity1_ of the process of describing the layout of an apartment or house , a much-studied _entity2_ discourse task _/entity2_ first characterized linguistically by Linde ( 1974 ) . The _entity3_ model _/entity3_ is embodied in a program , _entity4_ APT _/entity4_ , that can reproduce segments of actual tape-recorded descriptions , using _entity5_ _P_ organizational and discourse strategies _/entity5_ derived through analysis of our _entity6_ _C_ corpus _/entity6_ .	NONE entity5 entity6
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ _P_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ _C_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity10 entity13
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ CFG parsers _/entity3_ running in _entity4_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ _C_ grammar G _/entity5_ and a _entity6_ _P_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ CFG parsing _/entity11_ : a fast , practical _entity12_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity6 entity5
Motivated by the success of _entity1_ _C_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ _P_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity4 entity1
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ _C_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ _P_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity18 entity16
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ _C_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ _P_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity23 entity21
We present a _entity1_ practically unsupervised learning method _/entity1_ to produce _entity2_ single-snippet answers _/entity2_ to _entity3_ definition questions _/entity3_ in _entity4_ question answering systems _/entity4_ that supplement _entity5_ Web search engines _/entity5_ . The method exploits _entity6_ _P_ on-line encyclopedias and dictionaries _/entity6_ to generate automatically an arbitrarily large number of _entity7_ positive and negative definition examples _/entity7_ , which are then used to train an _entity8_ svm _/entity8_ to separate the two classes . We show experimentally that the proposed method is viable , that it outperforms the alternative of training the _entity9_ _C_ system _/entity9_ on _entity10_ questions _/entity10_ and _entity11_ news articles from trec _/entity11_ , and that it helps the _entity12_ search engine _/entity12_ handle _entity13_ definition questions _/entity13_ significantly better .	NONE entity6 entity9
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ _C_ paraphrases _/entity11_ extracted from a _entity12_ _P_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity12 entity11
This paper describes the framework of a _entity1_ _C_ Korean phonological knowledge base system _/entity1_ using the _entity2_ unification-based grammar formalism _/entity2_ : _entity3_ _P_ Korean Phonology Structure Grammar ( KPSG ) _/entity3_ . The approach of _entity4_ KPSG _/entity4_ provides an explicit development model for constructing a computational _entity5_ phonological system _/entity5_ : _entity6_ speech recognition _/entity6_ and _entity7_ synthesis system _/entity7_ . We show that the proposed approach is more describable than other approaches such as those employing a traditional _entity8_ generative phonological approach _/entity8_ .	NONE entity3 entity1
This paper describes _entity1_ _C_ FERRET _/entity1_ , an _entity2_ interactive question-answering ( Q/A ) system _/entity2_ designed to address the challenges of integrating _entity3_ _P_ automatic Q/A _/entity3_ applications into real-world environments . _entity4_ FERRET _/entity4_ utilizes a novel approach to _entity5_ Q/A _/entity5_ known as _entity6_ predictive questioning _/entity6_ which attempts to identify the _entity7_ questions _/entity7_ ( and _entity8_ answers _/entity8_ ) that _entity9_ users _/entity9_ need by analyzing how a _entity10_ user _/entity10_ interacts with a system while gathering information related to a particular scenario .	NONE entity3 entity1
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ _P_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ _C_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity10 entity11
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ _C_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ _P_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity14 entity13
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ _C_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ _P_ performance _/entity5_ of using _entity6_ ASR output _/entity6_ as opposed to _entity7_ human transcription _/entity7_ . Examination of the effect of _entity8_ features _/entity8_ shows that _entity9_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity5 entity2
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ _C_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ _P_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity4 entity2
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ _P_ answering agents _/entity5_ searching for _entity6_ _C_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity5 entity6
We give an analysis of _entity1_ ellipsis resolution _/entity1_ in terms of a straightforward _entity2_ discourse copying algorithm _/entity2_ that correctly predicts a wide range of phenomena . The treatment does not suffer from problems inherent in _entity3_ identity-of-relations analyses _/entity3_ . Furthermore , in contrast to the approach of Dalrymple et al . [ 1991 ] , the treatment directly encodes the intuitive distinction between _entity4_ _P_ full NPs _/entity4_ and the _entity5_ referential elements _/entity5_ that corefer with them through what we term _entity6_ _C_ role linking _/entity6_ . The correct _entity7_ predictions _/entity7_ for several problematic examples of _entity8_ ellipsis _/entity8_ naturally result . Finally , the analysis extends directly to other _entity9_ discourse copying phenomena _/entity9_ .	NONE entity4 entity6
_entity1_ _C_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ language-independent system _/entity2_ for _entity3_ automatic discovery of text _/entity3_ in _entity4_ _P_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ words _/entity9_ per _entity10_ language _/entity10_ .	NONE entity4 entity1
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ _C_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ _P_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity9 entity7
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ _C_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ _P_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity16 entity14
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ _P_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ _C_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity4 entity7
In this study , we propose a _entity1_ knowledge-independent method _/entity1_ for aligning _entity2_ terms _/entity2_ and thus extracting _entity3_ translations _/entity3_ from a _entity4_ small , domain-specific corpus _/entity4_ consisting of _entity5_ parallel English and Chinese court judgments _/entity5_ from Hong Kong . With a _entity6_ sentence-aligned corpus _/entity6_ , _entity7_ translation equivalences _/entity7_ are suggested by analysing the _entity8_ frequency profiles _/entity8_ of _entity9_ parallel concordances _/entity9_ . The method overcomes the limitations of _entity10_ conventional statistical methods _/entity10_ which require _entity11_ large corpora _/entity11_ to be effective , and _entity12_ lexical approaches _/entity12_ which depend on existing _entity13_ _C_ bilingual dictionaries _/entity13_ . Pilot testing on a _entity14_ parallel corpus _/entity14_ of about 113K _entity15_ _P_ Chinese words _/entity15_ and 120K _entity16_ English words _/entity16_ gives an encouraging 85 % _entity17_ precision _/entity17_ and 45 % _entity18_ recall _/entity18_ . Future work includes fine-tuning the _entity19_ algorithm _/entity19_ upon the analysis of the errors , and acquiring a _entity20_ translation lexicon _/entity20_ for _entity21_ legal terminology _/entity21_ by filtering out _entity22_ general terms _/entity22_ .	NONE entity15 entity13
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ _P_ edges _/entity28_ with a valid _entity29_ _C_ semantic _/entity29_ interpretation are ever introduced .	NONE entity28 entity29
This paper presents a _entity1_ _P_ maximum entropy word alignment algorithm _/entity1_ for _entity2_ Arabic-English _/entity2_ based on _entity3_ _C_ supervised training data _/entity3_ . We demonstrate that it is feasible to create _entity4_ training material _/entity4_ for problems in _entity5_ machine translation _/entity5_ and that a mixture of _entity6_ supervised and unsupervised methods _/entity6_ yields superior _entity7_ performance _/entity7_ . The _entity8_ probabilistic model _/entity8_ used in the _entity9_ alignment _/entity9_ directly models the _entity10_ link decisions _/entity10_ . Significant improvement over traditional _entity11_ word alignment techniques _/entity11_ is shown as well as improvement on several _entity12_ machine translation tests _/entity12_ . Performance of the algorithm is contrasted with _entity13_ human annotation performance _/entity13_ .	NONE entity1 entity3
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ _C_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ _P_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity6 entity3
In order to meet the needs of a publication of papers in English , many systems to run off texts have been developed . In this paper , we report a system _entity1_ FROFF _/entity1_ which can make a fair copy of not only texts but also graphs and tables indispensable to our papers . Its selection of _entity2_ fonts _/entity2_ , specification of _entity3_ _C_ character _/entity3_ size are dynamically changeable , and the _entity4_ _P_ typing location _/entity4_ can be also changed in lateral or longitudinal directions . Each _entity5_ character _/entity5_ has its own width and a line length is counted by the sum of each _entity6_ character _/entity6_ . By using commands or _entity7_ rules _/entity7_ which are defined to facilitate the construction of format expected or some _entity8_ mathematical expressions _/entity8_ , elaborate and pretty documents can be successfully obtained .	NONE entity4 entity3
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ formatted weather data _/entity2_ . Such _entity3_ _P_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ _C_ stereotyped text structure _/entity5_ . _entity6_ RAREAS _/entity6_ draws on several kinds of _entity7_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ bilingual or multi-lingual texts _/entity9_ .	NONE entity3 entity5
_entity1_ _P_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ _C_ system combination _/entity3_ for _entity4_ unsupervised WSD _/entity4_ . We investigate several _entity5_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity1 entity3
Motivated by the success of _entity1_ _P_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ _C_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity1 entity3
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ CCRs _/entity3_ are _entity4_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ categories _/entity5_ in _entity6_ local trees _/entity6_ which allow the _entity7_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ _P_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ _C_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	NONE entity13 entity14
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ _P_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ _C_ Prolog _/entity15_ , cf . _entity16_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity12 entity15
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ _P_ unigram _/entity16_ counts and _entity17_ _C_ phrase _/entity17_ length .	NONE entity16 entity17
_entity1_ Words _/entity1_ in _entity2_ _P_ Chinese text _/entity2_ are not naturally separated by _entity3_ delimiters _/entity3_ , which poses a challenge to _entity4_ _C_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ MT _/entity5_ , the widely used approach is to apply a _entity6_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity2 entity4
This paper describes a _entity1_ _P_ characters-based Chinese collocation system _/entity1_ and discusses the advantages of it over a traditional _entity2_ _C_ word-based system _/entity2_ . Since _entity3_ wordbreaks _/entity3_ are not conventionally marked in _entity4_ Chinese text corpora _/entity4_ , a _entity5_ character-based collocation system _/entity5_ has the dual advantages of avoiding _entity6_ pre-processing distortion _/entity6_ and directly accessing _entity7_ sub-lexical information _/entity7_ . Furthermore , _entity8_ word-based collocational properties _/entity8_ can be obtained through an auxiliary module of _entity9_ automatic segmentation _/entity9_ .	COMPARE entity1 entity2
This paper describes a recently collected _entity1_ _P_ spoken language corpus _/entity1_ for the _entity2_ _C_ ATIS ( Air Travel Information System ) domain _/entity2_ . This data collection effort has been co-ordinated by _entity3_ MADCOW ( Multi-site ATIS Data COllection Working group ) _/entity3_ . We summarize the motivation for this effort , the goals , the implementation of a _entity4_ multi-site data collection paradigm _/entity4_ , and the accomplishments of _entity5_ MADCOW _/entity5_ in monitoring the _entity6_ collection _/entity6_ and distribution of 12,000 _entity7_ utterances _/entity7_ of _entity8_ spontaneous speech _/entity8_ from five sites for use in a _entity9_ multi-site common evaluation of speech , natural language and spoken language _/entity9_ .	NONE entity1 entity2
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ pronoun resolution _/entity2_ in _entity3_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ features _/entity6_ designed for _entity7_ pronoun resolution _/entity7_ in _entity8_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ _P_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ _C_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	NONE entity10 entity11
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ _C_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ _P_ transliteration accuracy _/entity12_ significantly .	NONE entity12 entity9
This paper proposes that _entity1_ sentence analysis _/entity1_ should be treated as _entity2_ defeasible reasoning _/entity2_ , and presents such a treatment for _entity3_ Japanese sentence analyses _/entity3_ using an _entity4_ argumentation system _/entity4_ by Konolige , which is a _entity5_ formalization _/entity5_ of _entity6_ defeasible reasoning _/entity6_ , that includes _entity7_ _C_ arguments _/entity7_ and _entity8_ defeat rules _/entity8_ that capture _entity9_ _P_ defeasibility _/entity9_ .	NONE entity9 entity7
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ _P_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ _C_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity6 entity8
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ _P_ in _/entity2_ comparable , non-parallel corpora _entity3_ _C_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity2 entity3
Sources of _entity1_ training data _/entity1_ suitable for _entity2_ language modeling _/entity2_ of _entity3_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ _P_ data _/entity10_ by using _entity11_ _C_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	NONE entity10 entity11
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ _P_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ _C_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity2 entity5
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ _C_ surface forms _/entity10_ can be generated with _entity11_ _P_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	USAGE entity11 entity10
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ _C_ formatted weather data _/entity2_ . Such _entity3_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ _P_ stereotyped text structure _/entity5_ . _entity6_ RAREAS _/entity6_ draws on several kinds of _entity7_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ bilingual or multi-lingual texts _/entity9_ .	NONE entity5 entity2
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ _P_ words _/entity4_ and _entity5_ _C_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity4 entity5
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ language independent modules _/entity3_ of the _entity4_ _C_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ _P_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity5 entity4
Using _entity1_ natural language processing _/entity1_ , we carried out a trend survey on _entity2_ Japanese natural language processing studies _/entity2_ that have been done over the last ten years . We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas . This paper is useful for both recognizing trends in _entity3_ _C_ Japanese NLP _/entity3_ and constructing a method of supporting trend surveys using _entity4_ _P_ NLP _/entity4_ .	NONE entity4 entity3
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ _P_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ _C_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity28 entity30
_entity1_ Link detection _/entity1_ has been regarded as a core technology for the _entity2_ Topic Detection and Tracking tasks _/entity2_ of _entity3_ new event detection _/entity3_ . In this paper we formulate _entity4_ story link detection _/entity4_ and _entity5_ new event detection _/entity5_ as _entity6_ information retrieval task _/entity6_ and hypothesize on the impact of _entity7_ _P_ precision _/entity7_ and _entity8_ recall _/entity8_ on both systems . Motivated by these arguments , we introduce a number of new performance enhancing techniques including _entity9_ part of speech tagging _/entity9_ , new _entity10_ _C_ similarity measures _/entity10_ and expanded _entity11_ stop lists _/entity11_ . Experimental results validate our hypothesis .	NONE entity7 entity10
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ _C_ Chinese punctuation marks _/entity6_ in _entity7_ news commentary texts _/entity7_ : _entity8_ Colon _/entity8_ , _entity9_ _P_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity9 entity6
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ _C_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ _P_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity8 entity6
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ _C_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ _P_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity41 entity39
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ _P_ explicitation _/entity12_ or _entity13_ _C_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity12 entity13
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ _C_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ _P_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity5 entity3
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ _C_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ _P_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity14 entity12
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ _C_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ _P_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity13 entity10
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ _P_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ _C_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity4 entity6
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ _P_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ _C_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity4 entity6
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ _P_ phrases _/entity4_ from a _entity5_ word-aligned corpora _/entity5_ is proposed . A _entity6_ statistical translation model _/entity6_ is also presented that deals such _entity7_ _C_ phrases _/entity7_ , as well as a _entity8_ training method _/entity8_ based on the maximization of _entity9_ translation accuracy _/entity9_ , as measured with the _entity10_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity4 entity7
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ _C_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ _P_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity19 entity17
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ _C_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ _P_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity23 entity20
We present the first application of the _entity1_ _C_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ _P_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity3 entity1
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ LCS meaning components _/entity6_ . Our _entity7_ _P_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ _C_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity7 entity9
A system is described for acquiring a _entity1_ context-sensitive , phrase structure grammar _/entity1_ which is applied by a _entity2_ best-path , bottom-up , deterministic parser _/entity2_ . The _entity3_ grammar _/entity3_ was based on _entity4_ English news stories _/entity4_ and a high degree of success in _entity5_ parsing _/entity5_ is reported . Overall , this research concludes that _entity6_ CSG _/entity6_ is a computationally and conceptually tractable approach to the construction of _entity7_ _P_ phrase structure grammar _/entity7_ for _entity8_ _C_ news story text _/entity8_ .	NONE entity7 entity8
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ _P_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ _C_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity7 entity9
This paper presents necessary and sufficient conditions for the use of _entity1_ demonstrative expressions _/entity1_ in _entity2_ English _/entity2_ and discusses implications for current _entity3_ discourse processing algorithms _/entity3_ . We examine a broad range of _entity4_ texts _/entity4_ to show how the distribution of _entity5_ _P_ demonstrative forms and functions _/entity5_ is _entity6_ genre dependent _/entity6_ . This research is part of a larger study of _entity7_ anaphoric expressions _/entity7_ , the results of which will be incorporated into a _entity8_ _C_ natural language generation system _/entity8_ .	NONE entity5 entity8
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ _C_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ _P_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity7 entity4
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ _P_ user _/entity6_ . The issue of _entity7_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ _C_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity6 entity9
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ _P_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ _C_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity11 entity12
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ _C_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ _P_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity6 entity3
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ _P_ encyclopedias _/entity4_ and _entity5_ _C_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity4 entity5
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ _P_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ _C_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity28 entity30
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ _P_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ _C_ lexical gap _/entity22_ .	NONE entity19 entity22
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ _C_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ _P_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity20 entity18
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ _P_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ _C_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity17 entity20
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ _P_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ _C_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity5 entity8
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ _C_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ _P_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity9 entity7
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ _P_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ _C_ sense-tagged training data _/entity4_ from _entity5_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ nouns _/entity6_ in the _entity7_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ WSD programs _/entity14_ .	NONE entity3 entity4
In order to boost the _entity1_ translation quality _/entity1_ of _entity2_ EBMT _/entity2_ based on a small-sized _entity3_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ language model _/entity5_ of an in-domain _entity6_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ _C_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ BLEU score _/entity9_ and the _entity10_ _P_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity10 entity7
Towards deep analysis of _entity1_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ class-oriented framework _/entity2_ for collecting _entity3_ paraphrase examples _/entity3_ , in which _entity4_ _C_ sentential paraphrases _/entity4_ are collected for each _entity5_ _P_ paraphrase class _/entity5_ separately by means of _entity6_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ cost-efficiency _/entity9_ , _entity10_ exhaustiveness _/entity10_ , and _entity11_ reliability _/entity11_ .	NONE entity5 entity4
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ _C_ SILVA _/entity16_ , a _entity17_ _P_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity17 entity16
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ _C_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ _P_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity6 entity4
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ duration model _/entity3_ has reduced the _entity4_ error rate _/entity4_ by about 10 % in both _entity5_ triphone and semiphone systems _/entity5_ . A new _entity6_ _P_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ _C_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity6 entity8
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ _P_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ _C_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity2 entity4
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ _C_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ _P_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity9 entity7
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ _C_ pronominalization _/entity5_ , _entity6_ _P_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity6 entity5
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ _P_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ _C_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity7 entity10
This paper describes three relatively _entity1_ domain-independent capabilities _/entity1_ recently added to the _entity2_ Paramax spoken language understanding system _/entity2_ : _entity3_ non-monotonic reasoning _/entity3_ , _entity4_ implicit reference resolution _/entity4_ , and _entity5_ database query paraphrase _/entity5_ . In addition , we discuss the results of the _entity6_ February 1992 ATIS benchmark tests _/entity6_ . We describe a variation on the _entity7_ _P_ standard evaluation metric _/entity7_ which provides a more tightly controlled measure of progress . Finally , we briefly describe an experiment which we have done in extending the _entity8_ _C_ n-best speech/language integration architecture _/entity8_ to improving _entity9_ OCR _/entity9_ _entity10_ accuracy _/entity10_ .	NONE entity7 entity8
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ _C_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ _P_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity13 entity11
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ _C_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ _P_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity19 entity16
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ _C_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ _P_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity27 entity24
Past work of generating _entity1_ referring expressions _/entity1_ mainly utilized attributes of _entity2_ _C_ objects _/entity2_ and _entity3_ binary relations _/entity3_ between _entity4_ objects _/entity4_ . However , such an approach does not work well when there is no distinctive attribute among _entity5_ _P_ objects _/entity5_ . To overcome this limitation , this paper proposes a method utilizing the perceptual groups of _entity6_ objects _/entity6_ and _entity7_ n-ary relations _/entity7_ among them . The key is to identify groups of _entity8_ objects _/entity8_ that are naturally recognized by humans . We conducted psychological experiments with 42 subjects to collect _entity9_ referring expressions _/entity9_ in such situations , and built a _entity10_ generation algorithm _/entity10_ based on the results . The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions .	NONE entity5 entity2
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ _C_ human-computer interaction _/entity6_ . We analyzed _entity7_ eye gaze _/entity7_ , _entity8_ _P_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity8 entity6
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ _C_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ _P_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity4 entity2
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ _C_ proportion problem _/entity11_ and given a solution from a _entity12_ _P_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	TOPIC entity12 entity11
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ _C_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ _P_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity20 entity17
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ _C_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ _P_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity13 entity11
We provide a _entity1_ _P_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ _C_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity1 entity3
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ _P_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ _C_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity5 entity8
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ duration model _/entity3_ has reduced the _entity4_ _P_ error rate _/entity4_ by about 10 % in both _entity5_ triphone and semiphone systems _/entity5_ . A new _entity6_ _C_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity4 entity6
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ _P_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ _C_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity20 entity22
_entity1_ Systemic grammar _/entity1_ has been used for _entity2_ _C_ AI text generation _/entity2_ work in the past , but the _entity3_ implementations _/entity3_ have tended be ad hoc or inefficient . This paper presents an approach to systemic _entity4_ _P_ text generation _/entity4_ where _entity5_ AI problem solving techniques _/entity5_ are applied directly to an unadulterated _entity6_ systemic grammar _/entity6_ . This _entity7_ approach _/entity7_ is made possible by a special relationship between _entity8_ systemic grammar _/entity8_ and _entity9_ problem solving _/entity9_ : both are organized primarily as choosing from alternatives . The result is simple , efficient _entity10_ text generation _/entity10_ firmly based in a _entity11_ linguistic theory _/entity11_ .	NONE entity4 entity2
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ _P_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ _C_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity6 entity8
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ _C_ wordnets _/entity10_ are aligned to the _entity11_ _P_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity11 entity10
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ _C_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ _P_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity6 entity4
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ _P_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ _C_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity12 entity14
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ _P_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ _C_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity9 entity12
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ _P_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ _C_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity3 entity5
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ _C_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ _P_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity30 entity29
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ _C_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ _P_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity14 entity12
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ _P_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ _C_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity15 entity17
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ electronic discussions _/entity5_ that influence the _entity6_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ _P_ influences _/entity8_ . We tested the _entity9_ clustering and filtering processes _/entity9_ on _entity10_ _C_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity8 entity10
This paper discusses a _entity1_ decision-tree approach _/entity1_ to the problem of assigning _entity2_ probabilities _/entity2_ to _entity3_ words _/entity3_ following a given _entity4_ _C_ text _/entity4_ . In contrast with previous _entity5_ _P_ decision-tree language model attempts _/entity5_ , an algorithm for selecting _entity6_ nearly optimal questions _/entity6_ is considered . The model is to be tested on a standard task , _entity7_ The Wall Street Journal _/entity7_ , allowing a fair comparison with the well-known _entity8_ tri-gram model _/entity8_ .	NONE entity5 entity4
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ _P_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ _C_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	NONE entity8 entity11
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ _C_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ _P_ statistical ranking _/entity6_ than a previous approach to _entity7_ statistical generation _/entity7_ . An efficient _entity8_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ lattice-based approach _/entity9_ .	NONE entity6 entity3
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ _P_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ _C_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity26 entity28
In this paper we present a _entity1_ _C_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ _P_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ states _/entity9_ which handles some additional cases of multiple _entity10_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity3 entity1
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ _P_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ _C_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity9 entity12
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ _C_ training _/entity15_ to adapt to new _entity16_ _P_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity16 entity15
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ _P_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ _C_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity5 entity8
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ _P_ permutation rules _/entity7_ . We have given examples of its _entity8_ _C_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity7 entity8
We present a _entity1_ practically unsupervised learning method _/entity1_ to produce _entity2_ single-snippet answers _/entity2_ to _entity3_ definition questions _/entity3_ in _entity4_ question answering systems _/entity4_ that supplement _entity5_ Web search engines _/entity5_ . The method exploits _entity6_ on-line encyclopedias and dictionaries _/entity6_ to generate automatically an arbitrarily large number of _entity7_ _C_ positive and negative definition examples _/entity7_ , which are then used to train an _entity8_ _P_ svm _/entity8_ to separate the two classes . We show experimentally that the proposed method is viable , that it outperforms the alternative of training the _entity9_ system _/entity9_ on _entity10_ questions _/entity10_ and _entity11_ news articles from trec _/entity11_ , and that it helps the _entity12_ search engine _/entity12_ handle _entity13_ definition questions _/entity13_ significantly better .	NONE entity8 entity7
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ _P_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ _C_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity18 entity21
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ news commentary texts _/entity7_ : _entity8_ Colon _/entity8_ , _entity9_ Dash _/entity9_ , _entity10_ _P_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ Question Mark _/entity12_ , and _entity13_ _C_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity10 entity13
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ sense-tagged training data _/entity4_ from _entity5_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ nouns _/entity6_ in the _entity7_ _C_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ _P_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ WSD programs _/entity14_ .	NONE entity9 entity7
_entity1_ Graph unification _/entity1_ remains the most expensive part of _entity2_ unification-based grammar parsing _/entity2_ . We focus on one speed-up element in the design of _entity3_ unification algorithms _/entity3_ : avoidance of _entity4_ copying _/entity4_ of _entity5_ unmodified subgraphs _/entity5_ . We propose a method of attaining such a design through a method of _entity6_ structure-sharing _/entity6_ which avoids _entity7_ log ( d ) overheads _/entity7_ often associated with _entity8_ _P_ structure-sharing of graphs _/entity8_ without any use of costly _entity9_ dependency pointers _/entity9_ . The proposed scheme eliminates _entity10_ _C_ redundant copying _/entity10_ while maintaining the _entity11_ quasi-destructive scheme 's ability _/entity11_ to avoid _entity12_ over copying _/entity12_ and _entity13_ early copying _/entity13_ combined with its ability to handle _entity14_ cyclic structures _/entity14_ without algorithmic additions .	NONE entity8 entity10
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ _P_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ _C_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity10 entity13
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ _C_ events _/entity2_ are defined as _entity3_ event terms _/entity3_ and _entity4_ _P_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ documents _/entity9_ . Experimental results are encouraging .	NONE entity4 entity2
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ _C_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ _P_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity9 entity7
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ rules _/entity6_ between _entity7_ sentences _/entity7_ , within _entity8_ _P_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ _C_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity8 entity10
_entity1_ Pipelined Natural Language Generation ( NLG ) systems _/entity1_ have grown increasingly complex as _entity2_ architectural modules _/entity2_ were added to support _entity3_ language functionalities _/entity3_ such as _entity4_ referring expressions _/entity4_ , _entity5_ lexical choice _/entity5_ , and _entity6_ revision _/entity6_ . This has given rise to discussions about the relative placement of these new _entity7_ modules _/entity7_ in the overall _entity8_ _P_ architecture _/entity8_ . Recent work on another aspect of _entity9_ _C_ multi-paragraph text _/entity9_ , _entity10_ discourse markers _/entity10_ , indicates it is time to consider where a _entity11_ discourse marker insertion algorithm _/entity11_ fits in . We present examples which suggest that in a _entity12_ pipelined NLG architecture _/entity12_ , the best approach is to strongly tie it to a _entity13_ revision component _/entity13_ . Finally , we evaluate the approach in a working _entity14_ multi-page system _/entity14_ .	NONE entity8 entity9
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ system combination _/entity3_ for _entity4_ _P_ unsupervised WSD _/entity4_ . We investigate several _entity5_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ _C_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity4 entity7
This paper describes a method of _entity1_ interactively visualizing and directing the process _/entity1_ of _entity2_ translating a sentence _/entity2_ . The method allows a _entity3_ user _/entity3_ to explore a _entity4_ model _/entity4_ of _entity5_ syntax-based statistical machine translation ( MT ) _/entity5_ , to understand the _entity6_ model _/entity6_ 's strengths and weaknesses , and to compare it to other _entity7_ MT systems _/entity7_ . Using this _entity8_ visualization method _/entity8_ , we can find and address conceptual and practical problems in an _entity9_ _P_ MT system _/entity9_ . In our demonstration at _entity10_ _C_ ACL _/entity10_ , new _entity11_ users _/entity11_ of our tool will drive a _entity12_ syntax-based decoder _/entity12_ for themselves .	NONE entity9 entity10
Methods developed for _entity1_ spelling correction _/entity1_ for _entity2_ languages _/entity2_ like _entity3_ English _/entity3_ ( see the review by Kukich ( Kukich , 1992 ) ) are not readily applicable to _entity4_ agglutinative languages _/entity4_ . This poster presents an approach to _entity5_ spelling correction _/entity5_ in _entity6_ agglutinative languages _/entity6_ that is based on _entity7_ two-level morphology _/entity7_ and a _entity8_ _C_ dynamic-programming based search algorithm _/entity8_ . After an overview of our approach , we present results from experiments with _entity9_ spelling correction _/entity9_ in _entity10_ _P_ Turkish _/entity10_ .	NONE entity10 entity8
This paper explores the issue of using different _entity1_ co-occurrence similarities _/entity1_ between _entity2_ _C_ terms _/entity2_ for separating _entity3_ _P_ query terms _/entity3_ that are useful for _entity4_ retrieval _/entity4_ from those that are harmful . The hypothesis under examination is that _entity5_ useful terms _/entity5_ tend to be more similar to each other than to other _entity6_ query terms _/entity6_ . Preliminary experiments with similarities computed using _entity7_ first-order and second-order co-occurrence _/entity7_ seem to confirm the hypothesis . _entity8_ Term similarities _/entity8_ could then be used for determining which _entity9_ query terms _/entity9_ are useful and best reflect the user 's information need . A possible application would be to use this source of evidence for tuning the _entity10_ weights _/entity10_ of the _entity11_ query terms _/entity11_ .	NONE entity3 entity2
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ _P_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ _C_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity5 entity7
A new approach for _entity1_ Interactive Machine Translation _/entity1_ where the _entity2_ author _/entity2_ interacts during the creation or the modification of the _entity3_ document _/entity3_ is proposed . The explanation of an _entity4_ ambiguity _/entity4_ or an error for the purposes of correction does not use any concepts of the underlying _entity5_ _P_ linguistic theory _/entity5_ : it is a reformulation of the erroneous or ambiguous _entity6_ sentence _/entity6_ . The interaction is limited to the analysis step of the _entity7_ _C_ translation process _/entity7_ . This paper presents a new _entity8_ interactive disambiguation scheme _/entity8_ based on the _entity9_ paraphrasing _/entity9_ of a _entity10_ parser _/entity10_ 's multiple output . Some examples of _entity11_ paraphrasing _/entity11_ ambiguous _entity12_ sentences _/entity12_ are presented .	NONE entity5 entity7
This paper presents a _entity1_ machine learning approach _/entity1_ to _entity2_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ _C_ corpus-based sample _/entity5_ and formulate them as _entity6_ _P_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ clauses _/entity7_ to create a set of _entity8_ domain independent features _/entity8_ to annotate an _entity9_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity6 entity5
In the past the evaluation of _entity1_ machine translation systems _/entity1_ has focused on single system evaluations because there were only few systems available . But now there are several commercial systems for the same _entity2_ language pair _/entity2_ . This requires new methods of comparative evaluation . In the paper we propose a _entity3_ black-box method _/entity3_ for comparing the _entity4_ lexical coverage _/entity4_ of _entity5_ MT systems _/entity5_ . The method is based on lists of _entity6_ words _/entity6_ from different _entity7_ _P_ frequency classes _/entity7_ . It is shown how these _entity8_ _C_ word lists _/entity8_ can be compiled and used for testing . We also present the results of using our method on 6 _entity9_ MT systems _/entity9_ that translate between _entity10_ English _/entity10_ and _entity11_ German _/entity11_ .	NONE entity7 entity8
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ _C_ verbs _/entity15_ is used to filter out irrelevant _entity16_ _P_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity16 entity15
_entity1_ Graph unification _/entity1_ remains the most expensive part of _entity2_ unification-based grammar parsing _/entity2_ . We focus on one speed-up element in the design of _entity3_ unification algorithms _/entity3_ : avoidance of _entity4_ copying _/entity4_ of _entity5_ unmodified subgraphs _/entity5_ . We propose a method of attaining such a design through a method of _entity6_ _P_ structure-sharing _/entity6_ which avoids _entity7_ _C_ log ( d ) overheads _/entity7_ often associated with _entity8_ structure-sharing of graphs _/entity8_ without any use of costly _entity9_ dependency pointers _/entity9_ . The proposed scheme eliminates _entity10_ redundant copying _/entity10_ while maintaining the _entity11_ quasi-destructive scheme 's ability _/entity11_ to avoid _entity12_ over copying _/entity12_ and _entity13_ early copying _/entity13_ combined with its ability to handle _entity14_ cyclic structures _/entity14_ without algorithmic additions .	NONE entity6 entity7
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ _P_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ _C_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity29 entity32
A novel _entity1_ bootstrapping approach _/entity1_ to _entity2_ Named Entity ( NE ) tagging _/entity2_ using _entity3_ concept-based seeds _/entity3_ and _entity4_ _C_ successive learners _/entity4_ is presented . This approach only requires a few _entity5_ common noun _/entity5_ or _entity6_ pronoun _/entity6_ _entity7_ _P_ seeds _/entity7_ that correspond to the _entity8_ concept _/entity8_ for the targeted _entity9_ NE _/entity9_ , e.g . he/she/man/woman for _entity10_ PERSON NE _/entity10_ . The _entity11_ bootstrapping procedure _/entity11_ is implemented as training two _entity12_ successive learners _/entity12_ . First , _entity13_ decision list _/entity13_ is used to learn the _entity14_ parsing-based NE rules _/entity14_ . Then , a _entity15_ Hidden Markov Model _/entity15_ is trained on a _entity16_ corpus _/entity16_ automatically tagged by the first _entity17_ learner _/entity17_ . The resulting _entity18_ NE system _/entity18_ approaches _entity19_ supervised NE _/entity19_ performance for some _entity20_ NE types _/entity20_ .	NONE entity7 entity4
This paper proposes a practical approach employing _entity1_ n-gram models _/entity1_ and _entity2_ error-correction rules _/entity2_ for _entity3_ Thai key prediction _/entity3_ and _entity4_ Thai-English language identification _/entity4_ . The paper also proposes _entity5_ rule-reduction algorithm _/entity5_ applying _entity6_ mutual information _/entity6_ to reduce the _entity7_ error-correction rules _/entity7_ . Our algorithm reported more than 99 % _entity8_ _C_ accuracy _/entity8_ in both _entity9_ language identification _/entity9_ and _entity10_ _P_ key prediction _/entity10_ .	NONE entity10 entity8
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ _P_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ _C_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity2 entity3
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ _P_ path _/entity7_ between the same two _entity8_ _C_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity7 entity8
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ _P_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ _C_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity4 entity7
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ _P_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ _C_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity10 entity12
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ _C_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ _P_ arguments _/entity21_ have foundered on this problem .	NONE entity21 entity19
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ _C_ support vector machine _/entity13_ uses these _entity14_ _P_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	USAGE entity14 entity13
Currently several _entity1_ _P_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ _C_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity1 entity2
This paper considers the problem of automatic assessment of _entity1_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ discourse representation _/entity8_ supports the effective learning of a _entity9_ _P_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ _C_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity9 entity11
This paper discusses a _entity1_ decision-tree approach _/entity1_ to the problem of assigning _entity2_ probabilities _/entity2_ to _entity3_ words _/entity3_ following a given _entity4_ text _/entity4_ . In contrast with previous _entity5_ _C_ decision-tree language model attempts _/entity5_ , an algorithm for selecting _entity6_ nearly optimal questions _/entity6_ is considered . The model is to be tested on a standard task , _entity7_ The Wall Street Journal _/entity7_ , allowing a fair comparison with the well-known _entity8_ _P_ tri-gram model _/entity8_ .	NONE entity8 entity5
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ _C_ understanding _/entity14_ results and resolving the _entity15_ _P_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity15 entity14
Previous research has demonstrated the utility of _entity1_ clustering _/entity1_ in inducing _entity2_ semantic verb classes _/entity2_ from undisambiguated _entity3_ _C_ corpus data _/entity3_ . We describe a new approach which involves clustering _entity4_ subcategorization frame ( SCF ) _/entity4_ distributions using the _entity5_ Information Bottleneck _/entity5_ and _entity6_ _P_ nearest neighbour _/entity6_ methods . In contrast to previous work , we particularly focus on clustering _entity7_ polysemic verbs _/entity7_ . A novel _entity8_ evaluation scheme _/entity8_ is proposed which accounts for the effect of _entity9_ polysemy _/entity9_ on the _entity10_ clusters _/entity10_ , offering us a good insight into the potential and limitations of _entity11_ semantically classifying _/entity11_ _entity12_ undisambiguated SCF data _/entity12_ .	NONE entity6 entity3
We present a framework for _entity1_ word alignment _/entity1_ based on _entity2_ log-linear models _/entity2_ . All _entity3_ knowledge sources _/entity3_ are treated as _entity4_ feature functions _/entity4_ , which depend on the _entity5_ source langauge sentence _/entity5_ , the _entity6_ _P_ target language sentence _/entity6_ and possible additional variables . _entity7_ Log-linear models _/entity7_ allow _entity8_ _C_ statistical alignment models _/entity8_ to be easily extended by incorporating _entity9_ syntactic information _/entity9_ . In this paper , we use _entity10_ IBM Model 3 alignment probabilities _/entity10_ , _entity11_ POS correspondence _/entity11_ , and _entity12_ bilingual dictionary coverage _/entity12_ as _entity13_ features _/entity13_ . Our experiments show that _entity14_ log-linear models _/entity14_ significantly outperform _entity15_ IBM translation models _/entity15_ .	NONE entity6 entity8
In this paper we present a _entity1_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ _C_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ _P_ states _/entity9_ which handles some additional cases of multiple _entity10_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity9 entity6
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ _C_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ _P_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity19 entity16
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ _C_ news commentary texts _/entity7_ : _entity8_ _P_ Colon _/entity8_ , _entity9_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity8 entity7
Although adequate models of _entity1_ human language _/entity1_ for _entity2_ syntactic analysis _/entity2_ and _entity3_ semantic interpretation _/entity3_ are of at least _entity4_ context-free complexity _/entity4_ , for applications such as _entity5_ speech processing _/entity5_ in which speed is important _entity6_ finite-state models _/entity6_ are often preferred . These requirements may be reconciled by using the more complex _entity7_ _P_ grammar _/entity7_ to automatically derive a _entity8_ finite-state approximation _/entity8_ which can then be used as a filter to guide _entity9_ speech recognition _/entity9_ or to reject many hypotheses at an early stage of processing . A method is presented here for calculating such _entity10_ _C_ finite-state approximations _/entity10_ from _entity11_ context-free grammars _/entity11_ . It is essentially different from the algorithm introduced by Pereira and Wright ( 1991 ; 1996 ) , is faster in some cases , and has the advantage of being open-ended and adaptable .	NONE entity7 entity10
_entity1_ Large-scale natural language generation _/entity1_ requires the integration of vast amounts of _entity2_ _C_ knowledge _/entity2_ : lexical , grammatical , and conceptual . A _entity3_ _P_ robust generator _/entity3_ must be able to operate well even when pieces of _entity4_ knowledge _/entity4_ are missing . It must also be robust against _entity5_ incomplete or inaccurate inputs _/entity5_ . To attack these problems , we have built a _entity6_ hybrid generator _/entity6_ , in which gaps in _entity7_ symbolic knowledge _/entity7_ are filled by _entity8_ statistical methods _/entity8_ . We describe algorithms and show experimental results . We also discuss how the _entity9_ hybrid generation model _/entity9_ can be used to simplify current _entity10_ generators _/entity10_ and enhance their _entity11_ portability _/entity11_ , even when perfect _entity12_ knowledge _/entity12_ is in principle obtainable .	NONE entity3 entity2
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ _P_ model _/entity6_ in which descriptions of _entity7_ _C_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity6 entity7
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ _C_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ _P_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity21 entity19
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ _P_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ _C_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity8 entity9
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ _P_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ _C_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity7 entity10
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ _P_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ _C_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity10 entity12
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ _P_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ _C_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity9 entity11
In this paper , we introduce a _entity1_ generative probabilistic optical character recognition ( OCR ) model _/entity1_ that describes an end-to-end process in the _entity2_ noisy channel framework _/entity2_ , progressing from generation of _entity3_ true text _/entity3_ through its transformation into the _entity4_ noisy output _/entity4_ of an _entity5_ OCR system _/entity5_ . The _entity6_ model _/entity6_ is designed for use in _entity7_ error correction _/entity7_ , with a focus on _entity8_ post-processing _/entity8_ the _entity9_ output _/entity9_ of black-box _entity10_ OCR systems _/entity10_ in order to make it more useful for _entity11_ NLP tasks _/entity11_ . We present an implementation of the _entity12_ model _/entity12_ based on _entity13_ _C_ finite-state models _/entity13_ , demonstrate the _entity14_ model _/entity14_ 's ability to significantly reduce _entity15_ _P_ character and word error rate _/entity15_ , and provide evaluation results involving _entity16_ automatic extraction _/entity16_ of _entity17_ translation lexicons _/entity17_ from _entity18_ printed text _/entity18_ .	NONE entity15 entity13
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ statistical approaches _/entity6_ , which resolve _entity7_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ _C_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ _P_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity10 entity9
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ _P_ minimal DFA _/entity11_ , then identify _entity12_ _C_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity11 entity12
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ _P_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ _C_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity16 entity18
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ _P_ classifiers _/entity4_ to predict _entity5_ _C_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity4 entity5
We focus on the problem of building large _entity1_ _P_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ _C_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ LCS meaning components _/entity6_ . Our _entity7_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity1 entity3
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ _C_ extraction _/entity16_ of _entity17_ _P_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity17 entity16
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ _P_ content _/entity14_ that must be included in them , the _entity15_ _C_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity14 entity15
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ _C_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ _P_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity5 entity4
We present the first known _entity1_ _C_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ _P_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity4 entity1
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ character sets _/entity6_ should be treated differently and the changes between _entity7_ character types _/entity7_ are very important because _entity8_ Japanese script _/entity8_ has both _entity9_ ideograms _/entity9_ like _entity10_ Chinese _/entity10_ ( _entity11_ kanji _/entity11_ ) and _entity12_ phonograms _/entity12_ like _entity13_ _C_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ _P_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity16 entity13
We discuss _entity1_ maximum a posteriori estimation _/entity1_ of _entity2_ continuous density hidden Markov models ( CDHMM ) _/entity2_ . The classical _entity3_ _C_ MLE reestimation algorithms _/entity3_ , namely the _entity4_ forward-backward algorithm _/entity4_ and the _entity5_ _P_ segmental k-means algorithm _/entity5_ , are expanded and _entity6_ reestimation formulas _/entity6_ are given for _entity7_ HMM with Gaussian mixture observation densities _/entity7_ . Because of its adaptive nature , _entity8_ Bayesian learning _/entity8_ serves as a unified approach for the following four _entity9_ speech recognition _/entity9_ applications , namely _entity10_ parameter smoothing _/entity10_ , _entity11_ speaker adaptation _/entity11_ , _entity12_ speaker group modeling _/entity12_ and _entity13_ corrective training _/entity13_ . New experimental results on all four applications are provided to show the effectiveness of the _entity14_ MAP estimation approach _/entity14_ .	NONE entity5 entity3
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ human-computer interaction _/entity6_ . We analyzed _entity7_ eye gaze _/entity7_ , _entity8_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ _C_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ _P_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity13 entity10
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ _P_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ _C_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity34 entity36
Reducing _entity1_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ LM pruning _/entity3_ . They are probability , _entity4_ rank _/entity4_ , and _entity5_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ pruning criteria _/entity6_ in a real application of _entity7_ Chinese text input _/entity7_ in terms of _entity8_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ _P_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ _C_ CER _/entity13_ .	NONE entity11 entity13
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ pronoun resolution _/entity2_ in _entity3_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ _P_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ _C_ features _/entity6_ designed for _entity7_ pronoun resolution _/entity7_ in _entity8_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	NONE entity5 entity6
We provide a unified account of _entity1_ sentence-level and text-level anaphora _/entity1_ within the framework of a _entity2_ _C_ dependency-based grammar model _/entity2_ . Criteria for _entity3_ anaphora resolution _/entity3_ within _entity4_ sentence boundaries _/entity4_ rephrase major concepts from _entity5_ _P_ GB 's binding theory _/entity5_ , while those for _entity6_ text-level anaphora _/entity6_ incorporate an adapted version of a _entity7_ Grosz-Sidner-style focus model _/entity7_ .	NONE entity5 entity2
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ _C_ mechanical translation _/entity46_ , _entity47_ _P_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity47 entity46
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ _P_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ _C_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity17 entity19
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ _C_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ _P_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity17 entity14
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ _C_ MT system _/entity16_ using the _entity17_ _P_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity17 entity16
_entity1_ Statistical language modeling _/entity1_ remains a challenging task , in particular for _entity2_ morphologically rich languages _/entity2_ . Recently , new approaches based on _entity3_ factored language models _/entity3_ have been developed to address this problem . These _entity4_ _P_ models _/entity4_ provide principled ways of including additional _entity5_ conditioning variables _/entity5_ other than the _entity6_ preceding words _/entity6_ , such as _entity7_ _C_ morphological or syntactic features _/entity7_ . However , the number of possible choices for _entity8_ model parameters _/entity8_ creates a _entity9_ large space of models _/entity9_ that can not be searched exhaustively . This paper presents an _entity10_ entirely data-driven model selection procedure _/entity10_ based on _entity11_ genetic search _/entity11_ , which is shown to outperform both _entity12_ knowledge-based and random selection procedures _/entity12_ on two different _entity13_ language modeling tasks _/entity13_ ( _entity14_ Arabic _/entity14_ and _entity15_ Turkish _/entity15_ ) .	NONE entity4 entity7
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ _P_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ _C_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity15 entity17
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ _C_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ _P_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity9 entity7
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ _P_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ _C_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity4 entity7
In this paper we present a novel , customizable : _entity1_ _P_ IE paradigm _/entity1_ that takes advantage of _entity2_ _C_ predicate-argument structures _/entity2_ . We also introduce a new way of automatically identifying _entity3_ predicate argument structures _/entity3_ , which is central to our _entity4_ IE paradigm _/entity4_ . It is based on : ( 1 ) an extended set of _entity5_ features _/entity5_ ; and ( 2 ) _entity6_ inductive decision tree learning _/entity6_ . The experimental results prove our claim that accurate _entity7_ predicate-argument structures _/entity7_ enable high quality _entity8_ IE _/entity8_ results .	NONE entity1 entity2
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ _C_ associative relationship _/entity13_ , which we call _entity14_ _P_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity14 entity13
This paper presents an _entity1_ _P_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ _C_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity1 entity3
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ _P_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ _C_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity21 entity23
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ _C_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ _P_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity12 entity9
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ _P_ Korean and English language pairs _/entity18_ , we show that our _entity19_ _C_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity18 entity19
We argue in favor of the the use of _entity1_ labeled directed graph _/entity1_ to represent various types of _entity2_ linguistic structures _/entity2_ , and illustrate how this allows one to view _entity3_ NLP tasks _/entity3_ as _entity4_ graph transformations _/entity4_ . We present a general method for learning such _entity5_ _C_ transformations _/entity5_ from an _entity6_ annotated corpus _/entity6_ and describe experiments with two applications of the method : _entity7_ identification of non-local depenencies _/entity7_ ( using _entity8_ _P_ Penn Treebank data _/entity8_ ) and _entity9_ semantic role labeling _/entity9_ ( using _entity10_ Proposition Bank data _/entity10_ ) .	NONE entity8 entity5
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ _C_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ _P_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity15 entity13
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ _C_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ _P_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity6 entity4
The _entity1_ _C_ TAP-XL Automated Analyst 's Assistant _/entity1_ is an application designed to help an _entity2_ _P_ English _/entity2_ -speaking analyst write a _entity3_ topical report _/entity3_ , culling information from a large inflow of _entity4_ multilingual , multimedia data _/entity4_ . It gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other _entity5_ languages _/entity5_ by leveraging _entity6_ human language technology _/entity6_ .	NONE entity2 entity1
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ pronoun resolution _/entity2_ in _entity3_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ features _/entity6_ designed for _entity7_ _P_ pronoun resolution _/entity7_ in _entity8_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ _C_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	NONE entity7 entity10
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ _P_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ _C_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity21 entity24
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ _C_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ _P_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity15 entity14
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ _P_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ _C_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity4 entity7
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ sublanguage _/entity3_ , is proposed for identifying _entity4_ _C_ unknown words _/entity4_ , especially _entity5_ _P_ personal names _/entity5_ , in _entity6_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ adaptive dynamic word formation _/entity8_ , _entity9_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ WI systems _/entity12_ without the _entity13_ name identification _/entity13_ capability .	NONE entity5 entity4
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ _C_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ _P_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity19 entity17
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ _C_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ _P_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity11 entity8
In this paper , we introduce a _entity1_ generative probabilistic optical character recognition ( OCR ) model _/entity1_ that describes an end-to-end process in the _entity2_ noisy channel framework _/entity2_ , progressing from generation of _entity3_ true text _/entity3_ through its transformation into the _entity4_ noisy output _/entity4_ of an _entity5_ _C_ OCR system _/entity5_ . The _entity6_ model _/entity6_ is designed for use in _entity7_ error correction _/entity7_ , with a focus on _entity8_ _P_ post-processing _/entity8_ the _entity9_ output _/entity9_ of black-box _entity10_ OCR systems _/entity10_ in order to make it more useful for _entity11_ NLP tasks _/entity11_ . We present an implementation of the _entity12_ model _/entity12_ based on _entity13_ finite-state models _/entity13_ , demonstrate the _entity14_ model _/entity14_ 's ability to significantly reduce _entity15_ character and word error rate _/entity15_ , and provide evaluation results involving _entity16_ automatic extraction _/entity16_ of _entity17_ translation lexicons _/entity17_ from _entity18_ printed text _/entity18_ .	NONE entity8 entity5
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ _P_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ _C_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity10 entity13
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ _P_ user _/entity5_ 's _entity6_ _C_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity5 entity6
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ _C_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ _P_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity9 entity6
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ _P_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ _C_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity19 entity21
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ _C_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ _P_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity9 entity6
Methods developed for _entity1_ spelling correction _/entity1_ for _entity2_ languages _/entity2_ like _entity3_ English _/entity3_ ( see the review by Kukich ( Kukich , 1992 ) ) are not readily applicable to _entity4_ _C_ agglutinative languages _/entity4_ . This poster presents an approach to _entity5_ spelling correction _/entity5_ in _entity6_ agglutinative languages _/entity6_ that is based on _entity7_ _P_ two-level morphology _/entity7_ and a _entity8_ dynamic-programming based search algorithm _/entity8_ . After an overview of our approach , we present results from experiments with _entity9_ spelling correction _/entity9_ in _entity10_ Turkish _/entity10_ .	NONE entity7 entity4
The paper provides an overview of the research conducted at _entity1_ _C_ LIMSI _/entity1_ in the field of _entity2_ speech processing _/entity2_ , but also in the related areas of _entity3_ Human-Machine Communication _/entity3_ , including _entity4_ _P_ Natural Language Processing _/entity4_ , _entity5_ Non Verbal and Multimodal Communication _/entity5_ . Also presented are the commercial applications of some of the research projects . When applicable , the discussion is placed in the framework of international collaborations .	NONE entity4 entity1
_entity1_ Statistical language modeling _/entity1_ remains a challenging task , in particular for _entity2_ morphologically rich languages _/entity2_ . Recently , new approaches based on _entity3_ factored language models _/entity3_ have been developed to address this problem . These _entity4_ models _/entity4_ provide principled ways of including additional _entity5_ conditioning variables _/entity5_ other than the _entity6_ preceding words _/entity6_ , such as _entity7_ morphological or syntactic features _/entity7_ . However , the number of possible choices for _entity8_ model parameters _/entity8_ creates a _entity9_ large space of models _/entity9_ that can not be searched exhaustively . This paper presents an _entity10_ entirely data-driven model selection procedure _/entity10_ based on _entity11_ genetic search _/entity11_ , which is shown to outperform both _entity12_ knowledge-based and random selection procedures _/entity12_ on two different _entity13_ _P_ language modeling tasks _/entity13_ ( _entity14_ Arabic _/entity14_ and _entity15_ _C_ Turkish _/entity15_ ) .	NONE entity13 entity15
In this paper , we compare the relative effects of _entity1_ _C_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ _P_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity4 entity1
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ _P_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ _C_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity12 entity15
It is often assumed that when _entity1_ natural language processing _/entity1_ meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned . However , our experience with _entity2_ TACITUS _/entity2_ ; especially in the _entity3_ MUC-3 evaluation _/entity3_ , has shown that principled techniques for _entity4_ syntactic and pragmatic analysis _/entity4_ can be bolstered with methods for achieving robustness . We describe three techniques for making _entity5_ syntactic analysis _/entity5_ more robust -- -an _entity6_ _C_ agenda-based scheduling parser _/entity6_ , a _entity7_ recovery technique for failed parses _/entity7_ , and a new technique called _entity8_ terminal substring parsing _/entity8_ . For _entity9_ _P_ pragmatics processing _/entity9_ , we describe how the method of _entity10_ abductive inference _/entity10_ is inherently robust , in that an interpretation is always possible , so that in the absence of the required _entity11_ world knowledge _/entity11_ , performance degrades gracefully . Each of these techniques have been evaluated and the results of the evaluations are presented .	NONE entity9 entity6
We describe a simple _entity1_ _P_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ _C_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity1 entity4
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ _P_ statistical ranking _/entity6_ than a previous approach to _entity7_ statistical generation _/entity7_ . An efficient _entity8_ _C_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ lattice-based approach _/entity9_ .	NONE entity6 entity8
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ _C_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ _P_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity13 entity11
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ _C_ feature-based relation extraction _/entity16_ to further improve the _entity17_ _P_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity17 entity16
In this paper , we describe the research using _entity1_ machine learning techniques _/entity1_ to build a _entity2_ comma checker _/entity2_ to be integrated in a _entity3_ grammar checker _/entity3_ for _entity4_ Basque _/entity4_ . After several experiments , and trained with a little _entity5_ corpus _/entity5_ of 100,000 _entity6_ words _/entity6_ , the system guesses correctly not placing _entity7_ _P_ commas _/entity7_ with a _entity8_ precision _/entity8_ of 96 % and a _entity9_ recall _/entity9_ of 98 % . It also gets a _entity10_ _C_ precision _/entity10_ of 70 % and a _entity11_ recall _/entity11_ of 49 % in the task of placing _entity12_ commas _/entity12_ . Finally , we have shown that these results can be improved using a bigger and a more homogeneous _entity13_ corpus _/entity13_ to train , that is , a bigger _entity14_ corpus _/entity14_ written by one unique _entity15_ author _/entity15_ .	NONE entity7 entity10
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ _P_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ _C_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity15 entity18
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ rules _/entity6_ between _entity7_ _P_ sentences _/entity7_ , within _entity8_ _C_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity7 entity8
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ _C_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ _P_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity9 entity7
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ _P_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ _C_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity8 entity10
While _entity1_ _C_ sentence extraction _/entity1_ as an approach to _entity2_ _P_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity2 entity1
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ _P_ continuous density HMM _/entity9_ with _entity10_ _C_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity9 entity10
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ _P_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ _C_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity25 entity28
_entity1_ Statistical language modeling _/entity1_ remains a challenging task , in particular for _entity2_ morphologically rich languages _/entity2_ . Recently , new approaches based on _entity3_ factored language models _/entity3_ have been developed to address this problem . These _entity4_ models _/entity4_ provide principled ways of including additional _entity5_ conditioning variables _/entity5_ other than the _entity6_ preceding words _/entity6_ , such as _entity7_ morphological or syntactic features _/entity7_ . However , the number of possible choices for _entity8_ model parameters _/entity8_ creates a _entity9_ _P_ large space of models _/entity9_ that can not be searched exhaustively . This paper presents an _entity10_ _C_ entirely data-driven model selection procedure _/entity10_ based on _entity11_ genetic search _/entity11_ , which is shown to outperform both _entity12_ knowledge-based and random selection procedures _/entity12_ on two different _entity13_ language modeling tasks _/entity13_ ( _entity14_ Arabic _/entity14_ and _entity15_ Turkish _/entity15_ ) .	NONE entity9 entity10
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ _C_ patterns _/entity14_ in which less specific _entity15_ _P_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity15 entity14
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ _C_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ _P_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity4 entity2
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ _P_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ _C_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity12 entity14
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ _C_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ _P_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	PART_WHOLE entity17 entity16
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ _C_ PK-closed _/entity9_ , _entity10_ _P_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity10 entity9
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ _P_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ _C_ new domains _/entity14_ .	NONE entity12 entity14
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ _C_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ _P_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ human-computer interaction _/entity6_ . We analyzed _entity7_ eye gaze _/entity7_ , _entity8_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity4 entity2
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ formatted weather data _/entity2_ . Such _entity3_ _C_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ stereotyped text structure _/entity5_ . _entity6_ _P_ RAREAS _/entity6_ draws on several kinds of _entity7_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ bilingual or multi-lingual texts _/entity9_ .	NONE entity6 entity3
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ _C_ delimiters _/entity3_ , which poses a challenge to _entity4_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ MT _/entity5_ , the widely used approach is to apply a _entity6_ _P_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity6 entity3
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ _C_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ _P_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity8 entity6
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ _C_ email communication _/entity5_ where _entity6_ _P_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity6 entity5
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ _C_ ILIMP _/entity23_ in a modular _entity24_ _P_ syntactic analysis system _/entity24_ .	NONE entity24 entity23
In this paper _entity1_ _C_ discourse segments _/entity1_ are defined and a method for _entity2_ discourse segmentation _/entity2_ primarily based on _entity3_ abduction _/entity3_ of _entity4_ _P_ temporal relations _/entity4_ between _entity5_ segments _/entity5_ is proposed . This method is precise and _entity6_ computationally feasible _/entity6_ and is supported by previous work in the area of _entity7_ temporal anaphora resolution _/entity7_ .	NONE entity4 entity1
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ _P_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ _C_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity23 entity24
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ _P_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ _C_ distributional concept-distance measures _/entity15_ .	NONE entity13 entity15
This paper introduces a robust _entity1_ interactive method for speech understanding _/entity1_ . The _entity2_ generalized LR parsing _/entity2_ is enhanced in this approach . _entity3_ Parsing _/entity3_ proceeds from left to right correcting minor errors . When a very noisy _entity4_ portion _/entity4_ is detected , the _entity5_ parser _/entity5_ skips that _entity6_ portion _/entity6_ using a fake _entity7_ non-terminal symbol _/entity7_ . The unidentified _entity8_ portion _/entity8_ is resolved by _entity9_ re-utterance _/entity9_ of that _entity10_ portion _/entity10_ which is parsed very efficiently by using the _entity11_ parse record _/entity11_ of the first _entity12_ utterance _/entity12_ . The _entity13_ _C_ user _/entity13_ does not have to speak the whole _entity14_ _P_ sentence _/entity14_ again . This method is also capable of handling _entity15_ unknown words _/entity15_ , which is important in practical systems . Detected _entity16_ unknown words _/entity16_ can be incrementally incorporated into the _entity17_ dictionary _/entity17_ after the interaction with the _entity18_ user _/entity18_ . A _entity19_ pilot system _/entity19_ has shown great effectiveness of this approach .	NONE entity14 entity13
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ _C_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ _P_ dictionary-based word vectors _/entity10_ better reflect _entity11_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity10 entity9
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ _C_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ _P_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity9 entity6
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ _C_ pronoun resolution _/entity2_ in _entity3_ _P_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ features _/entity6_ designed for _entity7_ pronoun resolution _/entity7_ in _entity8_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	NONE entity3 entity2
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ word sense _/entity3_ , a _entity4_ topic signature _/entity4_ is a set of _entity5_ words _/entity5_ that tend to co-occur with it . _entity6_ Topic signatures _/entity6_ can be useful in a number of _entity7_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ topic signatures _/entity15_ on a _entity16_ _P_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ _C_ standard WSD datasets _/entity18_ , with promising results .	NONE entity16 entity18
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ _C_ domains _/entity20_ , the _entity21_ _P_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity21 entity20
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ _C_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ _P_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity18 entity17
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ _P_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ _C_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity3 entity6
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ _C_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ _P_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity8 entity7
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ _P_ synonymous expressions _/entity2_ based on the _entity3_ _C_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity2 entity3
This paper proposes an _entity1_ alignment adaptation approach _/entity1_ to improve _entity2_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ alignment adaptation _/entity3_ is to use _entity4_ out-of-domain corpus _/entity4_ to improve _entity5_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ _P_ precision _/entity11_ and _entity12_ _C_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity11 entity12
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ temporal expressions _/entity2_ in _entity3_ _C_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ temporal expressions _/entity4_ in a novel _entity5_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ _P_ expressions _/entity6_ fits well with our _entity7_ constraint-based representation _/entity7_ of time , _entity8_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ baseline _/entity10_ , and compares favorably with some of the closely related work .	NONE entity6 entity3
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ _C_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ _P_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	NONE entity8 entity7
It is often assumed that when _entity1_ natural language processing _/entity1_ meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned . However , our experience with _entity2_ TACITUS _/entity2_ ; especially in the _entity3_ MUC-3 evaluation _/entity3_ , has shown that principled techniques for _entity4_ syntactic and pragmatic analysis _/entity4_ can be bolstered with methods for achieving robustness . We describe three techniques for making _entity5_ syntactic analysis _/entity5_ more robust -- -an _entity6_ agenda-based scheduling parser _/entity6_ , a _entity7_ _P_ recovery technique for failed parses _/entity7_ , and a new technique called _entity8_ terminal substring parsing _/entity8_ . For _entity9_ pragmatics processing _/entity9_ , we describe how the method of _entity10_ _C_ abductive inference _/entity10_ is inherently robust , in that an interpretation is always possible , so that in the absence of the required _entity11_ world knowledge _/entity11_ , performance degrades gracefully . Each of these techniques have been evaluated and the results of the evaluations are presented .	NONE entity7 entity10
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ _P_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ _C_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity11 entity14
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ CFG parsers _/entity3_ running in _entity4_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ grammar G _/entity5_ and a _entity6_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ _C_ CFG parsing _/entity11_ : a fast , practical _entity12_ _P_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity12 entity11
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ _C_ rules _/entity7_ for determining the _entity8_ _P_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity8 entity7
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ _C_ verbs _/entity17_ with highly varied _entity18_ _P_ argument structures _/entity18_ .	NONE entity18 entity17
This paper introduces a robust _entity1_ interactive method for speech understanding _/entity1_ . The _entity2_ _C_ generalized LR parsing _/entity2_ is enhanced in this approach . _entity3_ Parsing _/entity3_ proceeds from left to right correcting minor errors . When a very noisy _entity4_ _P_ portion _/entity4_ is detected , the _entity5_ parser _/entity5_ skips that _entity6_ portion _/entity6_ using a fake _entity7_ non-terminal symbol _/entity7_ . The unidentified _entity8_ portion _/entity8_ is resolved by _entity9_ re-utterance _/entity9_ of that _entity10_ portion _/entity10_ which is parsed very efficiently by using the _entity11_ parse record _/entity11_ of the first _entity12_ utterance _/entity12_ . The _entity13_ user _/entity13_ does not have to speak the whole _entity14_ sentence _/entity14_ again . This method is also capable of handling _entity15_ unknown words _/entity15_ , which is important in practical systems . Detected _entity16_ unknown words _/entity16_ can be incrementally incorporated into the _entity17_ dictionary _/entity17_ after the interaction with the _entity18_ user _/entity18_ . A _entity19_ pilot system _/entity19_ has shown great effectiveness of this approach .	NONE entity4 entity2
Current _entity1_ natural language interfaces _/entity1_ have concentrated largely on determining the literal _entity2_ meaning _/entity2_ of _entity3_ input _/entity3_ from their _entity4_ users _/entity4_ . While such _entity5_ decoding _/entity5_ is an essential underpinning , much recent work suggests that _entity6_ natural language interfaces _/entity6_ will never appear cooperative or graceful unless they also incorporate numerous _entity7_ _P_ non-literal aspects of communication _/entity7_ , such as robust _entity8_ communication procedures _/entity8_ . This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these _entity9_ non-literal aspects of communication _/entity9_ ; that the new technology of powerful _entity10_ _C_ personal computers _/entity10_ with integral _entity11_ graphics displays _/entity11_ offers techniques superior to those of humans for these aspects , while still satisfying _entity12_ human communication needs _/entity12_ . The paper proposes _entity13_ interfaces _/entity13_ based on a judicious mixture of these techniques and the still valuable methods of more traditional _entity14_ natural language interfaces _/entity14_ .	NONE entity7 entity10
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ _C_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ _P_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity17 entity15
We propose a _entity1_ detection method _/entity1_ for orthographic variants caused by _entity2_ transliteration _/entity2_ in a large _entity3_ corpus _/entity3_ . The method employs two _entity4_ similarities _/entity4_ . One is _entity5_ _C_ string similarity _/entity5_ based on _entity6_ _P_ edit distance _/entity6_ . The other is _entity7_ contextual similarity _/entity7_ by a _entity8_ vector space model _/entity8_ . Experimental results show that the method performed a 0.889 _entity9_ F-measure _/entity9_ in an open test .	USAGE entity6 entity5
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ _C_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ _P_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity21 entity18
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ traditional MT architectural designs _/entity4_ . A new _entity5_ _C_ semantic representation _/entity5_ is proposed that uses _entity6_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ _P_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ interlingua _/entity9_ within a new _entity10_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ direct approaches _/entity12_ into a single system .	NONE entity8 entity5
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ _P_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ _C_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity19 entity20
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ HPSG _/entity2_ into _entity3_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ HPSG _/entity5_ has a more elaborated _entity6_ principle-based theory _/entity6_ of possible _entity7_ phrase structures _/entity7_ , _entity8_ TAG _/entity8_ provides the means to represent _entity9_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ _C_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ _P_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity12 entity10
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ _P_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ _C_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity6 entity8
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ _P_ lexicon _/entity20_ and with incomplete _entity21_ _C_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity20 entity21
We propose a method that automatically generates _entity1_ paraphrase _/entity1_ sets from _entity2_ seed sentences _/entity2_ to be used as _entity3_ reference sets _/entity3_ in objective _entity4_ machine translation evaluation measures _/entity4_ like _entity5_ BLEU _/entity5_ and _entity6_ _C_ NIST _/entity6_ . We measured the quality of the _entity7_ paraphrases _/entity7_ produced in an experiment , i.e. , ( i ) their _entity8_ grammaticality _/entity8_ : at least 99 % correct _entity9_ _P_ sentences _/entity9_ ; ( ii ) their _entity10_ equivalence in meaning _/entity10_ : at least 96 % correct _entity11_ paraphrases _/entity11_ either by _entity12_ meaning equivalence _/entity12_ or _entity13_ entailment _/entity13_ ; and , ( iii ) the amount of internal _entity14_ lexical and syntactical variation _/entity14_ in a set of _entity15_ paraphrases _/entity15_ : slightly superior to that of _entity16_ hand-produced sets _/entity16_ . The _entity17_ paraphrase _/entity17_ sets produced by this method thus seem adequate as _entity18_ reference sets _/entity18_ to be used for _entity19_ MT evaluation _/entity19_ .	NONE entity9 entity6
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ _P_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ _C_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity7 entity10
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ word sense _/entity3_ , a _entity4_ topic signature _/entity4_ is a set of _entity5_ words _/entity5_ that tend to co-occur with it . _entity6_ Topic signatures _/entity6_ can be useful in a number of _entity7_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ _P_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ _C_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ topic signatures _/entity15_ on a _entity16_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ standard WSD datasets _/entity18_ , with promising results .	NONE entity8 entity10
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ _P_ words _/entity10_ contributing to the _entity11_ _C_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity10 entity11
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ _P_ meaning _/entity8_ of larger _entity9_ _C_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	MODEL-FEATURE entity8 entity9
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ _C_ data _/entity15_ to be included in and excluded from _entity16_ _P_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity16 entity15
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ _P_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ _C_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity8 entity11
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ _C_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ _P_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity17 entity16
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ _C_ ASR _/entity16_ and _entity17_ _P_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity17 entity16
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ _P_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ _C_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity16 entity18
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ _P_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ _C_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity4 entity6
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ _C_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ _P_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity7 entity4
In order to boost the _entity1_ translation quality _/entity1_ of _entity2_ _P_ EBMT _/entity2_ based on a small-sized _entity3_ _C_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ language model _/entity5_ of an in-domain _entity6_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ BLEU score _/entity9_ and the _entity10_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity2 entity3
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ natural language interface _/entity2_ to _entity3_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ sentences _/entity6_ in _entity7_ text _/entity7_ or _entity8_ _P_ text-to-speech form _/entity8_ . _entity9_ Deictic reference _/entity9_ and _entity10_ _C_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity8 entity10
We introduce a new _entity1_ interactive corpus exploration tool _/entity1_ called _entity2_ InfoMagnets _/entity2_ . _entity3_ InfoMagnets _/entity3_ aims at making _entity4_ exploratory corpus analysis _/entity4_ accessible to researchers who are not experts in _entity5_ text mining _/entity5_ . As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between _entity6_ language _/entity6_ and _entity7_ behavioral patterns _/entity7_ in two distinct domains : _entity8_ tutorial dialogue _/entity8_ ( Kumar et al. , submitted ) and _entity9_ on-line communities _/entity9_ ( Arguello et al. , 2006 ) . As an _entity10_ _C_ educational tool _/entity10_ , it has been used as part of a unit on _entity11_ _P_ protocol analysis _/entity11_ in an _entity12_ Educational Research Methods course _/entity12_ .	NONE entity11 entity10
In this paper we describe a systematic approach for creating a _entity1_ dialog management system _/entity1_ based on a _entity2_ Construct Algebra _/entity2_ , a _entity3_ collection of relations and operations _/entity3_ on a _entity4_ task representation _/entity4_ . These _entity5_ relations and operations _/entity5_ are _entity6_ analytical components _/entity6_ for building higher level abstractions called _entity7_ dialog motivators _/entity7_ . The _entity8_ _P_ dialog manager _/entity8_ , consisting of a _entity9_ collection of dialog motivators _/entity9_ , is entirely built using the _entity10_ _C_ Construct Algebra _/entity10_ .	NONE entity8 entity10
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ delimiters _/entity3_ , which poses a challenge to _entity4_ _C_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ MT _/entity5_ , the widely used approach is to apply a _entity6_ Chinese word segmenter _/entity6_ trained from _entity7_ _P_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity7 entity4
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ system combination _/entity3_ for _entity4_ unsupervised WSD _/entity4_ . We investigate several _entity5_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ _C_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ _P_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity10 entity7
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ _C_ multi-event stories published over time _/entity3_ . _entity4_ _P_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity4 entity3
This paper describes a method of _entity1_ interactively visualizing and directing the process _/entity1_ of _entity2_ translating a sentence _/entity2_ . The method allows a _entity3_ user _/entity3_ to explore a _entity4_ model _/entity4_ of _entity5_ syntax-based statistical machine translation ( MT ) _/entity5_ , to understand the _entity6_ model _/entity6_ 's strengths and weaknesses , and to compare it to other _entity7_ _C_ MT systems _/entity7_ . Using this _entity8_ _P_ visualization method _/entity8_ , we can find and address conceptual and practical problems in an _entity9_ MT system _/entity9_ . In our demonstration at _entity10_ ACL _/entity10_ , new _entity11_ users _/entity11_ of our tool will drive a _entity12_ syntax-based decoder _/entity12_ for themselves .	NONE entity8 entity7
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ _C_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ _P_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity8 entity6
A proper treatment of _entity1_ syntax _/entity1_ and _entity2_ semantics _/entity2_ in _entity3_ machine translation _/entity3_ is introduced and discussed from the empirical viewpoint . For _entity4_ English-Japanese machine translation _/entity4_ , the _entity5_ syntax directed approach _/entity5_ is effective where the _entity6_ Heuristic Parsing Model ( HPM ) _/entity6_ and the _entity7_ Syntactic Role System _/entity7_ play important roles . For _entity8_ Japanese-English translation _/entity8_ , the _entity9_ _C_ semantics directed approach _/entity9_ is powerful where the _entity10_ _P_ Conceptual Dependency Diagram ( CDD ) _/entity10_ and the _entity11_ Augmented Case Marker System _/entity11_ ( which is a kind of _entity12_ Semantic Role System _/entity12_ ) play essential roles . Some examples of the difference between _entity13_ Japanese sentence structure _/entity13_ and _entity14_ English sentence structure _/entity14_ , which is vital to _entity15_ machine translation _/entity15_ are also discussed together with various interesting _entity16_ ambiguities _/entity16_ .	NONE entity10 entity9
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ _P_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ _C_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity4 entity5
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ _P_ `` overlapping constraint '' _/entity16_ with a _entity17_ _C_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity16 entity17
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ _C_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ _P_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity5 entity3
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ _P_ similarity _/entity8_ between _entity9_ _C_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	MODEL-FEATURE entity8 entity9
Although every _entity1_ natural language system _/entity1_ needs a _entity2_ computational lexicon _/entity2_ , each system puts different amounts and types of information into its _entity3_ lexicon _/entity3_ according to its individual needs . However , some of the information needed across systems is shared or identical information . This paper presents our experience in planning and building _entity4_ COMPLEX _/entity4_ , a _entity5_ _C_ computational lexicon _/entity5_ designed to be a repository of _entity6_ shared lexical information _/entity6_ for use by _entity7_ Natural Language Processing ( NLP ) systems _/entity7_ . We have drawn primarily on explicit and implicit information from _entity8_ _P_ machine-readable dictionaries ( MRD 's ) _/entity8_ to create a _entity9_ broad coverage lexicon _/entity9_ .	NONE entity8 entity5
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ class of languages _/entity2_ obtained by adding _entity3_ reduplication _/entity3_ to _entity4_ context-free languages _/entity4_ . The _entity5_ model _/entity5_ is a _entity6_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ _P_ model _/entity12_ appears capable of accommodating the sort of _entity13_ _C_ reduplications _/entity13_ that have been observed to occur in _entity14_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity12 entity13
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ _P_ senses _/entity10_ of _entity11_ _C_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity10 entity11
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ _C_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ _P_ rule development _/entity12_ .	NONE entity12 entity10
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ class of languages _/entity2_ obtained by adding _entity3_ reduplication _/entity3_ to _entity4_ _P_ context-free languages _/entity4_ . The _entity5_ _C_ model _/entity5_ is a _entity6_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ model _/entity12_ appears capable of accommodating the sort of _entity13_ reduplications _/entity13_ that have been observed to occur in _entity14_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity4 entity5
Reducing _entity1_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ LM pruning _/entity3_ . They are probability , _entity4_ rank _/entity4_ , and _entity5_ _P_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ pruning criteria _/entity6_ in a real application of _entity7_ _C_ Chinese text input _/entity7_ in terms of _entity8_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ CER _/entity13_ .	NONE entity5 entity7
_entity1_ Topical blog post retrieval _/entity1_ is the task of ranking _entity2_ blog posts _/entity2_ with respect to their _entity3_ _P_ relevance _/entity3_ for a given _entity4_ topic _/entity4_ . To improve _entity5_ _C_ topical blog post retrieval _/entity5_ we incorporate _entity6_ textual credibility indicators _/entity6_ in the _entity7_ retrieval process _/entity7_ . We consider two groups of _entity8_ indicators _/entity8_ : post level ( determined using information about individual _entity9_ blog posts _/entity9_ only ) and blog level ( determined using information from the underlying _entity10_ blogs _/entity10_ ) . We describe how to estimate these _entity11_ indicators _/entity11_ and how to integrate them into a _entity12_ retrieval approach _/entity12_ based on _entity13_ language models _/entity13_ . Experiments on the _entity14_ TREC Blog track test set _/entity14_ show that both groups of _entity15_ credibility indicators _/entity15_ significantly improve _entity16_ retrieval effectiveness _/entity16_ ; the best performance is achieved when combining them .	NONE entity3 entity5
We propose a method of organizing reading materials for _entity1_ vocabulary learning _/entity1_ . It enables us to select a concise set of reading _entity2_ texts _/entity2_ ( from a _entity3_ target corpus _/entity3_ ) that contains all the _entity4_ target vocabulary _/entity4_ to be learned . We used a specialized _entity5_ _C_ vocabulary _/entity5_ for an English certification test as the _entity6_ _P_ target vocabulary _/entity6_ and used _entity7_ English Wikipedia _/entity7_ , a free-content encyclopedia , as the _entity8_ target corpus _/entity8_ . The organized reading materials would enable learners not only to study the _entity9_ target vocabulary _/entity9_ efficiently but also to gain a variety of knowledge through reading . The reading materials are available on our web site .	NONE entity6 entity5
_entity1_ Manual acquisition _/entity1_ of _entity2_ _C_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ _P_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity5 entity2
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ _P_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ _C_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity16 entity18
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ _P_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ _C_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity15 entity18
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ _P_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ _C_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity11 entity13
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ _P_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ _C_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity2 entity4
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ _C_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ _P_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity32 entity30
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ _C_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ _P_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity20 entity17
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ _P_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ _C_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity14 entity17
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ _C_ news articles _/entity2_ describing _entity3_ _P_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity3 entity2
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ _C_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ _P_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity30 entity27
The multiplicative fragment of _entity1_ linear logic _/entity1_ has found a number of applications in _entity2_ computational linguistics _/entity2_ : in the _entity3_ `` glue language '' _/entity3_ approach to _entity4_ _C_ LFG semantics _/entity4_ , and in the formulation and _entity5_ parsing _/entity5_ of various _entity6_ categorial grammars _/entity6_ . These applications call for efficient deduction methods . Although a number of deduction methods for _entity7_ _P_ multiplicative linear logic _/entity7_ are known , none of them are tabular methods , which bring a substantial efficiency gain by avoiding redundant computation ( cf . chart methods in _entity8_ CFG parsing _/entity8_ ) : this paper presents such a method , and discusses its use in relation to the above applications .	NONE entity7 entity4
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ _P_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ _C_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity7 entity10
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ _C_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ _P_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity19 entity17
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ _P_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ _C_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity17 entity20
We propose a solution to the challenge of the _entity1_ CoNLL 2008 shared task _/entity1_ that uses a _entity2_ generative history-based latent variable model _/entity2_ to predict the most likely _entity3_ derivation _/entity3_ of a _entity4_ synchronous dependency parser _/entity4_ for both _entity5_ syntactic and semantic dependencies _/entity5_ . The submitted _entity6_ model _/entity6_ yields 79.1 % _entity7_ macro-average F1 performance _/entity7_ , for the joint task , 86.9 % _entity8_ syntactic dependencies LAS _/entity8_ and 71.0 % _entity9_ semantic dependencies F1 _/entity9_ . A larger _entity10_ _C_ model _/entity10_ trained after the deadline achieves 80.5 % _entity11_ macro-average F1 _/entity11_ , 87.6 % _entity12_ syntactic dependencies LAS _/entity12_ , and 73.1 % _entity13_ _P_ semantic dependencies F1 _/entity13_ .	NONE entity13 entity10
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ _P_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ _C_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity9 entity10
This paper describes an implemented program that takes a _entity1_ tagged text corpus _/entity1_ and generates a partial list of the _entity2_ subcategorization frames _/entity2_ in which each _entity3_ verb _/entity3_ occurs . The completeness of the output list increases monotonically with the total _entity4_ occurrences _/entity4_ of each _entity5_ verb _/entity5_ in the _entity6_ _P_ training corpus _/entity6_ . _entity7_ False positive rates _/entity7_ are one to three percent . Five _entity8_ subcategorization frames _/entity8_ are currently detected and we foresee no impediment to detecting many more . Ultimately , we expect to provide a large _entity9_ _C_ subcategorization dictionary _/entity9_ to the _entity10_ NLP community _/entity10_ and to train _entity11_ dictionaries _/entity11_ for specific _entity12_ corpora _/entity12_ .	NONE entity6 entity9
The project presented here is a part of a long term research program aiming at a full _entity1_ lexicon grammar for Polish ( SyntLex ) _/entity1_ . The main of this project is _entity2_ computer-assisted acquisition and morpho-syntactic description of verb-noun collocations _/entity2_ in _entity3_ _P_ Polish _/entity3_ . We present methodology and resources obtained in three main project phases which are : _entity4_ dictionary-based acquisition _/entity4_ of _entity5_ collocation lexicon _/entity5_ , feasibility study for _entity6_ _C_ corpus-based lexicon enlargement _/entity6_ phase , _entity7_ corpus-based lexicon enlargement _/entity7_ and _entity8_ collocation description _/entity8_ . In this paper we focus on the results of the third phase . The presented here _entity9_ corpus-based approach _/entity9_ permitted us to triple the size the _entity10_ verb-noun collocation dictionary for Polish _/entity10_ . In the paper we describe the _entity11_ SyntLex Dictionary of Collocations _/entity11_ and announce some future research intended to be a separate project continuation .	NONE entity3 entity6
This paper presents a novel _entity1_ ensemble learning approach _/entity1_ to resolving _entity2_ German pronouns _/entity2_ . _entity3_ Boosting _/entity3_ , the method in question , combines the moderately accurate _entity4_ hypotheses _/entity4_ of several _entity5_ classifiers _/entity5_ to form a highly accurate one . Experiments show that this approach is superior to a single _entity6_ decision-tree classifier _/entity6_ . Furthermore , we present a _entity7_ standalone system _/entity7_ that resolves _entity8_ pronouns _/entity8_ in _entity9_ unannotated text _/entity9_ by using a fully automatic sequence of _entity10_ _C_ preprocessing modules _/entity10_ that mimics the _entity11_ manual annotation process _/entity11_ . Although the system performs well within a limited _entity12_ textual domain _/entity12_ , further research is needed to make it effective for _entity13_ _P_ open-domain question answering _/entity13_ and _entity14_ text summarisation _/entity14_ .	NONE entity13 entity10
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ _C_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ _P_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity19 entity16
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ _P_ parsing _/entity4_ . We compare two wide-coverage _entity5_ lexicalized grammars of English _/entity5_ , _entity6_ LEXSYS _/entity6_ and _entity7_ _C_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity4 entity7
This paper investigates some _entity1_ _P_ computational problems _/entity1_ associated with _entity2_ probabilistic translation models _/entity2_ that have recently been adopted in the literature on _entity3_ _C_ machine translation _/entity3_ . These _entity4_ models _/entity4_ can be viewed as pairs of _entity5_ probabilistic context-free grammars _/entity5_ working in a 'synchronous ' way . Two _entity6_ hardness _/entity6_ results for the class _entity7_ NP _/entity7_ are reported , along with an _entity8_ exponential time lower-bound _/entity8_ for certain classes of algorithms that are currently used in the literature .	NONE entity1 entity3
This paper describes a new , _entity1_ large scale discourse-level annotation _/entity1_ project - the _entity2_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ discourse structure _/entity3_ that is based on identifying _entity4_ discourse connectives _/entity4_ and their _entity5_ arguments _/entity5_ . The _entity6_ _C_ PDTB _/entity6_ is being built directly on top of the _entity7_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ _P_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ inter-annotator agreement _/entity11_ - both the _entity12_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity9 entity6
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ _P_ derived strings _/entity27_ ; and that _entity28_ _C_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity27 entity28
Multimodal interfaces require effective _entity1_ parsing _/entity1_ and understanding of _entity2_ utterances _/entity2_ whose content is distributed across multiple input modes . Johnston 1998 presents an approach in which strategies for _entity3_ multimodal integration _/entity3_ are stated declaratively using a _entity4_ unification-based grammar _/entity4_ that is used by a _entity5_ multidimensional chart parser _/entity5_ to compose inputs . This approach is highly expressive and supports a broad class of _entity6_ _C_ interfaces _/entity6_ , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input . In this paper , we present an alternative approach in which _entity7_ multimodal parsing and understanding _/entity7_ are achieved using a _entity8_ _P_ weighted finite-state device _/entity8_ which takes _entity9_ speech and gesture streams _/entity9_ as inputs and outputs their joint interpretation . This approach is significantly more efficient , enables tight-coupling of multimodal understanding with _entity10_ speech recognition _/entity10_ , and provides a general probabilistic framework for _entity11_ multimodal ambiguity resolution _/entity11_ .	NONE entity8 entity6
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ _P_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ _C_ automatic alignments _/entity19_ .	NONE entity17 entity19
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ _P_ novice users _/entity23_ without increasing the _entity24_ _C_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity23 entity24
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ _P_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ _C_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity13 entity14
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ _C_ proposition _/entity11_ is referred to as _entity12_ _P_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity12 entity11
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ _C_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ _P_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity40 entity37
The _entity1_ _P_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ _C_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity1 entity2
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ _C_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ _P_ scores _/entity20_ .	NONE entity20 entity19
This paper proposes an approach to _entity1_ full parsing _/entity1_ suitable for _entity2_ _P_ Information Extraction _/entity2_ from _entity3_ _C_ texts _/entity3_ . Sequences of cascades of _entity4_ rules _/entity4_ deterministically analyze the _entity5_ text _/entity5_ , building _entity6_ unambiguous structures _/entity6_ . Initially basic _entity7_ chunks _/entity7_ are analyzed ; then _entity8_ argumental relations _/entity8_ are recognized ; finally _entity9_ modifier attachment _/entity9_ is performed and the _entity10_ global parse tree _/entity10_ is built . The approach was proven to work for three _entity11_ languages _/entity11_ and different _entity12_ domains _/entity12_ . It was implemented in the _entity13_ IE module _/entity13_ of _entity14_ FACILE , a EU project for multilingual text classification and IE _/entity14_ .	NONE entity2 entity3
In this paper we present our recent work on harvesting _entity1_ _C_ English-Chinese bitexts _/entity1_ of the laws of Hong Kong from the _entity2_ _P_ Web _/entity2_ and aligning them to the _entity3_ subparagraph _/entity3_ level via utilizing the _entity4_ numbering system _/entity4_ in the _entity5_ legal text hierarchy _/entity5_ . Basic methodology and practical techniques are reported in detail . The resultant _entity6_ bilingual corpus _/entity6_ , 10.4M _entity7_ English words _/entity7_ and 18.3M _entity8_ Chinese characters _/entity8_ , is an authoritative and comprehensive _entity9_ text collection _/entity9_ covering the specific and special domain of HK laws . It is particularly valuable to _entity10_ empirical MT research _/entity10_ . This piece of work has also laid a foundation for exploring and harvesting _entity11_ English-Chinese bitexts _/entity11_ in a larger volume from the _entity12_ Web _/entity12_ .	NONE entity2 entity1
This paper describes three relatively _entity1_ domain-independent capabilities _/entity1_ recently added to the _entity2_ Paramax spoken language understanding system _/entity2_ : _entity3_ _P_ non-monotonic reasoning _/entity3_ , _entity4_ implicit reference resolution _/entity4_ , and _entity5_ database query paraphrase _/entity5_ . In addition , we discuss the results of the _entity6_ _C_ February 1992 ATIS benchmark tests _/entity6_ . We describe a variation on the _entity7_ standard evaluation metric _/entity7_ which provides a more tightly controlled measure of progress . Finally , we briefly describe an experiment which we have done in extending the _entity8_ n-best speech/language integration architecture _/entity8_ to improving _entity9_ OCR _/entity9_ _entity10_ accuracy _/entity10_ .	NONE entity3 entity6
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ _P_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ _C_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity18 entity20
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ _P_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ _C_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity10 entity12
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ _P_ graphs _/entity10_ are , in fact , _entity11_ _C_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity10 entity11
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ _C_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ _P_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity9 entity8
We present a new HMM tagger that exploits _entity1_ context _/entity1_ on both sides of a _entity2_ word _/entity2_ to be tagged , and evaluate it in both the _entity3_ unsupervised and supervised case _/entity3_ . Along the way , we present the first comprehensive comparison of _entity4_ unsupervised methods for part-of-speech tagging _/entity4_ , noting that published results to date have not been comparable across _entity5_ corpora _/entity5_ or _entity6_ _P_ lexicons _/entity6_ . Observing that the _entity7_ quality _/entity7_ of the _entity8_ _C_ lexicon _/entity8_ greatly impacts the _entity9_ accuracy _/entity9_ that can be achieved by the _entity10_ algorithms _/entity10_ , we present a method of _entity11_ HMM training _/entity11_ that improves _entity12_ accuracy _/entity12_ when _entity13_ training _/entity13_ of _entity14_ lexical probabilities _/entity14_ is unstable . Finally , we show how this new tagger achieves state-of-the-art results in a _entity15_ supervised , non-training intensive framework _/entity15_ .	NONE entity6 entity8
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ _C_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ _P_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity7 entity5
An extension to the _entity1_ GPSG grammatical formalism _/entity1_ is proposed , allowing _entity2_ non-terminals _/entity2_ to consist of finite sequences of _entity3_ category labels _/entity3_ , and allowing _entity4_ schematic variables _/entity4_ to range over such sequences . The extension is shown to be sufficient to provide a strongly adequate _entity5_ grammar _/entity5_ for _entity6_ _P_ crossed serial dependencies _/entity6_ , as found in e.g . _entity7_ _C_ Dutch subordinate clauses _/entity7_ . The structures induced for such _entity8_ constructions _/entity8_ are argued to be more appropriate to data involving _entity9_ conjunction _/entity9_ than some previous proposals have been . The extension is shown to be parseable by a simple extension to an existing _entity10_ parsing method _/entity10_ for _entity11_ GPSG _/entity11_ .	NONE entity6 entity7
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ _C_ testing _/entity20_ of changes to the _entity21_ _P_ language definition _/entity21_ .	NONE entity21 entity20
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ _C_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ _P_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity6 entity3
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ _P_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ _C_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity13 entity14
_entity1_ Manual acquisition _/entity1_ of _entity2_ _P_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ _C_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity2 entity5
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ _C_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ _P_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity14 entity12
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ _C_ task _/entity12_ . We evaluate the demands that _entity13_ _P_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity13 entity12
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ _P_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ _C_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity15 entity17
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ _C_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ _P_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity5 entity3
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ _C_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ _P_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity15 entity13
This paper addresses the issue of _entity1_ _P_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ _C_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity1 entity4
In this paper we show how two standard outputs from _entity1_ information extraction ( IE ) systems _/entity1_ - _entity2_ named entity annotations _/entity2_ and _entity3_ scenario templates _/entity3_ - can be used to enhance access to _entity4_ text collections _/entity4_ via a standard _entity5_ _C_ text browser _/entity5_ . We describe how this information is used in a _entity6_ prototype system _/entity6_ designed to support _entity7_ information workers _/entity7_ ' access to a _entity8_ _P_ pharmaceutical news archive _/entity8_ as part of their _entity9_ industry watch _/entity9_ function . We also report results of a preliminary , _entity10_ qualitative user evaluation _/entity10_ of the system , which while broadly positive indicates further work needs to be done on the _entity11_ interface _/entity11_ to make _entity12_ users _/entity12_ aware of the increased potential of _entity13_ IE-enhanced text browsers _/entity13_ .	NONE entity8 entity5
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ _C_ recognition _/entity13_ with this _entity14_ _P_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity14 entity13
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ _C_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ TAGs _/entity14_ to be used beyond their role in _entity15_ syntax proper _/entity15_ . We discuss the application of _entity16_ _P_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity16 entity13
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ _C_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ _P_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity9 entity6
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ _C_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ _P_ CF grammars _/entity16_ .	NONE entity16 entity13
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ _P_ sense _/entity7_ of an original _entity8_ _C_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	MODEL-FEATURE entity7 entity8
_entity1_ Large-scale natural language generation _/entity1_ requires the integration of vast amounts of _entity2_ knowledge _/entity2_ : lexical , grammatical , and conceptual . A _entity3_ _C_ robust generator _/entity3_ must be able to operate well even when pieces of _entity4_ _P_ knowledge _/entity4_ are missing . It must also be robust against _entity5_ incomplete or inaccurate inputs _/entity5_ . To attack these problems , we have built a _entity6_ hybrid generator _/entity6_ , in which gaps in _entity7_ symbolic knowledge _/entity7_ are filled by _entity8_ statistical methods _/entity8_ . We describe algorithms and show experimental results . We also discuss how the _entity9_ hybrid generation model _/entity9_ can be used to simplify current _entity10_ generators _/entity10_ and enhance their _entity11_ portability _/entity11_ , even when perfect _entity12_ knowledge _/entity12_ is in principle obtainable .	USAGE entity4 entity3
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ _P_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ _C_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity6 entity8
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ _C_ IR _/entity6_ , _entity7_ _P_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity7 entity6
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ _C_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ _P_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity29 entity26
The reality of _entity1_ analogies between words _/entity1_ is refuted by noone ( e.g. , I walked is to to walk as I laughed is to to laugh , noted I walked : to walk : : I laughed : to laugh ) . But _entity2_ _C_ computational linguists _/entity2_ seem to be quite dubious about _entity3_ analogies between sentences _/entity3_ : they would not be enough numerous to be of any use . We report experiments conducted on a _entity4_ _P_ multilingual corpus _/entity4_ to estimate the number of _entity5_ analogies _/entity5_ among the _entity6_ sentences _/entity6_ that it contains . We give two estimates , a lower one and a higher one . As an _entity7_ analogy _/entity7_ must be valid on the level of _entity8_ form _/entity8_ as well as on the level of _entity9_ meaning _/entity9_ , we relied on the idea that _entity10_ translation _/entity10_ should preserve _entity11_ meaning _/entity11_ to test for similar _entity12_ meanings _/entity12_ .	NONE entity4 entity2
In this paper we sketch an approach for _entity1_ _P_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ _C_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity1 entity3
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ _P_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ _C_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	NONE entity3 entity5
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ _P_ description _/entity8_ is a good clue to find the answer for our _entity9_ _C_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity8 entity9
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ _P_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ _C_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity3 entity6
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ _C_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ _P_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity28 entity26
We examine the relationship between the two _entity1_ grammatical formalisms _/entity1_ : _entity2_ Tree Adjoining Grammars _/entity2_ and _entity3_ _C_ Head Grammars _/entity3_ . We briefly investigate the weak _entity4_ equivalence _/entity4_ of the two _entity5_ _P_ formalisms _/entity5_ . We then turn to a discussion comparing the _entity6_ linguistic expressiveness _/entity6_ of the two _entity7_ formalisms _/entity7_ .	NONE entity5 entity3
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ _C_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ _P_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity5 entity3
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ _C_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ _P_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity7 entity5
Previous research has demonstrated the utility of _entity1_ clustering _/entity1_ in inducing _entity2_ semantic verb classes _/entity2_ from undisambiguated _entity3_ corpus data _/entity3_ . We describe a new approach which involves clustering _entity4_ subcategorization frame ( SCF ) _/entity4_ distributions using the _entity5_ _C_ Information Bottleneck _/entity5_ and _entity6_ nearest neighbour _/entity6_ methods . In contrast to previous work , we particularly focus on clustering _entity7_ _P_ polysemic verbs _/entity7_ . A novel _entity8_ evaluation scheme _/entity8_ is proposed which accounts for the effect of _entity9_ polysemy _/entity9_ on the _entity10_ clusters _/entity10_ , offering us a good insight into the potential and limitations of _entity11_ semantically classifying _/entity11_ _entity12_ undisambiguated SCF data _/entity12_ .	NONE entity7 entity5
We directly investigate a subject of much recent debate : do _entity1_ _P_ word sense disambigation models _/entity1_ help _entity2_ statistical machine translation _/entity2_ _entity3_ quality _/entity3_ ? We present empirical results casting doubt on this common , but unproved , assumption . Using a state-of-the-art _entity4_ _C_ Chinese word sense disambiguation model _/entity4_ to choose _entity5_ translation candidates _/entity5_ for a typical _entity6_ IBM statistical MT system _/entity6_ , we find that _entity7_ word sense disambiguation _/entity7_ does not yield significantly better _entity8_ translation quality _/entity8_ than the _entity9_ statistical machine translation system _/entity9_ alone . _entity10_ Error analysis _/entity10_ suggests several key factors behind this surprising finding , including inherent limitations of current _entity11_ statistical MT architectures _/entity11_ .	NONE entity1 entity4
In this paper , we present an _entity1_ unlexicalized parser _/entity1_ for _entity2_ German _/entity2_ which employs _entity3_ smoothing _/entity3_ and _entity4_ suffix analysis _/entity4_ to achieve a _entity5_ labelled bracket F-score _/entity5_ of 76.2 , higher than previously reported results on the _entity6_ _C_ NEGRA corpus _/entity6_ . In addition to the high _entity7_ accuracy _/entity7_ of the model , the use of _entity8_ _P_ smoothing _/entity8_ in an _entity9_ unlexicalized parser _/entity9_ allows us to better examine the interplay between _entity10_ smoothing _/entity10_ and _entity11_ parsing _/entity11_ results .	NONE entity8 entity6
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ _P_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ _C_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity17 entity20
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ _C_ CFG rules _/entity8_ are automatically induced from a _entity9_ _P_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	USAGE entity9 entity8
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ _C_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ _P_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity8 entity6
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ _P_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ _C_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity10 entity12
It is often assumed that when _entity1_ natural language processing _/entity1_ meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned . However , our experience with _entity2_ TACITUS _/entity2_ ; especially in the _entity3_ MUC-3 evaluation _/entity3_ , has shown that principled techniques for _entity4_ syntactic and pragmatic analysis _/entity4_ can be bolstered with methods for achieving robustness . We describe three techniques for making _entity5_ syntactic analysis _/entity5_ more robust -- -an _entity6_ agenda-based scheduling parser _/entity6_ , a _entity7_ recovery technique for failed parses _/entity7_ , and a new technique called _entity8_ _P_ terminal substring parsing _/entity8_ . For _entity9_ pragmatics processing _/entity9_ , we describe how the method of _entity10_ abductive inference _/entity10_ is inherently robust , in that an interpretation is always possible , so that in the absence of the required _entity11_ _C_ world knowledge _/entity11_ , performance degrades gracefully . Each of these techniques have been evaluated and the results of the evaluations are presented .	NONE entity8 entity11
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ _P_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ _C_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity6 entity9
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ _P_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ _C_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity11 entity14
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ _C_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ _P_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity27 entity26
This paper ties up some loose ends in _entity1_ finite-state Optimality Theory _/entity1_ . First , it discusses how to perform _entity2_ comprehension _/entity2_ under _entity3_ Optimality Theory grammars _/entity3_ consisting of _entity4_ finite-state constraints _/entity4_ . _entity5_ Comprehension _/entity5_ has not been much studied in _entity6_ OT _/entity6_ ; we show that unlike _entity7_ production _/entity7_ , it does not always yield a regular set , making _entity8_ finite-state methods _/entity8_ inapplicable . However , after giving a suitably flexible presentation of _entity9_ _P_ OT _/entity9_ , we show carefully how to treat _entity10_ _C_ comprehension _/entity10_ under recent _entity11_ variants of OT _/entity11_ in which _entity12_ grammars _/entity12_ can be compiled into _entity13_ finite-state transducers _/entity13_ . We then unify these variants , showing that _entity14_ compilation _/entity14_ is possible if all components of the _entity15_ grammar _/entity15_ are _entity16_ regular relations _/entity16_ , including the _entity17_ harmony ordering _/entity17_ on _entity18_ scored candidates _/entity18_ .	NONE entity9 entity10
Past work of generating _entity1_ referring expressions _/entity1_ mainly utilized attributes of _entity2_ objects _/entity2_ and _entity3_ binary relations _/entity3_ between _entity4_ objects _/entity4_ . However , such an approach does not work well when there is no distinctive attribute among _entity5_ objects _/entity5_ . To overcome this limitation , this paper proposes a method utilizing the perceptual groups of _entity6_ objects _/entity6_ and _entity7_ _P_ n-ary relations _/entity7_ among them . The key is to identify groups of _entity8_ _C_ objects _/entity8_ that are naturally recognized by humans . We conducted psychological experiments with 42 subjects to collect _entity9_ referring expressions _/entity9_ in such situations , and built a _entity10_ generation algorithm _/entity10_ based on the results . The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions .	NONE entity7 entity8
This paper discusses two problems that arise in the _entity1_ Generation _/entity1_ of _entity2_ Referring Expressions _/entity2_ : ( a ) _entity3_ numeric-valued attributes _/entity3_ , such as size or location ; ( b ) _entity4_ _C_ perspective-taking _/entity4_ in _entity5_ reference _/entity5_ . Both problems , it is argued , can be resolved if some structure is imposed on the available knowledge prior to _entity6_ content determination _/entity6_ . We describe a _entity7_ _P_ clustering algorithm _/entity7_ which is sufficiently general to be applied to these diverse problems , discuss its application , and evaluate its performance .	NONE entity7 entity4
This paper describes a _entity1_ characters-based Chinese collocation system _/entity1_ and discusses the advantages of it over a traditional _entity2_ word-based system _/entity2_ . Since _entity3_ wordbreaks _/entity3_ are not conventionally marked in _entity4_ Chinese text corpora _/entity4_ , a _entity5_ _C_ character-based collocation system _/entity5_ has the dual advantages of avoiding _entity6_ pre-processing distortion _/entity6_ and directly accessing _entity7_ _P_ sub-lexical information _/entity7_ . Furthermore , _entity8_ word-based collocational properties _/entity8_ can be obtained through an auxiliary module of _entity9_ automatic segmentation _/entity9_ .	NONE entity7 entity5
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ _C_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ _P_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity13 entity10
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ _P_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ _C_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity13 entity16
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ _C_ discourse relations _/entity12_ between _entity13_ _P_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity13 entity12
This paper describes an _entity1_ _P_ unsupervised learning method _/entity1_ for _entity2_ _C_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	USAGE entity1 entity2
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ _P_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ _C_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity13 entity16
_entity1_ _C_ Automatic estimation _/entity1_ of _entity2_ _P_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity2 entity1
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ _P_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ _C_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity9 entity12
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ _P_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ _C_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity30 entity33
In this paper we present our recent work on harvesting _entity1_ English-Chinese bitexts _/entity1_ of the laws of Hong Kong from the _entity2_ Web _/entity2_ and aligning them to the _entity3_ subparagraph _/entity3_ level via utilizing the _entity4_ _P_ numbering system _/entity4_ in the _entity5_ legal text hierarchy _/entity5_ . Basic methodology and practical techniques are reported in detail . The resultant _entity6_ bilingual corpus _/entity6_ , 10.4M _entity7_ _C_ English words _/entity7_ and 18.3M _entity8_ Chinese characters _/entity8_ , is an authoritative and comprehensive _entity9_ text collection _/entity9_ covering the specific and special domain of HK laws . It is particularly valuable to _entity10_ empirical MT research _/entity10_ . This piece of work has also laid a foundation for exploring and harvesting _entity11_ English-Chinese bitexts _/entity11_ in a larger volume from the _entity12_ Web _/entity12_ .	NONE entity4 entity7
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ _P_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ _C_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity7 entity10
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ _C_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ _P_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity7 entity4
_entity1_ Chart parsing _/entity1_ is _entity2_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ chart _/entity3_ that works outward from _entity4_ islands _/entity4_ and makes sense of as much of the _entity5_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ _C_ fragments _/entity7_ occur in the _entity8_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ _P_ islands _/entity9_ , until possibly completely missing _entity10_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity9 entity7
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ _P_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ _C_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity2 entity5
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ _P_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ _C_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity6 entity9
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ _P_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ _C_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity5 entity8
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ _P_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ _C_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity13 entity16
This paper proposes an _entity1_ alignment adaptation approach _/entity1_ to improve _entity2_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ _C_ alignment adaptation _/entity3_ is to use _entity4_ _P_ out-of-domain corpus _/entity4_ to improve _entity5_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ precision _/entity11_ and _entity12_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity4 entity3
The _entity1_ TAP-XL Automated Analyst 's Assistant _/entity1_ is an application designed to help an _entity2_ _P_ English _/entity2_ -speaking analyst write a _entity3_ _C_ topical report _/entity3_ , culling information from a large inflow of _entity4_ multilingual , multimedia data _/entity4_ . It gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other _entity5_ languages _/entity5_ by leveraging _entity6_ human language technology _/entity6_ .	NONE entity2 entity3
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ _C_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ _P_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity6 entity4
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ morphological and syntactic information _/entity3_ . While the _entity4_ model _/entity4_ is more complex than those which have been employed for _entity5_ unsupervised learning _/entity5_ of _entity6_ POS tags in English _/entity6_ , which use only _entity7_ _P_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ morphology _/entity9_ as well . In many _entity10_ _C_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ Bulgarian _/entity15_ , a _entity16_ Slavic language _/entity16_ with relatively _entity17_ free word order _/entity17_ and _entity18_ rich morphology _/entity18_ .	NONE entity7 entity10
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ class of languages _/entity2_ obtained by adding _entity3_ _C_ reduplication _/entity3_ to _entity4_ context-free languages _/entity4_ . The _entity5_ model _/entity5_ is a _entity6_ _P_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ model _/entity12_ appears capable of accommodating the sort of _entity13_ reduplications _/entity13_ that have been observed to occur in _entity14_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity6 entity3
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ _P_ language definition _/entity17_ that is natural in terms of the _entity18_ _C_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity17 entity18
This paper introduces a method for _entity1_ computational analysis of move structures _/entity1_ in _entity2_ abstracts _/entity2_ of _entity3_ research articles _/entity3_ . In our approach , _entity4_ sentences _/entity4_ in a given _entity5_ abstract _/entity5_ are analyzed and labeled with a specific _entity6_ move _/entity6_ in light of various _entity7_ rhetorical functions _/entity7_ . The method involves automatically gathering a large number of _entity8_ abstracts _/entity8_ from the _entity9_ _P_ Web _/entity9_ and building a _entity10_ language model _/entity10_ of _entity11_ abstract moves _/entity11_ . We also present a prototype _entity12_ _C_ concordancer _/entity12_ , _entity13_ CARE _/entity13_ , which exploits the _entity14_ move-tagged abstracts _/entity14_ for _entity15_ digital learning _/entity15_ . This system provides a promising approach to _entity16_ Web-based computer-assisted academic writing _/entity16_ .	NONE entity9 entity12
In this paper , we will describe a _entity1_ _C_ search tool _/entity1_ for a huge set of _entity2_ ngrams _/entity2_ . The tool supports _entity3_ _P_ queries _/entity3_ with an arbitrary number of _entity4_ wildcards _/entity4_ . It takes a fraction of a second for a search , and can provide the _entity5_ fillers _/entity5_ of the _entity6_ wildcards _/entity6_ . The system runs on a single Linux PC with reasonable size _entity7_ memory _/entity7_ ( less than 4GB ) and _entity8_ disk space _/entity8_ ( less than 400GB ) . This system can be a very useful tool for _entity9_ linguistic knowledge discovery _/entity9_ and other _entity10_ NLP tasks _/entity10_ .	NONE entity3 entity1
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ _C_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ _P_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity10 entity8
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ _C_ term extraction _/entity9_ based on _entity10_ _P_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	USAGE entity10 entity9
We present an efficient algorithm for the _entity1_ redundancy elimination problem _/entity1_ : Given an _entity2_ underspecified semantic representation ( USR ) _/entity2_ of a _entity3_ scope ambiguity _/entity3_ , compute an _entity4_ USR _/entity4_ with fewer mutually _entity5_ equivalent readings _/entity5_ . The algorithm operates on _entity6_ _C_ underspecified chart representations _/entity6_ which are derived from _entity7_ dominance graphs _/entity7_ ; it can be applied to the _entity8_ USRs _/entity8_ computed by _entity9_ _P_ large-scale grammars _/entity9_ . We evaluate the algorithm on a _entity10_ corpus _/entity10_ , and show that it reduces the degree of _entity11_ ambiguity _/entity11_ significantly while taking negligible runtime .	NONE entity9 entity6
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ _P_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ _C_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity3 entity6
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ _P_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ _C_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity12 entity14
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ _C_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ _P_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity20 entity17
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ _C_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ _P_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity8 entity5
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ _P_ example-base _/entity17_ - a bottom up approach . These _entity18_ _C_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity17 entity18
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ _C_ words _/entity14_ that represent new _entity15_ _P_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity15 entity14
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ _P_ speech data _/entity11_ from many _entity12_ _C_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity11 entity12
This paper explores the issue of using different _entity1_ co-occurrence similarities _/entity1_ between _entity2_ terms _/entity2_ for separating _entity3_ query terms _/entity3_ that are useful for _entity4_ retrieval _/entity4_ from those that are harmful . The hypothesis under examination is that _entity5_ _C_ useful terms _/entity5_ tend to be more similar to each other than to other _entity6_ _P_ query terms _/entity6_ . Preliminary experiments with similarities computed using _entity7_ first-order and second-order co-occurrence _/entity7_ seem to confirm the hypothesis . _entity8_ Term similarities _/entity8_ could then be used for determining which _entity9_ query terms _/entity9_ are useful and best reflect the user 's information need . A possible application would be to use this source of evidence for tuning the _entity10_ weights _/entity10_ of the _entity11_ query terms _/entity11_ .	NONE entity6 entity5
We argue in favor of the the use of _entity1_ labeled directed graph _/entity1_ to represent various types of _entity2_ linguistic structures _/entity2_ , and illustrate how this allows one to view _entity3_ _P_ NLP tasks _/entity3_ as _entity4_ _C_ graph transformations _/entity4_ . We present a general method for learning such _entity5_ transformations _/entity5_ from an _entity6_ annotated corpus _/entity6_ and describe experiments with two applications of the method : _entity7_ identification of non-local depenencies _/entity7_ ( using _entity8_ Penn Treebank data _/entity8_ ) and _entity9_ semantic role labeling _/entity9_ ( using _entity10_ Proposition Bank data _/entity10_ ) .	NONE entity3 entity4
We present a _entity1_ Czech-English statistical machine translation system _/entity1_ which performs _entity2_ tree-to-tree translation _/entity2_ of _entity3_ dependency structures _/entity3_ . The only _entity4_ bilingual resource _/entity4_ required is a _entity5_ sentence-aligned parallel corpus _/entity5_ . All other _entity6_ resources _/entity6_ are _entity7_ _P_ monolingual _/entity7_ . We also refer to an _entity8_ evaluation method _/entity8_ and plan to compare our _entity9_ system 's output _/entity9_ with a _entity10_ _C_ benchmark system _/entity10_ .	NONE entity7 entity10
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ _C_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ _P_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity7 entity6
This paper describes _entity1_ FERRET _/entity1_ , an _entity2_ interactive question-answering ( Q/A ) system _/entity2_ designed to address the challenges of integrating _entity3_ automatic Q/A _/entity3_ applications into real-world environments . _entity4_ FERRET _/entity4_ utilizes a novel approach to _entity5_ _P_ Q/A _/entity5_ known as _entity6_ predictive questioning _/entity6_ which attempts to identify the _entity7_ _C_ questions _/entity7_ ( and _entity8_ answers _/entity8_ ) that _entity9_ users _/entity9_ need by analyzing how a _entity10_ user _/entity10_ interacts with a system while gathering information related to a particular scenario .	NONE entity5 entity7
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ _C_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ _P_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity9 entity6
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ _C_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ _P_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity16 entity14
In order to boost the _entity1_ translation quality _/entity1_ of _entity2_ EBMT _/entity2_ based on a small-sized _entity3_ _C_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ language model _/entity5_ of an in-domain _entity6_ _P_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ BLEU score _/entity9_ and the _entity10_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity6 entity3
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ _P_ Extended CF grammars _/entity12_ ( _entity13_ _C_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity12 entity13
We describe a novel technique and implemented system for constructing a _entity1_ subcategorization dictionary _/entity1_ from _entity2_ textual corpora _/entity2_ . Each _entity3_ dictionary entry _/entity3_ encodes the _entity4_ relative frequency of occurrence _/entity4_ of a comprehensive set of _entity5_ subcategorization classes _/entity5_ for _entity6_ English _/entity6_ . An initial experiment , on a sample of 14 _entity7_ verbs _/entity7_ which exhibit _entity8_ multiple complementation patterns _/entity8_ , demonstrates that the technique achieves _entity9_ accuracy _/entity9_ comparable to previous approaches , which are all limited to a highly restricted set of _entity10_ _P_ subcategorization classes _/entity10_ . We also demonstrate that a _entity11_ _C_ subcategorization dictionary _/entity11_ built with the system improves the _entity12_ accuracy _/entity12_ of a _entity13_ parser _/entity13_ by an appreciable amount	NONE entity10 entity11
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ traditional MT architectural designs _/entity4_ . A new _entity5_ semantic representation _/entity5_ is proposed that uses _entity6_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ _C_ interlingua _/entity9_ within a new _entity10_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ _P_ direct approaches _/entity12_ into a single system .	NONE entity12 entity9
In this paper we introduce a _entity1_ modal language LT _/entity1_ for imposing _entity2_ constraints _/entity2_ on _entity3_ trees _/entity3_ , and an extension _entity4_ LT ( LF ) _/entity4_ for imposing _entity5_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ GPSG _/entity9_ can be captured in _entity10_ _C_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ _P_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	NONE entity12 entity10
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ _C_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ _P_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity14 entity11
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ _C_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ _P_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity10 entity7
In this paper , we present a framework , _entity1_ _C_ Presentor _/entity1_ , for the development and customization of _entity2_ hypertext presentation generators _/entity2_ . _entity3_ Presentor _/entity3_ offers intuitive and powerful _entity4_ _P_ declarative languages _/entity4_ specifying the presentation at different levels : macro-planning , micro-planning , realization , and formatting . _entity5_ Presentor _/entity5_ is implemented and is portable cross-platform and cross-domain . It has been used with success in several application domains including weather forecasting , _entity6_ object modeling _/entity6_ , system description and _entity7_ requirements summarization _/entity7_ .	NONE entity4 entity1
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ _P_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ _C_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity12 entity15
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ _P_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ _C_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	USAGE entity6 entity7
We discuss _entity1_ maximum a posteriori estimation _/entity1_ of _entity2_ continuous density hidden Markov models ( CDHMM ) _/entity2_ . The classical _entity3_ MLE reestimation algorithms _/entity3_ , namely the _entity4_ forward-backward algorithm _/entity4_ and the _entity5_ segmental k-means algorithm _/entity5_ , are expanded and _entity6_ reestimation formulas _/entity6_ are given for _entity7_ HMM with Gaussian mixture observation densities _/entity7_ . Because of its adaptive nature , _entity8_ Bayesian learning _/entity8_ serves as a unified approach for the following four _entity9_ speech recognition _/entity9_ applications , namely _entity10_ parameter smoothing _/entity10_ , _entity11_ _C_ speaker adaptation _/entity11_ , _entity12_ speaker group modeling _/entity12_ and _entity13_ _P_ corrective training _/entity13_ . New experimental results on all four applications are provided to show the effectiveness of the _entity14_ MAP estimation approach _/entity14_ .	NONE entity13 entity11
In this paper a novel solution to automatic and _entity1_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ _C_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ _P_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ WSI algorithm _/entity14_ .	NONE entity4 entity2
An attempt has been made to use an _entity1_ Augmented Transition Network _/entity1_ as a procedural _entity2_ dialog model _/entity2_ . The development of such a _entity3_ model _/entity3_ appears to be important in several respects : as a device to represent and to use different _entity4_ dialog schemata _/entity4_ proposed in empirical _entity5_ conversation analysis _/entity5_ ; as a device to represent and to use _entity6_ models of verbal interaction _/entity6_ ; as a device combining knowledge about _entity7_ dialog schemata _/entity7_ and about _entity8_ _P_ verbal interaction _/entity8_ with knowledge about _entity9_ _C_ task-oriented and goal-directed dialogs _/entity9_ . A standard _entity10_ ATN _/entity10_ should be further developed in order to account for the _entity11_ verbal interactions _/entity11_ of _entity12_ task-oriented dialogs _/entity12_ .	NONE entity8 entity9
We propose a framework to derive the _entity1_ _C_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ _P_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity4 entity1
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ _C_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ _P_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity7 entity6
We consider the problem of computing the _entity1_ Kullback-Leibler distance _/entity1_ , also called the _entity2_ relative entropy _/entity2_ , between a _entity3_ probabilistic context-free grammar _/entity3_ and a _entity4_ probabilistic finite automaton _/entity4_ . We show that there is a _entity5_ closed-form ( analytical ) solution _/entity5_ for one part of the _entity6_ Kullback-Leibler distance _/entity6_ , viz . the _entity7_ cross-entropy _/entity7_ . We discuss several applications of the result to the problem of _entity8_ _P_ distributional approximation _/entity8_ of _entity9_ probabilistic context-free grammars _/entity9_ by means of _entity10_ _C_ probabilistic finite automata _/entity10_ .	NONE entity8 entity10
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ _C_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ _P_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity18 entity15
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ _C_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ _P_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity20 entity18
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ _P_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ _C_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity6 entity9
A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature , practical applications rather than `` toy '' implementations . Evaluation is also crucial to assessing competing claims and identifying promising technical approaches . While work in _entity1_ speech recognition ( SR ) _/entity1_ has a history of evaluation methodologies that permit comparison among various systems , until recently no methodology existed for either developers of _entity2_ natural language ( NL ) interfaces _/entity2_ or researchers in _entity3_ speech understanding ( SU ) _/entity3_ to evaluate and compare the systems they developed . Recently considerable progress has been made by a number of groups involved in the _entity4_ _C_ DARPA Spoken Language Systems ( SLS ) program _/entity4_ to agree on a methodology for comparative evaluation of _entity5_ _P_ SLS systems _/entity5_ , and that methodology has been put into practice several times in comparative tests of several _entity6_ SLS systems _/entity6_ . These evaluations are probably the only _entity7_ NL evaluations _/entity7_ other than the series of _entity8_ Message Understanding Conferences _/entity8_ ( Sundheim , 1989 ; Sundheim , 1991 ) to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems ( Palmer et al. , 1989 ; Neal et al. , 1991 ) . This paper describes a practical _entity9_ `` black-box '' methodology _/entity9_ for automatic evaluation of _entity10_ question-answering NL systems _/entity10_ . While each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and it can be used with either _entity11_ speech or text input _/entity11_ . The particular characteristics of the approach are described in the following section : subsequent sections present its implementation in the _entity12_ DARPA SLS community _/entity12_ , and some problems and directions for future development .	NONE entity5 entity4
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ _P_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ _C_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity10 entity11
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ _P_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ _C_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity15 entity17
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ _P_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ _C_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity5 entity8
_entity1_ Manual acquisition _/entity1_ of _entity2_ _P_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ _C_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity2 entity3
This paper presents a new approach to _entity1_ _C_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ trees _/entity3_ , or _entity4_ _P_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ statistical ranking _/entity6_ than a previous approach to _entity7_ statistical generation _/entity7_ . An efficient _entity8_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ lattice-based approach _/entity9_ .	NONE entity4 entity1
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ _P_ features _/entity13_ , without concerns about how these _entity14_ _C_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity13 entity14
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ human-computer interaction _/entity6_ . We analyzed _entity7_ eye gaze _/entity7_ , _entity8_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ _C_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ _P_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity12 entity11
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ _P_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ _C_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity6 entity9
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ _C_ restricted subset _/entity6_ of _entity7_ _P_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity7 entity6
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ _P_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ _C_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity11 entity13
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ _P_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ _C_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity9 entity12
It is often assumed that when _entity1_ natural language processing _/entity1_ meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned . However , our experience with _entity2_ TACITUS _/entity2_ ; especially in the _entity3_ _C_ MUC-3 evaluation _/entity3_ , has shown that principled techniques for _entity4_ syntactic and pragmatic analysis _/entity4_ can be bolstered with methods for achieving robustness . We describe three techniques for making _entity5_ syntactic analysis _/entity5_ more robust -- -an _entity6_ _P_ agenda-based scheduling parser _/entity6_ , a _entity7_ recovery technique for failed parses _/entity7_ , and a new technique called _entity8_ terminal substring parsing _/entity8_ . For _entity9_ pragmatics processing _/entity9_ , we describe how the method of _entity10_ abductive inference _/entity10_ is inherently robust , in that an interpretation is always possible , so that in the absence of the required _entity11_ world knowledge _/entity11_ , performance degrades gracefully . Each of these techniques have been evaluated and the results of the evaluations are presented .	NONE entity6 entity3
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ _P_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ _C_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity10 entity13
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ _C_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ _P_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity6 entity4
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ _P_ phrases _/entity15_ in the _entity16_ _C_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity15 entity16
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ _P_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ _C_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity7 entity9
_entity1_ Determiners _/entity1_ play an important role in conveying the _entity2_ meaning _/entity2_ of an _entity3_ utterance _/entity3_ , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the _entity4_ _C_ global meaning _/entity4_ of a _entity5_ sentence _/entity5_ , even if not in a precise way . Another problem with _entity6_ _P_ determiners _/entity6_ is their inherent _entity7_ ambiguity _/entity7_ . In this paper we propose a _entity8_ logical formalism _/entity8_ , which , among other things , is suitable for representing _entity9_ determiners _/entity9_ without forcing a particular _entity10_ interpretation _/entity10_ when their _entity11_ meaning _/entity11_ is still not clear .	NONE entity6 entity4
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ _P_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ _C_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity26 entity29
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ _P_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ _C_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity6 entity9
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ _C_ trigram language model _/entity12_ to determine the most probable _entity13_ _P_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity13 entity12
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ _C_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ grammar coverage problems _/entity3_ we use a _entity4_ _P_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity4 entity1
This paper describes a _entity1_ computational model _/entity1_ of _entity2_ word segmentation _/entity2_ and presents simulation results on _entity3_ _P_ realistic acquisition _/entity3_ . In particular , we explore the capacity and limitations of _entity4_ statistical learning mechanisms _/entity4_ that have recently gained prominence in _entity5_ cognitive psychology _/entity5_ and _entity6_ _C_ linguistics _/entity6_ .	NONE entity3 entity6
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ _C_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ _P_ predicates _/entity17_ .	NONE entity17 entity14
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ class of languages _/entity2_ obtained by adding _entity3_ reduplication _/entity3_ to _entity4_ context-free languages _/entity4_ . The _entity5_ model _/entity5_ is a _entity6_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ _P_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ model _/entity12_ appears capable of accommodating the sort of _entity13_ _C_ reduplications _/entity13_ that have been observed to occur in _entity14_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity10 entity13
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ _P_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ _C_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity11 entity14
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ _C_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ _P_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity6 entity3
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ _C_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ _P_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity14 entity11
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ _P_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ _C_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity5 entity8
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ _C_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ _P_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity6 entity3
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ _C_ words _/entity21_ ) , which is comparable to that of an _entity22_ _P_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity22 entity21
This paper proposes _entity1_ document oriented preference sets ( DoPS ) _/entity1_ for the disambiguation of the _entity2_ dependency structure _/entity2_ of _entity3_ sentences _/entity3_ . The _entity4_ DoPS system _/entity4_ extracts preference knowledge from a _entity5_ target document _/entity5_ or other _entity6_ documents _/entity6_ automatically . _entity7_ Sentence ambiguities _/entity7_ can be resolved by using domain targeted preference knowledge without using complicated large _entity8_ _C_ knowledgebases _/entity8_ . _entity9_ _P_ Implementation _/entity9_ and _entity10_ empirical results _/entity10_ are described for the the analysis of _entity11_ dependency structures _/entity11_ of _entity12_ Japanese patent claim sentences _/entity12_ .	NONE entity9 entity8
This paper discusses the application of _entity1_ Unification Categorial Grammar ( UCG ) _/entity1_ to the framework of _entity2_ Isomorphic Grammars _/entity2_ for _entity3_ Machine Translation _/entity3_ pioneered by Landsbergen . The _entity4_ Isomorphic Grammars approach to MT _/entity4_ involves developing the _entity5_ grammars _/entity5_ of the _entity6_ Source and Target languages _/entity6_ in parallel , in order to ensure that _entity7_ _C_ SL _/entity7_ and _entity8_ TL _/entity8_ expressions which stand in the _entity9_ translation relation _/entity9_ have _entity10_ _P_ isomorphic derivations _/entity10_ . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited , obviating the need for answers to _entity11_ semantic questions _/entity11_ that we do not yet have . _entity12_ Semantic _/entity12_ and other information may still be incorporated , but as constraints on the _entity13_ translation relation _/entity13_ , not as levels of _entity14_ textual representation _/entity14_ . After introducing this approach to _entity15_ MT system _/entity15_ design , and the basics of _entity16_ monolingual UCG _/entity16_ , we will show how the two can be integrated , and present an example from an implemented _entity17_ bi-directional English-Spanish fragment _/entity17_ . Finally we will present some outstanding problems with the approach .	NONE entity10 entity7
In this paper , we reported experiments of _entity1_ _P_ unsupervised automatic acquisition _/entity1_ of _entity2_ Italian and English verb subcategorization frames ( SCFs ) _/entity2_ from _entity3_ general and domain corpora _/entity3_ . The proposed technique operates on _entity4_ _C_ syntactically shallow-parsed corpora _/entity4_ on the basis of a limited number of _entity5_ search heuristics _/entity5_ not relying on any previous _entity6_ lexico-syntactic knowledge _/entity6_ about _entity7_ SCFs _/entity7_ . Although preliminary , reported results are in line with _entity8_ state-of-the-art lexical acquisition systems _/entity8_ . The issue of whether _entity9_ verbs _/entity9_ sharing similar _entity10_ SCFs distributions _/entity10_ happen to share _entity11_ similar semantic properties _/entity11_ as well was also explored by clustering _entity12_ verbs _/entity12_ that share _entity13_ frames _/entity13_ with the same _entity14_ distribution _/entity14_ using the _entity15_ Minimum Description Length Principle ( MDL ) _/entity15_ . First experiments in this direction were carried out on _entity16_ Italian verbs _/entity16_ with encouraging results .	NONE entity1 entity4
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ _P_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ _C_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity11 entity12
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ formatted weather data _/entity2_ . Such _entity3_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ _P_ stereotyped text structure _/entity5_ . _entity6_ RAREAS _/entity6_ draws on several kinds of _entity7_ _C_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ bilingual or multi-lingual texts _/entity9_ .	NONE entity5 entity7
In this paper we present a _entity1_ statistical profile _/entity1_ of the _entity2_ Named Entity task _/entity2_ , a specific _entity3_ information extraction task _/entity3_ for which _entity4_ _C_ corpora _/entity4_ in several _entity5_ languages _/entity5_ are available . Using the _entity6_ _P_ results _/entity6_ of the _entity7_ statistical analysis _/entity7_ , we propose an _entity8_ algorithm _/entity8_ for _entity9_ lower bound estimation _/entity9_ for _entity10_ Named Entity corpora _/entity10_ and discuss the significance of the _entity11_ cross-lingual comparisons _/entity11_ provided by the _entity12_ analysis _/entity12_ .	NONE entity6 entity4
Previous research has demonstrated the utility of _entity1_ clustering _/entity1_ in inducing _entity2_ semantic verb classes _/entity2_ from undisambiguated _entity3_ corpus data _/entity3_ . We describe a new approach which involves clustering _entity4_ subcategorization frame ( SCF ) _/entity4_ distributions using the _entity5_ Information Bottleneck _/entity5_ and _entity6_ nearest neighbour _/entity6_ methods . In contrast to previous work , we particularly focus on clustering _entity7_ polysemic verbs _/entity7_ . A novel _entity8_ evaluation scheme _/entity8_ is proposed which accounts for the effect of _entity9_ _C_ polysemy _/entity9_ on the _entity10_ clusters _/entity10_ , offering us a good insight into the potential and limitations of _entity11_ _P_ semantically classifying _/entity11_ _entity12_ undisambiguated SCF data _/entity12_ .	NONE entity11 entity9
Past work of generating _entity1_ referring expressions _/entity1_ mainly utilized attributes of _entity2_ objects _/entity2_ and _entity3_ binary relations _/entity3_ between _entity4_ _P_ objects _/entity4_ . However , such an approach does not work well when there is no distinctive attribute among _entity5_ objects _/entity5_ . To overcome this limitation , this paper proposes a method utilizing the perceptual groups of _entity6_ objects _/entity6_ and _entity7_ _C_ n-ary relations _/entity7_ among them . The key is to identify groups of _entity8_ objects _/entity8_ that are naturally recognized by humans . We conducted psychological experiments with 42 subjects to collect _entity9_ referring expressions _/entity9_ in such situations , and built a _entity10_ generation algorithm _/entity10_ based on the results . The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions .	NONE entity4 entity7
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ _P_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ _C_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity14 entity16
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ _P_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ _C_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity9 entity11
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ _P_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ _C_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity19 entity20
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ _P_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ _C_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity21 entity22
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ _P_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ _C_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity18 entity19
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ _P_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ _C_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity8 entity10
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ _P_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ _C_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity13 entity15
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ _P_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ _C_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity4 entity7
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ _C_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ _P_ translation speed and quality _/entity20_ .	NONE entity20 entity17
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ _P_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ _C_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity15 entity18
In this paper , we propose a novel _entity1_ Cooperative Model _/entity1_ for _entity2_ natural language understanding _/entity2_ in a _entity3_ dialogue system _/entity3_ . We build this based on both _entity4_ Finite State Model ( FSM ) _/entity4_ and _entity5_ Statistical Learning Model ( SLM ) _/entity5_ . _entity6_ _C_ FSM _/entity6_ provides two strategies for _entity7_ language understanding _/entity7_ and have a high accuracy but little robustness and flexibility . _entity8_ _P_ Statistical approach _/entity8_ is much more robust but less accurate . _entity9_ Cooperative Model _/entity9_ incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies .	NONE entity8 entity6
Sources of _entity1_ training data _/entity1_ suitable for _entity2_ language modeling _/entity2_ of _entity3_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ _C_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ _P_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	NONE entity7 entity5
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ ungrammatical input _/entity8_ . While _entity9_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ passives _/entity11_ , _entity12_ _C_ relative clauses _/entity12_ and _entity13_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ _P_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ Plume _/entity16_ to handle _entity17_ passives _/entity17_ , _entity18_ relative clauses _/entity18_ , and _entity19_ interrogatives _/entity19_ in a general manner .	NONE entity15 entity12
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ _C_ annotation experiment _/entity7_ and showed that _entity8_ _P_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity8 entity7
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ _C_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ _P_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity11 entity8
Soames 1979 provides some counterexamples to the _entity1_ _C_ theory of natural language presuppositions _/entity1_ that is presented in Gazdar 1979 . Soames 1982 provides a theory which explains these counterexamples . Mercer 1987 rejects the solution found in Soames 1982 leaving these counterexamples unexplained . By reappraising these insightful counterexamples , the _entity2_ inferential theory for natural language presuppositions _/entity2_ described in Mercer 1987 , 1988 gives a simple and straightforward explanation for the _entity3_ presuppositional nature _/entity3_ of these _entity4_ _P_ sentences _/entity4_ .	NONE entity4 entity1
To support engaging human users in robust , _entity1_ mixed-initiative speech dialogue interactions _/entity1_ which reach beyond current capabilities in _entity2_ dialogue systems _/entity2_ , the _entity3_ DARPA Communicator program _/entity3_ [ 1 ] is funding the development of a _entity4_ distributed message-passing infrastructure _/entity4_ for _entity5_ dialogue systems _/entity5_ which all _entity6_ Communicator _/entity6_ participants are using . In this presentation , we describe the features of and _entity7_ _C_ requirements _/entity7_ for a genuinely useful _entity8_ _P_ software infrastructure _/entity8_ for this purpose .	NONE entity8 entity7
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ _C_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ _P_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity16 entity15
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ _C_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ _P_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	USAGE entity5 entity4
By generalizing the notion of _entity1_ location of a constituent _/entity1_ to allow _entity2_ discontinuous locations _/entity2_ , one can describe the _entity3_ discontinuous constituents _/entity3_ of _entity4_ non-configurational languages _/entity4_ . These _entity5_ discontinuous constituents _/entity5_ can be described by a variant of _entity6_ _P_ definite clause grammars _/entity6_ , and these _entity7_ grammars _/entity7_ can be used in conjunction with a _entity8_ proof procedure _/entity8_ to create a _entity9_ _C_ parser for non-configurational languages _/entity9_ .	NONE entity6 entity9
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ _P_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ _C_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity13 entity16
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ _P_ temporal connective _/entity6_ , gives the wrong _entity7_ _C_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity6 entity7
We present a _entity1_ _P_ syntax-based constraint _/entity1_ for _entity2_ word alignment _/entity2_ , known as the _entity3_ cohesion constraint _/entity3_ . It requires disjoint _entity4_ _C_ English phrases _/entity4_ to be mapped to non-overlapping intervals in the _entity5_ French sentence _/entity5_ . We evaluate the utility of this _entity6_ constraint _/entity6_ in two different algorithms . The results show that it can provide a significant improvement in _entity7_ alignment quality _/entity7_ .	NONE entity1 entity4
In this paper we present a _entity1_ statistical profile _/entity1_ of the _entity2_ Named Entity task _/entity2_ , a specific _entity3_ information extraction task _/entity3_ for which _entity4_ corpora _/entity4_ in several _entity5_ languages _/entity5_ are available . Using the _entity6_ results _/entity6_ of the _entity7_ statistical analysis _/entity7_ , we propose an _entity8_ algorithm _/entity8_ for _entity9_ _C_ lower bound estimation _/entity9_ for _entity10_ _P_ Named Entity corpora _/entity10_ and discuss the significance of the _entity11_ cross-lingual comparisons _/entity11_ provided by the _entity12_ analysis _/entity12_ .	NONE entity10 entity9
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ _C_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ _P_ new domains _/entity14_ .	NONE entity14 entity11
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ training and test data _/entity5_ with respect to _entity6_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ _P_ training data _/entity7_ labeled with _entity8_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ _C_ domain _/entity9_ , _entity10_ topic _/entity10_ and time .	NONE entity7 entity9
Multimodal interfaces require effective _entity1_ _C_ parsing _/entity1_ and understanding of _entity2_ utterances _/entity2_ whose content is distributed across multiple input modes . Johnston 1998 presents an approach in which strategies for _entity3_ multimodal integration _/entity3_ are stated declaratively using a _entity4_ _P_ unification-based grammar _/entity4_ that is used by a _entity5_ multidimensional chart parser _/entity5_ to compose inputs . This approach is highly expressive and supports a broad class of _entity6_ interfaces _/entity6_ , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input . In this paper , we present an alternative approach in which _entity7_ multimodal parsing and understanding _/entity7_ are achieved using a _entity8_ weighted finite-state device _/entity8_ which takes _entity9_ speech and gesture streams _/entity9_ as inputs and outputs their joint interpretation . This approach is significantly more efficient , enables tight-coupling of multimodal understanding with _entity10_ speech recognition _/entity10_ , and provides a general probabilistic framework for _entity11_ multimodal ambiguity resolution _/entity11_ .	NONE entity4 entity1
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ _C_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ _P_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity11 entity8
We describe a _entity1_ dialogue system _/entity1_ that works with its interlocutor to identify objects . Our contributions include a concise , _entity2_ modular architecture _/entity2_ with reversible processes of _entity3_ _P_ understanding _/entity3_ and _entity4_ _C_ generation _/entity4_ , an _entity5_ information-state model of reference _/entity5_ , and flexible links between _entity6_ semantics _/entity6_ and _entity7_ collaborative problem solving _/entity7_ .	NONE entity3 entity4
We present a new _entity1_ part-of-speech tagger _/entity1_ that demonstrates the following ideas : ( i ) explicit use of both preceding and following _entity2_ tag contexts _/entity2_ via a _entity3_ dependency network representation _/entity3_ , ( ii ) broad use of _entity4_ lexical features _/entity4_ , including _entity5_ jointly conditioning on multiple consecutive words _/entity5_ , ( iii ) effective use of _entity6_ _C_ priors _/entity6_ in _entity7_ conditional loglinear models _/entity7_ , and ( iv ) fine-grained modeling of _entity8_ unknown word features _/entity8_ . Using these ideas together , the resulting _entity9_ _P_ tagger _/entity9_ gives a 97.24 % _entity10_ accuracy _/entity10_ on the _entity11_ Penn Treebank WSJ _/entity11_ , an _entity12_ error reduction _/entity12_ of 4.4 % on the best previous single automatically learned _entity13_ tagging _/entity13_ result .	NONE entity9 entity6
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ _P_ word-based alignments _/entity9_ and _entity10_ _C_ lexical weighting _/entity10_ of _entity11_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ phrases _/entity12_ longer than three _entity13_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity9 entity10
Current _entity1_ natural language interfaces _/entity1_ have concentrated largely on determining the literal _entity2_ meaning _/entity2_ of _entity3_ input _/entity3_ from their _entity4_ users _/entity4_ . While such _entity5_ decoding _/entity5_ is an essential underpinning , much recent work suggests that _entity6_ natural language interfaces _/entity6_ will never appear cooperative or graceful unless they also incorporate numerous _entity7_ non-literal aspects of communication _/entity7_ , such as robust _entity8_ communication procedures _/entity8_ . This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these _entity9_ _C_ non-literal aspects of communication _/entity9_ ; that the new technology of powerful _entity10_ personal computers _/entity10_ with integral _entity11_ graphics displays _/entity11_ offers techniques superior to those of humans for these aspects , while still satisfying _entity12_ _P_ human communication needs _/entity12_ . The paper proposes _entity13_ interfaces _/entity13_ based on a judicious mixture of these techniques and the still valuable methods of more traditional _entity14_ natural language interfaces _/entity14_ .	NONE entity12 entity9
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ _C_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ _P_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity11 entity10
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ CFG parsers _/entity3_ running in _entity4_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ _P_ grammar G _/entity5_ and a _entity6_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ _C_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ CFG parsing _/entity11_ : a fast , practical _entity12_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity5 entity8
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ _P_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ _C_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity41 entity43
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ _C_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ _P_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity7 entity5
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ _C_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ _P_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity6 entity3
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ _C_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ _P_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity6 entity4
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ _P_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ _C_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity3 entity6
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ _C_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ _P_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity10 entity7
In order to build robust _entity1_ automatic abstracting systems _/entity1_ , there is a need for better _entity2_ training resources _/entity2_ than are currently available . In this paper , we introduce an _entity3_ annotation scheme _/entity3_ for scientific articles which can be used to build such a _entity4_ resource _/entity4_ in a consistent way . The seven categories of the _entity5_ _P_ scheme _/entity5_ are based on _entity6_ rhetorical moves _/entity6_ of _entity7_ _C_ argumentation _/entity7_ . Our experimental results show that the _entity8_ scheme _/entity8_ is stable , reproducible and intuitive to use .	NONE entity5 entity7
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ LFG _/entity4_ and _entity5_ UDRS _/entity5_ formalisms . It provides a _entity6_ model theoretic interpretation _/entity6_ and an _entity7_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ f-structures _/entity9_ through the _entity10_ _C_ translation images _/entity10_ of _entity11_ f-structures _/entity11_ as _entity12_ _P_ UDRSs _/entity12_ .	NONE entity12 entity10
A new approach for _entity1_ Interactive Machine Translation _/entity1_ where the _entity2_ author _/entity2_ interacts during the creation or the modification of the _entity3_ document _/entity3_ is proposed . The explanation of an _entity4_ _P_ ambiguity _/entity4_ or an error for the purposes of correction does not use any concepts of the underlying _entity5_ linguistic theory _/entity5_ : it is a reformulation of the erroneous or ambiguous _entity6_ sentence _/entity6_ . The interaction is limited to the analysis step of the _entity7_ _C_ translation process _/entity7_ . This paper presents a new _entity8_ interactive disambiguation scheme _/entity8_ based on the _entity9_ paraphrasing _/entity9_ of a _entity10_ parser _/entity10_ 's multiple output . Some examples of _entity11_ paraphrasing _/entity11_ ambiguous _entity12_ sentences _/entity12_ are presented .	NONE entity4 entity7
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ _P_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ _C_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity6 entity8
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ _C_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ _P_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity7 entity4
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ _P_ subcategorization cues _/entity9_ from _entity10_ _C_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity9 entity10
Reducing _entity1_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ LM pruning _/entity3_ . They are probability , _entity4_ rank _/entity4_ , and _entity5_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ pruning criteria _/entity6_ in a real application of _entity7_ Chinese text input _/entity7_ in terms of _entity8_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ _C_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ _P_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ CER _/entity13_ .	NONE entity11 entity9
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ _C_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ _P_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity32 entity30
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ _P_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ _C_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity7 entity10
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ _P_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ _C_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity8 entity10
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ _P_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ _C_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity5 entity8
We examine the relationship between the two _entity1_ _C_ grammatical formalisms _/entity1_ : _entity2_ _P_ Tree Adjoining Grammars _/entity2_ and _entity3_ Head Grammars _/entity3_ . We briefly investigate the weak _entity4_ equivalence _/entity4_ of the two _entity5_ formalisms _/entity5_ . We then turn to a discussion comparing the _entity6_ linguistic expressiveness _/entity6_ of the two _entity7_ formalisms _/entity7_ .	NONE entity2 entity1
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ _P_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ _C_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity5 entity6
This paper proposes an approach to _entity1_ full parsing _/entity1_ suitable for _entity2_ Information Extraction _/entity2_ from _entity3_ texts _/entity3_ . Sequences of cascades of _entity4_ rules _/entity4_ deterministically analyze the _entity5_ text _/entity5_ , building _entity6_ _C_ unambiguous structures _/entity6_ . Initially basic _entity7_ chunks _/entity7_ are analyzed ; then _entity8_ argumental relations _/entity8_ are recognized ; finally _entity9_ _P_ modifier attachment _/entity9_ is performed and the _entity10_ global parse tree _/entity10_ is built . The approach was proven to work for three _entity11_ languages _/entity11_ and different _entity12_ domains _/entity12_ . It was implemented in the _entity13_ IE module _/entity13_ of _entity14_ FACILE , a EU project for multilingual text classification and IE _/entity14_ .	NONE entity9 entity6
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ _C_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ _P_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity31 entity28
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ _P_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ _C_ average precision metric _/entity17_ .	NONE entity14 entity17
_entity1_ Statistical machine translation ( SMT ) _/entity1_ is currently one of the hot spots in _entity2_ natural language processing _/entity2_ . Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that _entity3_ SMT _/entity3_ gives competitive results to _entity4_ rule-based translation systems _/entity4_ , requiring significantly less development time . This is particularly important when building _entity5_ translation systems _/entity5_ for new _entity6_ language pairs _/entity6_ or new _entity7_ domains _/entity7_ . This workshop is intended to give an introduction to _entity8_ statistical machine translation _/entity8_ with a focus on practical considerations . Participants should be able , after attending this workshop , to set out building an _entity9_ SMT system _/entity9_ themselves and achieving good _entity10_ baseline results _/entity10_ in a short time . The tutorial will cover the basics of _entity11_ SMT _/entity11_ : Theory will be put into practice . _entity12_ STTK _/entity12_ , a _entity13_ statistical machine translation tool kit _/entity13_ , will be introduced and used to build a working _entity14_ translation system _/entity14_ . _entity15_ STTK _/entity15_ has been developed by the presenter and co-workers over a number of years and is currently used as the basis of _entity16_ CMU 's SMT system _/entity16_ . It has also successfully been coupled with _entity17_ _C_ rule-based and example based machine translation modules _/entity17_ to build a _entity18_ multi engine machine translation system _/entity18_ . The _entity19_ source code _/entity19_ of the _entity20_ _P_ tool kit _/entity20_ will be made available .	NONE entity20 entity17
This paper introduces a robust _entity1_ interactive method for speech understanding _/entity1_ . The _entity2_ generalized LR parsing _/entity2_ is enhanced in this approach . _entity3_ Parsing _/entity3_ proceeds from left to right correcting minor errors . When a very noisy _entity4_ portion _/entity4_ is detected , the _entity5_ parser _/entity5_ skips that _entity6_ portion _/entity6_ using a fake _entity7_ _C_ non-terminal symbol _/entity7_ . The unidentified _entity8_ portion _/entity8_ is resolved by _entity9_ re-utterance _/entity9_ of that _entity10_ _P_ portion _/entity10_ which is parsed very efficiently by using the _entity11_ parse record _/entity11_ of the first _entity12_ utterance _/entity12_ . The _entity13_ user _/entity13_ does not have to speak the whole _entity14_ sentence _/entity14_ again . This method is also capable of handling _entity15_ unknown words _/entity15_ , which is important in practical systems . Detected _entity16_ unknown words _/entity16_ can be incrementally incorporated into the _entity17_ dictionary _/entity17_ after the interaction with the _entity18_ user _/entity18_ . A _entity19_ pilot system _/entity19_ has shown great effectiveness of this approach .	NONE entity10 entity7
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ _P_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ _C_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity7 entity9
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ _C_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ _P_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ phrase translations _/entity9_ in our _entity10_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity4 entity2
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ _C_ parse tree _/entity12_ that will determine the correct _entity13_ _P_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity13 entity12
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ _C_ derived string _/entity29_ that is found most often in the _entity30_ _P_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity30 entity29
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ _P_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ _C_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity12 entity15
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ _P_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ _C_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity15 entity18
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ _P_ discourse relations _/entity2_ in _entity3_ _C_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	PART_WHOLE entity2 entity3
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ _P_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ _C_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity8 entity11
Despite much recent progress on accurate _entity1_ semantic role labeling _/entity1_ , previous work has largely used _entity2_ independent classifiers _/entity2_ , possibly combined with separate _entity3_ label sequence models _/entity3_ via _entity4_ Viterbi decoding _/entity4_ . This stands in stark contrast to the linguistic observation that a _entity5_ core argument frame _/entity5_ is a joint structure , with strong _entity6_ dependencies _/entity6_ between _entity7_ arguments _/entity7_ . We show how to build a _entity8_ joint model _/entity8_ of _entity9_ _C_ argument frames _/entity9_ , incorporating novel _entity10_ _P_ features _/entity10_ that model these interactions into _entity11_ discriminative log-linear models _/entity11_ . This system achieves an _entity12_ error reduction _/entity12_ of 22 % on all _entity13_ arguments _/entity13_ and 32 % on _entity14_ core arguments _/entity14_ over a state-of-the art independent _entity15_ classifier _/entity15_ for _entity16_ gold-standard parse trees _/entity16_ on _entity17_ PropBank _/entity17_ .	NONE entity10 entity9
This paper proposes the _entity1_ Hierarchical Directed Acyclic Graph ( HDAG ) Kernel _/entity1_ for _entity2_ structured natural language data _/entity2_ . The _entity3_ HDAG Kernel _/entity3_ directly accepts several levels of both _entity4_ chunks _/entity4_ and their _entity5_ relations _/entity5_ , and then efficiently computes the _entity6_ weighed sum _/entity6_ of the number of common _entity7_ _C_ attribute sequences _/entity7_ of the _entity8_ HDAGs _/entity8_ . We applied the proposed method to _entity9_ question classification _/entity9_ and _entity10_ _P_ sentence alignment tasks _/entity10_ to evaluate its performance as a _entity11_ similarity measure _/entity11_ and a _entity12_ kernel function _/entity12_ . The results of the experiments demonstrate that the _entity13_ HDAG Kernel _/entity13_ is superior to other _entity14_ kernel functions _/entity14_ and _entity15_ baseline methods _/entity15_ .	NONE entity10 entity7
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ _P_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ _C_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity13 entity14
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ _P_ morphological and syntactic information _/entity3_ . While the _entity4_ _C_ model _/entity4_ is more complex than those which have been employed for _entity5_ unsupervised learning _/entity5_ of _entity6_ POS tags in English _/entity6_ , which use only _entity7_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ morphology _/entity9_ as well . In many _entity10_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ Bulgarian _/entity15_ , a _entity16_ Slavic language _/entity16_ with relatively _entity17_ free word order _/entity17_ and _entity18_ rich morphology _/entity18_ .	NONE entity3 entity4
This paper describes the understanding process of the _entity1_ spatial descriptions _/entity1_ in _entity2_ Japanese _/entity2_ . In order to understand the described _entity3_ world _/entity3_ , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space . It is done by an experimental _entity4_ computer program _/entity4_ _entity5_ SPRINT _/entity5_ , which takes _entity6_ natural language texts _/entity6_ and produces a _entity7_ model _/entity7_ of the described _entity8_ world _/entity8_ . To reconstruct the _entity9_ model _/entity9_ , the authors extract the _entity10_ _P_ qualitative spatial constraints _/entity10_ from the _entity11_ _C_ text _/entity11_ , and represent them as the _entity12_ numerical constraints _/entity12_ on the _entity13_ spatial attributes _/entity13_ of the _entity14_ entities _/entity14_ . This makes it possible to express the vagueness of the _entity15_ spatial concepts _/entity15_ and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints . The interpretation reflects the _entity16_ temporary belief _/entity16_ about the _entity17_ world _/entity17_ .	PART_WHOLE entity10 entity11
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ _P_ proposition _/entity11_ is referred to as _entity12_ _C_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity11 entity12
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ _C_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ _P_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity13 entity10
Interpreting _entity1_ metaphors _/entity1_ is an integral and inescapable process in _entity2_ human understanding of natural language _/entity2_ . This paper discusses a _entity3_ method of analyzing metaphors _/entity3_ based on the existence of a small number of _entity4_ generalized metaphor mappings _/entity4_ . Each _entity5_ generalized metaphor _/entity5_ contains a _entity6_ recognition network _/entity6_ , a _entity7_ basic mapping _/entity7_ , additional _entity8_ _P_ transfer mappings _/entity8_ , and an _entity9_ implicit intention component _/entity9_ . It is argued that the method reduces _entity10_ _C_ metaphor interpretation _/entity10_ from a _entity11_ reconstruction _/entity11_ to a _entity12_ recognition task _/entity12_ . Implications towards automating certain aspects of _entity13_ language learning _/entity13_ are also discussed .	NONE entity8 entity10
The study addresses the problem of _entity1_ _C_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ _P_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity3 entity1
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ _C_ character _/entity11_ level . The use of _entity12_ _P_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity12 entity11
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ _C_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ _P_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity12 entity10
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ _C_ word-aligned corpora _/entity5_ is proposed . A _entity6_ _P_ statistical translation model _/entity6_ is also presented that deals such _entity7_ phrases _/entity7_ , as well as a _entity8_ training method _/entity8_ based on the maximization of _entity9_ translation accuracy _/entity9_ , as measured with the _entity10_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity6 entity5
Despite much recent progress on accurate _entity1_ semantic role labeling _/entity1_ , previous work has largely used _entity2_ independent classifiers _/entity2_ , possibly combined with separate _entity3_ label sequence models _/entity3_ via _entity4_ Viterbi decoding _/entity4_ . This stands in stark contrast to the linguistic observation that a _entity5_ core argument frame _/entity5_ is a joint structure , with strong _entity6_ dependencies _/entity6_ between _entity7_ arguments _/entity7_ . We show how to build a _entity8_ _C_ joint model _/entity8_ of _entity9_ argument frames _/entity9_ , incorporating novel _entity10_ features _/entity10_ that model these interactions into _entity11_ _P_ discriminative log-linear models _/entity11_ . This system achieves an _entity12_ error reduction _/entity12_ of 22 % on all _entity13_ arguments _/entity13_ and 32 % on _entity14_ core arguments _/entity14_ over a state-of-the art independent _entity15_ classifier _/entity15_ for _entity16_ gold-standard parse trees _/entity16_ on _entity17_ PropBank _/entity17_ .	NONE entity11 entity8
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ _P_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ _C_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity11 entity14
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ sublanguage _/entity3_ , is proposed for identifying _entity4_ unknown words _/entity4_ , especially _entity5_ personal names _/entity5_ , in _entity6_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ adaptive dynamic word formation _/entity8_ , _entity9_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ _C_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ WI systems _/entity12_ without the _entity13_ _P_ name identification _/entity13_ capability .	NONE entity13 entity10
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ _P_ canonical answer _/entity10_ , returning either true or false . The _entity11_ _C_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity10 entity11
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ _P_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ _C_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity6 entity9
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to disambiguate various relations between _entity2_ named entities _/entity2_ by use of various _entity3_ lexical and syntactic features _/entity3_ from the _entity4_ contexts _/entity4_ . It works by calculating _entity5_ eigenvectors _/entity5_ of an _entity6_ adjacency graph _/entity6_ 's _entity7_ _P_ Laplacian _/entity7_ to recover a _entity8_ _C_ submanifold _/entity8_ of data from a _entity9_ high dimensionality space _/entity9_ and then performing _entity10_ cluster number estimation _/entity10_ on the _entity11_ eigenvectors _/entity11_ . Experiment results on _entity12_ ACE corpora _/entity12_ show that this _entity13_ spectral clustering based approach _/entity13_ outperforms the other _entity14_ clustering methods _/entity14_ .	NONE entity7 entity8
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ natural language interface _/entity2_ to _entity3_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ _C_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ sentences _/entity6_ in _entity7_ _P_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity7 entity5
This paper considers the problem of automatic assessment of _entity1_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ _C_ discourse representation _/entity8_ supports the effective learning of a _entity9_ _P_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity9 entity8
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ _P_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ _C_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity13 entity16
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ _P_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ _C_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity10 entity11
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ _C_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ _P_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity25 entity23
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ _P_ rules _/entity6_ between _entity7_ _C_ sentences _/entity7_ , within _entity8_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity6 entity7
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ _P_ ASR _/entity16_ and _entity17_ _C_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	COMPARE entity16 entity17
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ _C_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ _P_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity11 entity8
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ _C_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ _P_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity5 entity2
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ _C_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ _P_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity16 entity13
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ _P_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ _C_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity16 entity18
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ _P_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ _C_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity6 entity9
In this paper , we describe the research using _entity1_ machine learning techniques _/entity1_ to build a _entity2_ comma checker _/entity2_ to be integrated in a _entity3_ grammar checker _/entity3_ for _entity4_ Basque _/entity4_ . After several experiments , and trained with a little _entity5_ corpus _/entity5_ of 100,000 _entity6_ words _/entity6_ , the system guesses correctly not placing _entity7_ commas _/entity7_ with a _entity8_ precision _/entity8_ of 96 % and a _entity9_ _C_ recall _/entity9_ of 98 % . It also gets a _entity10_ precision _/entity10_ of 70 % and a _entity11_ _P_ recall _/entity11_ of 49 % in the task of placing _entity12_ commas _/entity12_ . Finally , we have shown that these results can be improved using a bigger and a more homogeneous _entity13_ corpus _/entity13_ to train , that is , a bigger _entity14_ corpus _/entity14_ written by one unique _entity15_ author _/entity15_ .	NONE entity11 entity9
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ _P_ strings _/entity22_ will be derived by substituting these related _entity23_ _C_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity22 entity23
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ _C_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ _P_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity11 entity8
We propose a _entity1_ detection method _/entity1_ for orthographic variants caused by _entity2_ transliteration _/entity2_ in a large _entity3_ _C_ corpus _/entity3_ . The method employs two _entity4_ _P_ similarities _/entity4_ . One is _entity5_ string similarity _/entity5_ based on _entity6_ edit distance _/entity6_ . The other is _entity7_ contextual similarity _/entity7_ by a _entity8_ vector space model _/entity8_ . Experimental results show that the method performed a 0.889 _entity9_ F-measure _/entity9_ in an open test .	NONE entity4 entity3
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ _P_ domains _/entity19_ for _entity20_ _C_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity19 entity20
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ _C_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ _P_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity13 entity12
In this paper , we propose a novel _entity1_ _P_ Cooperative Model _/entity1_ for _entity2_ natural language understanding _/entity2_ in a _entity3_ _C_ dialogue system _/entity3_ . We build this based on both _entity4_ Finite State Model ( FSM ) _/entity4_ and _entity5_ Statistical Learning Model ( SLM ) _/entity5_ . _entity6_ FSM _/entity6_ provides two strategies for _entity7_ language understanding _/entity7_ and have a high accuracy but little robustness and flexibility . _entity8_ Statistical approach _/entity8_ is much more robust but less accurate . _entity9_ Cooperative Model _/entity9_ incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies .	NONE entity1 entity3
In this paper , we present an _entity1_ _P_ unlexicalized parser _/entity1_ for _entity2_ German _/entity2_ which employs _entity3_ smoothing _/entity3_ and _entity4_ _C_ suffix analysis _/entity4_ to achieve a _entity5_ labelled bracket F-score _/entity5_ of 76.2 , higher than previously reported results on the _entity6_ NEGRA corpus _/entity6_ . In addition to the high _entity7_ accuracy _/entity7_ of the model , the use of _entity8_ smoothing _/entity8_ in an _entity9_ unlexicalized parser _/entity9_ allows us to better examine the interplay between _entity10_ smoothing _/entity10_ and _entity11_ parsing _/entity11_ results .	NONE entity1 entity4
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ _C_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ _P_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity5 entity3
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ _P_ models _/entity8_ are _entity9_ _C_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity8 entity9
Sources of _entity1_ training data _/entity1_ suitable for _entity2_ language modeling _/entity2_ of _entity3_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ _C_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ _P_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	NONE entity10 entity7
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ ungrammatical input _/entity8_ . While _entity9_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ passives _/entity11_ , _entity12_ relative clauses _/entity12_ and _entity13_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ Plume _/entity16_ to handle _entity17_ _P_ passives _/entity17_ , _entity18_ relative clauses _/entity18_ , and _entity19_ _C_ interrogatives _/entity19_ in a general manner .	NONE entity17 entity19
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ _P_ query optimisation _/entity16_ in a _entity17_ _C_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	USAGE entity16 entity17
In this paper , we will describe a _entity1_ search tool _/entity1_ for a huge set of _entity2_ _P_ ngrams _/entity2_ . The tool supports _entity3_ queries _/entity3_ with an arbitrary number of _entity4_ _C_ wildcards _/entity4_ . It takes a fraction of a second for a search , and can provide the _entity5_ fillers _/entity5_ of the _entity6_ wildcards _/entity6_ . The system runs on a single Linux PC with reasonable size _entity7_ memory _/entity7_ ( less than 4GB ) and _entity8_ disk space _/entity8_ ( less than 400GB ) . This system can be a very useful tool for _entity9_ linguistic knowledge discovery _/entity9_ and other _entity10_ NLP tasks _/entity10_ .	NONE entity2 entity4
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ _P_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ _C_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity13 entity16
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ _P_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ _C_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity11 entity14
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ _P_ phrases _/entity12_ longer than three _entity13_ words _/entity13_ and learning _entity14_ _C_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity12 entity14
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ _C_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ _P_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity4 entity2
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ _P_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ _C_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity8 entity10
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ _P_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ _C_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity3 entity4
In this paper I will argue for a _entity1_ model of grammatical processing _/entity1_ that is based on _entity2_ uniform processing _/entity2_ and _entity3_ knowledge sources _/entity3_ . The main _entity4_ feature _/entity4_ of this model is to view _entity5_ parsing _/entity5_ and _entity6_ generation _/entity6_ as two strongly interleaved tasks performed by a single _entity7_ _P_ parametrized deduction _/entity7_ process . It will be shown that this view supports flexible and efficient _entity8_ _C_ natural language processing _/entity8_ .	NONE entity7 entity8
For _entity1_ intelligent interactive systems _/entity1_ to communicate with _entity2_ humans _/entity2_ in a natural manner , they must have knowledge about the _entity3_ system users _/entity3_ . This paper explores the role of _entity4_ _P_ user modeling _/entity4_ in such _entity5_ systems _/entity5_ . It begins with a characterization of what a _entity6_ _C_ user model _/entity6_ is and how it can be used . The types of information that a _entity7_ user model _/entity7_ may be required to keep about a _entity8_ user _/entity8_ are then identified and discussed . _entity9_ User models _/entity9_ themselves can vary greatly depending on the requirements of the situation and the implementation , so several dimensions along which they can be classified are presented . Since acquiring the knowledge for a _entity10_ user model _/entity10_ is a fundamental problem in _entity11_ user modeling _/entity11_ , a section is devoted to this topic . Next , the benefits and costs of implementing a _entity12_ user modeling component _/entity12_ for a system are weighed in light of several aspects of the _entity13_ interaction requirements _/entity13_ that may be imposed by the system . Finally , the current state of research in _entity14_ user modeling _/entity14_ is summarized , and future research topics that must be addressed in order to achieve powerful , general _entity15_ user modeling systems _/entity15_ are assessed .	NONE entity4 entity6
A proper treatment of _entity1_ syntax _/entity1_ and _entity2_ semantics _/entity2_ in _entity3_ machine translation _/entity3_ is introduced and discussed from the empirical viewpoint . For _entity4_ English-Japanese machine translation _/entity4_ , the _entity5_ syntax directed approach _/entity5_ is effective where the _entity6_ Heuristic Parsing Model ( HPM ) _/entity6_ and the _entity7_ Syntactic Role System _/entity7_ play important roles . For _entity8_ _C_ Japanese-English translation _/entity8_ , the _entity9_ semantics directed approach _/entity9_ is powerful where the _entity10_ _P_ Conceptual Dependency Diagram ( CDD ) _/entity10_ and the _entity11_ Augmented Case Marker System _/entity11_ ( which is a kind of _entity12_ Semantic Role System _/entity12_ ) play essential roles . Some examples of the difference between _entity13_ Japanese sentence structure _/entity13_ and _entity14_ English sentence structure _/entity14_ , which is vital to _entity15_ machine translation _/entity15_ are also discussed together with various interesting _entity16_ ambiguities _/entity16_ .	NONE entity10 entity8
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ _P_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ _C_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity22 entity23
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ _P_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ _C_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity33 entity36
The _entity1_ JAVELIN system _/entity1_ integrates a flexible , _entity2_ planning-based architecture _/entity2_ with a variety of _entity3_ language processing modules _/entity3_ to provide an _entity4_ open-domain question answering capability _/entity4_ on _entity5_ free text _/entity5_ . The demonstration will focus on how _entity6_ JAVELIN _/entity6_ processes _entity7_ _P_ questions _/entity7_ and retrieves the most likely _entity8_ answer candidates _/entity8_ from the given _entity9_ _C_ text corpus _/entity9_ . The operation of the system will be explained in depth through browsing the _entity10_ repository _/entity10_ of _entity11_ data objects _/entity11_ created by the system during each _entity12_ question answering session _/entity12_ .	NONE entity7 entity9
A method for _entity1_ error correction _/entity1_ of _entity2_ ill-formed input _/entity2_ is described that acquires _entity3_ dialogue patterns _/entity3_ in typical usage and uses these _entity4_ patterns _/entity4_ to predict new inputs . _entity5_ Error correction _/entity5_ is done by strongly biasing _entity6_ parsing _/entity6_ toward expected _entity7_ meanings _/entity7_ unless clear evidence from the input shows the current _entity8_ sentence _/entity8_ is not expected . A _entity9_ dialogue acquisition and tracking algorithm _/entity9_ is presented along with a description of its _entity10_ implementation _/entity10_ in a _entity11_ voice interactive system _/entity11_ . A series of tests are described that show the power of the _entity12_ _P_ error correction methodology _/entity12_ when _entity13_ _C_ stereotypic dialogue _/entity13_ occurs .	NONE entity12 entity13
A research program is described in which a particular _entity1_ representational format for meaning _/entity1_ is tested as broadly as possible . In this format , developed by the LNR research group at The University of California at San Diego , _entity2_ verbs _/entity2_ are represented as interconnected sets of _entity3_ subpredicates _/entity3_ . These _entity4_ subpredicates _/entity4_ may be thought of as the almost inevitable _entity5_ _C_ inferences _/entity5_ that a _entity6_ listener _/entity6_ makes when a _entity7_ verb _/entity7_ is used in a _entity8_ _P_ sentence _/entity8_ . They confer a _entity9_ meaning structure _/entity9_ on the _entity10_ sentence _/entity10_ in which the _entity11_ verb _/entity11_ is used .	NONE entity8 entity5
In order to build robust _entity1_ automatic abstracting systems _/entity1_ , there is a need for better _entity2_ training resources _/entity2_ than are currently available . In this paper , we introduce an _entity3_ annotation scheme _/entity3_ for scientific articles which can be used to build such a _entity4_ resource _/entity4_ in a consistent way . The seven categories of the _entity5_ _C_ scheme _/entity5_ are based on _entity6_ _P_ rhetorical moves _/entity6_ of _entity7_ argumentation _/entity7_ . Our experimental results show that the _entity8_ scheme _/entity8_ is stable , reproducible and intuitive to use .	NONE entity6 entity5
In this paper , we describe a _entity1_ _C_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ _P_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity3 entity1
We propose a solution to the challenge of the _entity1_ CoNLL 2008 shared task _/entity1_ that uses a _entity2_ generative history-based latent variable model _/entity2_ to predict the most likely _entity3_ derivation _/entity3_ of a _entity4_ _C_ synchronous dependency parser _/entity4_ for both _entity5_ syntactic and semantic dependencies _/entity5_ . The submitted _entity6_ model _/entity6_ yields 79.1 % _entity7_ _P_ macro-average F1 performance _/entity7_ , for the joint task , 86.9 % _entity8_ syntactic dependencies LAS _/entity8_ and 71.0 % _entity9_ semantic dependencies F1 _/entity9_ . A larger _entity10_ model _/entity10_ trained after the deadline achieves 80.5 % _entity11_ macro-average F1 _/entity11_ , 87.6 % _entity12_ syntactic dependencies LAS _/entity12_ , and 73.1 % _entity13_ semantic dependencies F1 _/entity13_ .	NONE entity7 entity4
Multimodal interfaces require effective _entity1_ parsing _/entity1_ and understanding of _entity2_ utterances _/entity2_ whose content is distributed across multiple input modes . Johnston 1998 presents an approach in which strategies for _entity3_ multimodal integration _/entity3_ are stated declaratively using a _entity4_ unification-based grammar _/entity4_ that is used by a _entity5_ multidimensional chart parser _/entity5_ to compose inputs . This approach is highly expressive and supports a broad class of _entity6_ interfaces _/entity6_ , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input . In this paper , we present an alternative approach in which _entity7_ multimodal parsing and understanding _/entity7_ are achieved using a _entity8_ weighted finite-state device _/entity8_ which takes _entity9_ speech and gesture streams _/entity9_ as inputs and outputs their joint interpretation . This approach is significantly more efficient , enables tight-coupling of multimodal understanding with _entity10_ _P_ speech recognition _/entity10_ , and provides a general probabilistic framework for _entity11_ _C_ multimodal ambiguity resolution _/entity11_ .	NONE entity10 entity11
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ _P_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ _C_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity8 entity11
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ _P_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ _C_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity12 entity14
The _entity1_ GE NLToolset _/entity1_ is a set of _entity2_ text interpretation tools _/entity2_ designed to be easily adapted to new _entity3_ _C_ domains _/entity3_ . This report summarizes the system and its performance on the _entity4_ _P_ MUC-4 task _/entity4_ .	NONE entity4 entity3
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ _P_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ _C_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity8 entity10
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ _P_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ _C_ parsers _/entity24_ .	NONE entity21 entity24
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ _C_ semantic equivalence _/entity5_ and _entity6_ _P_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity6 entity5
In this paper , we will describe a _entity1_ search tool _/entity1_ for a huge set of _entity2_ ngrams _/entity2_ . The tool supports _entity3_ queries _/entity3_ with an arbitrary number of _entity4_ wildcards _/entity4_ . It takes a fraction of a second for a search , and can provide the _entity5_ fillers _/entity5_ of the _entity6_ wildcards _/entity6_ . The system runs on a single Linux PC with reasonable size _entity7_ _C_ memory _/entity7_ ( less than 4GB ) and _entity8_ _P_ disk space _/entity8_ ( less than 400GB ) . This system can be a very useful tool for _entity9_ linguistic knowledge discovery _/entity9_ and other _entity10_ NLP tasks _/entity10_ .	NONE entity8 entity7
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ context-free parsing _/entity2_ is presented . The _entity3_ parser _/entity3_ computes a compact _entity4_ parse forest representation _/entity4_ of the complete set of possible _entity5_ analyses for large treebank grammars _/entity5_ and long _entity6_ input sentences _/entity6_ . The _entity7_ parser _/entity7_ uses _entity8_ bit-vector operations _/entity8_ to parallelise the _entity9_ _P_ basic parsing operations _/entity9_ . The _entity10_ _C_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	NONE entity9 entity10
We give an analysis of _entity1_ ellipsis resolution _/entity1_ in terms of a straightforward _entity2_ discourse copying algorithm _/entity2_ that correctly predicts a wide range of phenomena . The treatment does not suffer from problems inherent in _entity3_ _P_ identity-of-relations analyses _/entity3_ . Furthermore , in contrast to the approach of Dalrymple et al . [ 1991 ] , the treatment directly encodes the intuitive distinction between _entity4_ full NPs _/entity4_ and the _entity5_ referential elements _/entity5_ that corefer with them through what we term _entity6_ _C_ role linking _/entity6_ . The correct _entity7_ predictions _/entity7_ for several problematic examples of _entity8_ ellipsis _/entity8_ naturally result . Finally , the analysis extends directly to other _entity9_ discourse copying phenomena _/entity9_ .	NONE entity3 entity6
Reducing _entity1_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ LM pruning _/entity3_ . They are probability , _entity4_ rank _/entity4_ , and _entity5_ _P_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ _C_ pruning criteria _/entity6_ in a real application of _entity7_ Chinese text input _/entity7_ in terms of _entity8_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ CER _/entity13_ .	NONE entity5 entity6
This article is devoted to the problem of _entity1_ _P_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ _C_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity1 entity4
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ LCS meaning components _/entity6_ . Our _entity7_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ _P_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ _C_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity10 entity12
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ sentence plans _/entity12_ for a given _entity13_ _P_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ _C_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ plan _/entity16_ . The _entity17_ SPR _/entity17_ uses _entity18_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ top human-ranked sentence plan _/entity22_ .	NONE entity13 entity15
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ _C_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ _P_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity5 entity3
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ multiple speakers _/entity5_ without _entity6_ _C_ explicit speaker enrollment _/entity6_ . Combining _entity7_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ _P_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity9 entity6
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ _C_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ _P_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity9 entity7
We present an operable definition of _entity1_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ discourse model _/entity3_ and _entity4_ _C_ knowledge store _/entity4_ is introduced enabling the _entity5_ decomposition _/entity5_ and _entity6_ formal representation _/entity6_ of its _entity7_ _P_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity7 entity4
In this paper , we describe a _entity1_ _C_ phrase-based unigram model _/entity1_ for _entity2_ _P_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity2 entity1
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ _P_ construction _/entity14_ can give rise to . A _entity15_ _C_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity14 entity15
Current _entity1_ natural language interfaces _/entity1_ have concentrated largely on determining the literal _entity2_ meaning _/entity2_ of _entity3_ input _/entity3_ from their _entity4_ users _/entity4_ . While such _entity5_ decoding _/entity5_ is an essential underpinning , much recent work suggests that _entity6_ natural language interfaces _/entity6_ will never appear cooperative or graceful unless they also incorporate numerous _entity7_ non-literal aspects of communication _/entity7_ , such as robust _entity8_ communication procedures _/entity8_ . This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these _entity9_ non-literal aspects of communication _/entity9_ ; that the new technology of powerful _entity10_ personal computers _/entity10_ with integral _entity11_ _P_ graphics displays _/entity11_ offers techniques superior to those of humans for these aspects , while still satisfying _entity12_ human communication needs _/entity12_ . The paper proposes _entity13_ interfaces _/entity13_ based on a judicious mixture of these techniques and the still valuable methods of more traditional _entity14_ _C_ natural language interfaces _/entity14_ .	NONE entity11 entity14
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ editor _/entity3_ was to provide an adequate tool to help _entity4_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ linguistic theory _/entity6_ . If we want valuable _entity7_ _C_ lexicons _/entity7_ and _entity8_ grammars _/entity8_ to achieve complex _entity9_ _P_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity9 entity7
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to disambiguate various relations between _entity2_ named entities _/entity2_ by use of various _entity3_ lexical and syntactic features _/entity3_ from the _entity4_ contexts _/entity4_ . It works by calculating _entity5_ eigenvectors _/entity5_ of an _entity6_ adjacency graph _/entity6_ 's _entity7_ _P_ Laplacian _/entity7_ to recover a _entity8_ submanifold _/entity8_ of data from a _entity9_ _C_ high dimensionality space _/entity9_ and then performing _entity10_ cluster number estimation _/entity10_ on the _entity11_ eigenvectors _/entity11_ . Experiment results on _entity12_ ACE corpora _/entity12_ show that this _entity13_ spectral clustering based approach _/entity13_ outperforms the other _entity14_ clustering methods _/entity14_ .	NONE entity7 entity9
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ _C_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ _P_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity18 entity16
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ _C_ bigram _/entity19_ , incorporates a _entity20_ _P_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity20 entity19
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ _C_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ _P_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity6 entity3
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ _P_ question phrases _/entity5_ in _entity6_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ _C_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ MRR _/entity14_ .	NONE entity5 entity8
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ _C_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ _P_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity11 entity8
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ _C_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ _P_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity22 entity19
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ _C_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ _P_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity20 entity17
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ _P_ rule-invocation strategies _/entity11_ within _entity12_ _C_ context-free chart parsing _/entity12_ .	PART_WHOLE entity11 entity12
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ _P_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ _C_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity7 entity10
We describe a novel technique and implemented system for constructing a _entity1_ subcategorization dictionary _/entity1_ from _entity2_ textual corpora _/entity2_ . Each _entity3_ dictionary entry _/entity3_ encodes the _entity4_ relative frequency of occurrence _/entity4_ of a comprehensive set of _entity5_ subcategorization classes _/entity5_ for _entity6_ English _/entity6_ . An initial experiment , on a sample of 14 _entity7_ verbs _/entity7_ which exhibit _entity8_ multiple complementation patterns _/entity8_ , demonstrates that the technique achieves _entity9_ accuracy _/entity9_ comparable to previous approaches , which are all limited to a highly restricted set of _entity10_ _C_ subcategorization classes _/entity10_ . We also demonstrate that a _entity11_ subcategorization dictionary _/entity11_ built with the system improves the _entity12_ _P_ accuracy _/entity12_ of a _entity13_ parser _/entity13_ by an appreciable amount	NONE entity12 entity10
_entity1_ Taiwan Child Language Corpus _/entity1_ contains _entity2_ _C_ scripts _/entity2_ transcribed from about 330 hours of _entity3_ recordings _/entity3_ of fourteen young children from _entity4_ _P_ Southern Min Chinese _/entity4_ speaking families in Taiwan . The format of the _entity5_ corpus _/entity5_ adopts the _entity6_ Child Language Data Exchange System ( CHILDES ) _/entity6_ . The size of the _entity7_ corpus _/entity7_ is about 1.6 million _entity8_ words _/entity8_ . In this paper , we describe _entity9_ data collection _/entity9_ , _entity10_ transcription _/entity10_ , _entity11_ word segmentation _/entity11_ , and _entity12_ part-of-speech annotation _/entity12_ of this _entity13_ corpus _/entity13_ . Applications of the _entity14_ corpus _/entity14_ are also discussed .	NONE entity4 entity2
We present results on _entity1_ addressee identification _/entity1_ in _entity2_ four-participants face-to-face meetings _/entity2_ using _entity3_ Bayesian Network _/entity3_ and _entity4_ Naive Bayes classifiers _/entity4_ . First , we investigate how well the _entity5_ addressee _/entity5_ of a _entity6_ dialogue act _/entity6_ can be predicted based on _entity7_ gaze _/entity7_ , _entity8_ utterance _/entity8_ and _entity9_ conversational context features _/entity9_ . Then , we explore whether information about _entity10_ meeting context _/entity10_ can aid _entity11_ classifiers _/entity11_ ' _entity12_ _P_ performances _/entity12_ . Both _entity13_ classifiers _/entity13_ perform the best when _entity14_ conversational context _/entity14_ and _entity15_ _C_ utterance features _/entity15_ are combined with _entity16_ speaker 's gaze information _/entity16_ . The _entity17_ classifiers _/entity17_ show little _entity18_ gain _/entity18_ from information about _entity19_ meeting context _/entity19_ .	NONE entity12 entity15
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ _P_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ _C_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity11 entity14
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ LCS meaning components _/entity6_ . Our _entity7_ _P_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ _C_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity7 entity8
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ _P_ learning algorithm _/entity10_ from _entity11_ _C_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity10 entity11
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ _P_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ _C_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity10 entity11
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ _P_ feature structure-directed generation _/entity2_ , developed for a _entity3_ _C_ dialogue translation system _/entity3_ . The system utilizes _entity4_ typed feature structures _/entity4_ to control the _entity5_ top-down derivation _/entity5_ in a declarative way . This _entity6_ generation system _/entity6_ also uses _entity7_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ derivation tree _/entity8_ . The _entity9_ grammar _/entity9_ for this _entity10_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	USAGE entity2 entity3
We argue in favor of the the use of _entity1_ labeled directed graph _/entity1_ to represent various types of _entity2_ linguistic structures _/entity2_ , and illustrate how this allows one to view _entity3_ NLP tasks _/entity3_ as _entity4_ _C_ graph transformations _/entity4_ . We present a general method for learning such _entity5_ transformations _/entity5_ from an _entity6_ annotated corpus _/entity6_ and describe experiments with two applications of the method : _entity7_ _P_ identification of non-local depenencies _/entity7_ ( using _entity8_ Penn Treebank data _/entity8_ ) and _entity9_ semantic role labeling _/entity9_ ( using _entity10_ Proposition Bank data _/entity10_ ) .	NONE entity7 entity4
In this paper we discuss a proposed _entity1_ user knowledge modeling architecture _/entity1_ for the _entity2_ _C_ ICICLE system _/entity2_ , a _entity3_ language tutoring application _/entity3_ for deaf learners of _entity4_ _P_ written English _/entity4_ . The model will represent the _entity5_ language proficiency _/entity5_ of the user and is designed to be referenced during both _entity6_ writing analysis _/entity6_ and _entity7_ feedback production _/entity7_ . We motivate our _entity8_ model design _/entity8_ by citing relevant research on _entity9_ second language and cognitive skill acquisition _/entity9_ , and briefly discuss preliminary empirical evidence supporting the _entity10_ design _/entity10_ . We conclude by showing how our _entity11_ design _/entity11_ can provide a rich and _entity12_ robust information base _/entity12_ to a language assessment / correction application by modeling _entity13_ user proficiency _/entity13_ at a high level of granularity and specificity .	NONE entity4 entity2
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ _C_ antonymy _/entity6_ , _entity7_ _P_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity7 entity6
This paper proposes the _entity1_ Hierarchical Directed Acyclic Graph ( HDAG ) Kernel _/entity1_ for _entity2_ structured natural language data _/entity2_ . The _entity3_ HDAG Kernel _/entity3_ directly accepts several levels of both _entity4_ chunks _/entity4_ and their _entity5_ _C_ relations _/entity5_ , and then efficiently computes the _entity6_ weighed sum _/entity6_ of the number of common _entity7_ _P_ attribute sequences _/entity7_ of the _entity8_ HDAGs _/entity8_ . We applied the proposed method to _entity9_ question classification _/entity9_ and _entity10_ sentence alignment tasks _/entity10_ to evaluate its performance as a _entity11_ similarity measure _/entity11_ and a _entity12_ kernel function _/entity12_ . The results of the experiments demonstrate that the _entity13_ HDAG Kernel _/entity13_ is superior to other _entity14_ kernel functions _/entity14_ and _entity15_ baseline methods _/entity15_ .	NONE entity7 entity5
Previous research has demonstrated the utility of _entity1_ clustering _/entity1_ in inducing _entity2_ _P_ semantic verb classes _/entity2_ from undisambiguated _entity3_ corpus data _/entity3_ . We describe a new approach which involves clustering _entity4_ subcategorization frame ( SCF ) _/entity4_ distributions using the _entity5_ _C_ Information Bottleneck _/entity5_ and _entity6_ nearest neighbour _/entity6_ methods . In contrast to previous work , we particularly focus on clustering _entity7_ polysemic verbs _/entity7_ . A novel _entity8_ evaluation scheme _/entity8_ is proposed which accounts for the effect of _entity9_ polysemy _/entity9_ on the _entity10_ clusters _/entity10_ , offering us a good insight into the potential and limitations of _entity11_ semantically classifying _/entity11_ _entity12_ undisambiguated SCF data _/entity12_ .	NONE entity2 entity5
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ system combination _/entity3_ for _entity4_ unsupervised WSD _/entity4_ . We investigate several _entity5_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ combination methods _/entity7_ rely on _entity8_ _P_ predominant senses _/entity8_ which are derived automatically from _entity9_ _C_ raw text _/entity9_ . Experiments using the _entity10_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity8 entity9
The _entity1_ GE NLToolset _/entity1_ is a set of _entity2_ _P_ text interpretation tools _/entity2_ designed to be easily adapted to new _entity3_ _C_ domains _/entity3_ . This report summarizes the system and its performance on the _entity4_ MUC-4 task _/entity4_ .	NONE entity2 entity3
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ _P_ WWER _/entity14_ based on a _entity15_ _C_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity14 entity15
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ context-free parsing _/entity2_ is presented . The _entity3_ parser _/entity3_ computes a compact _entity4_ parse forest representation _/entity4_ of the complete set of possible _entity5_ analyses for large treebank grammars _/entity5_ and long _entity6_ _P_ input sentences _/entity6_ . The _entity7_ parser _/entity7_ uses _entity8_ bit-vector operations _/entity8_ to parallelise the _entity9_ _C_ basic parsing operations _/entity9_ . The _entity10_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	NONE entity6 entity9
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ _C_ traditional MT architectural designs _/entity4_ . A new _entity5_ semantic representation _/entity5_ is proposed that uses _entity6_ _P_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ interlingua _/entity9_ within a new _entity10_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ direct approaches _/entity12_ into a single system .	NONE entity6 entity4
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ TAGs _/entity14_ to be used beyond their role in _entity15_ _C_ syntax proper _/entity15_ . We discuss the application of _entity16_ _P_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity16 entity15
In this paper a novel solution to automatic and _entity1_ _P_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ _C_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ WSI algorithm _/entity14_ .	NONE entity1 entity2
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ _P_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ _C_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity6 entity9
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ _P_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ _C_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity8 entity9
In this paper a _entity1_ morphological component _/entity1_ with a limited capability to automatically interpret ( and generate ) _entity2_ derived words _/entity2_ is presented . The system combines an extended _entity3_ two-level morphology _/entity3_ [ Trost , 1991a ; Trost , 1991b ] with a _entity4_ feature-based word grammar _/entity4_ building on a _entity5_ hierarchical lexicon _/entity5_ . _entity6_ Polymorphemic stems _/entity6_ not explicitly stored in the _entity7_ _P_ lexicon _/entity7_ are given a _entity8_ compositional interpretation _/entity8_ . That way the system allows to minimize redundancy in the _entity9_ _C_ lexicon _/entity9_ because _entity10_ derived words _/entity10_ that are transparent need not to be stored explicitly . Also , _entity11_ words formed ad-hoc _/entity11_ can be recognized correctly . The system is implemented in CommonLisp and has been tested on examples from _entity12_ German derivation _/entity12_ .	NONE entity7 entity9
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ _P_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ _C_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity14 entity15
_entity1_ _P_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ _C_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity1 entity4
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ _C_ hard decisions _/entity17_ using the _entity18_ _P_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity18 entity17
In this paper , we describe the research using _entity1_ machine learning techniques _/entity1_ to build a _entity2_ comma checker _/entity2_ to be integrated in a _entity3_ grammar checker _/entity3_ for _entity4_ Basque _/entity4_ . After several experiments , and trained with a little _entity5_ corpus _/entity5_ of 100,000 _entity6_ words _/entity6_ , the system guesses correctly not placing _entity7_ commas _/entity7_ with a _entity8_ _P_ precision _/entity8_ of 96 % and a _entity9_ recall _/entity9_ of 98 % . It also gets a _entity10_ _C_ precision _/entity10_ of 70 % and a _entity11_ recall _/entity11_ of 49 % in the task of placing _entity12_ commas _/entity12_ . Finally , we have shown that these results can be improved using a bigger and a more homogeneous _entity13_ corpus _/entity13_ to train , that is , a bigger _entity14_ corpus _/entity14_ written by one unique _entity15_ author _/entity15_ .	NONE entity8 entity10
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ _P_ queries _/entity11_ containing them . I show that the _entity12_ _C_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity11 entity12
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ _P_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ _C_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	NONE entity12 entity15
We propose a solution to the challenge of the _entity1_ CoNLL 2008 shared task _/entity1_ that uses a _entity2_ generative history-based latent variable model _/entity2_ to predict the most likely _entity3_ derivation _/entity3_ of a _entity4_ synchronous dependency parser _/entity4_ for both _entity5_ syntactic and semantic dependencies _/entity5_ . The submitted _entity6_ model _/entity6_ yields 79.1 % _entity7_ macro-average F1 performance _/entity7_ , for the joint task , 86.9 % _entity8_ syntactic dependencies LAS _/entity8_ and 71.0 % _entity9_ _P_ semantic dependencies F1 _/entity9_ . A larger _entity10_ model _/entity10_ trained after the deadline achieves 80.5 % _entity11_ _C_ macro-average F1 _/entity11_ , 87.6 % _entity12_ syntactic dependencies LAS _/entity12_ , and 73.1 % _entity13_ semantic dependencies F1 _/entity13_ .	NONE entity9 entity11
This paper describes the understanding process of the _entity1_ spatial descriptions _/entity1_ in _entity2_ Japanese _/entity2_ . In order to understand the described _entity3_ world _/entity3_ , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space . It is done by an experimental _entity4_ computer program _/entity4_ _entity5_ SPRINT _/entity5_ , which takes _entity6_ natural language texts _/entity6_ and produces a _entity7_ _P_ model _/entity7_ of the described _entity8_ _C_ world _/entity8_ . To reconstruct the _entity9_ model _/entity9_ , the authors extract the _entity10_ qualitative spatial constraints _/entity10_ from the _entity11_ text _/entity11_ , and represent them as the _entity12_ numerical constraints _/entity12_ on the _entity13_ spatial attributes _/entity13_ of the _entity14_ entities _/entity14_ . This makes it possible to express the vagueness of the _entity15_ spatial concepts _/entity15_ and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints . The interpretation reflects the _entity16_ temporary belief _/entity16_ about the _entity17_ world _/entity17_ .	MODEL-FEATURE entity7 entity8
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ _P_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ _C_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity2 entity5
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ _P_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ _C_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity22 entity24
This paper describes a new , _entity1_ large scale discourse-level annotation _/entity1_ project - the _entity2_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ discourse structure _/entity3_ that is based on identifying _entity4_ discourse connectives _/entity4_ and their _entity5_ arguments _/entity5_ . The _entity6_ PDTB _/entity6_ is being built directly on top of the _entity7_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ _P_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ inter-annotator agreement _/entity11_ - both the _entity12_ _C_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity10 entity12
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ _P_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ _C_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity12 entity15
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ _C_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ _P_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity3 entity2
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ _C_ Comparator _/entity18_ . Though some details of the _entity19_ _P_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity19 entity18
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ _C_ bilingual corpora _/entity2_ , which utilizes the _entity3_ _P_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity3 entity2
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ feature structure-directed generation _/entity2_ , developed for a _entity3_ dialogue translation system _/entity3_ . The system utilizes _entity4_ typed feature structures _/entity4_ to control the _entity5_ top-down derivation _/entity5_ in a declarative way . This _entity6_ generation system _/entity6_ also uses _entity7_ _P_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ derivation tree _/entity8_ . The _entity9_ grammar _/entity9_ for this _entity10_ _C_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	NONE entity7 entity10
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ _P_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ _C_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity4 entity7
In this paper , we improve an _entity1_ _P_ unsupervised learning method _/entity1_ using the _entity2_ _C_ Expectation-Maximization ( EM ) algorithm _/entity2_ proposed by Nigam et al . for _entity3_ text classification problems _/entity3_ in order to apply it to _entity4_ word sense disambiguation ( WSD ) problems _/entity4_ . The improved method stops the _entity5_ EM algorithm _/entity5_ at the _entity6_ optimum iteration number _/entity6_ . To estimate that number , we propose two methods . In experiments , we solved 50 _entity7_ noun WSD problems _/entity7_ in the _entity8_ Japanese Dictionary Task in SENSEVAL2 _/entity8_ . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for _entity9_ verb WSD problems _/entity9_ .	NONE entity1 entity2
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ _C_ for which only scarce _/entity15_ resources _entity16_ _P_ are available . _/entity16_	NONE entity16 entity15
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ _C_ semantic information _/entity13_ such as _entity14_ _P_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity14 entity13
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ _C_ word vectors _/entity3_ in the _entity4_ _P_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ dictionary-based word vectors _/entity10_ better reflect _entity11_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity4 entity3
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ _C_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ _P_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	NONE entity21 entity18
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ _C_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ _P_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity17 entity15
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ _C_ suffix _/entity15_ , achieving similar _entity16_ _P_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity16 entity15
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ _P_ shallow linguistic features _/entity5_ of _entity6_ _C_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	MODEL-FEATURE entity5 entity6
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ _P_ context-free parsing _/entity2_ is presented . The _entity3_ parser _/entity3_ computes a compact _entity4_ parse forest representation _/entity4_ of the complete set of possible _entity5_ _C_ analyses for large treebank grammars _/entity5_ and long _entity6_ input sentences _/entity6_ . The _entity7_ parser _/entity7_ uses _entity8_ bit-vector operations _/entity8_ to parallelise the _entity9_ basic parsing operations _/entity9_ . The _entity10_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	NONE entity2 entity5
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ _C_ word clusters _/entity7_ or _entity8_ _P_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity8 entity7
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ _C_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ _P_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity6 entity5
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ _P_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ _C_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity9 entity12
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ _P_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ _C_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity5 entity7
We present the first known _entity1_ _P_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ _C_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity1 entity2
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ _P_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ _C_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity2 entity5
Following recent developments in the _entity1_ _P_ automatic evaluation _/entity1_ of _entity2_ machine translation _/entity2_ and _entity3_ document summarization _/entity3_ , we present a similar approach , implemented in a measure called _entity4_ _C_ POURPRE _/entity4_ , for _entity5_ automatically evaluating answers to definition questions _/entity5_ . Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response . The lack of automatic methods for _entity6_ scoring system output _/entity6_ is an impediment to progress in the field , which we address with this work . Experiments with the _entity7_ TREC 2003 and TREC 2004 QA tracks _/entity7_ indicate that _entity8_ rankings _/entity8_ produced by our metric correlate highly with _entity9_ official rankings _/entity9_ , and that _entity10_ POURPRE _/entity10_ outperforms direct application of existing metrics .	NONE entity1 entity4
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ _C_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ _P_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity6 entity4
We propose a method that automatically generates _entity1_ paraphrase _/entity1_ sets from _entity2_ seed sentences _/entity2_ to be used as _entity3_ reference sets _/entity3_ in objective _entity4_ machine translation evaluation measures _/entity4_ like _entity5_ BLEU _/entity5_ and _entity6_ NIST _/entity6_ . We measured the quality of the _entity7_ paraphrases _/entity7_ produced in an experiment , i.e. , ( i ) their _entity8_ grammaticality _/entity8_ : at least 99 % correct _entity9_ sentences _/entity9_ ; ( ii ) their _entity10_ _C_ equivalence in meaning _/entity10_ : at least 96 % correct _entity11_ paraphrases _/entity11_ either by _entity12_ _P_ meaning equivalence _/entity12_ or _entity13_ entailment _/entity13_ ; and , ( iii ) the amount of internal _entity14_ lexical and syntactical variation _/entity14_ in a set of _entity15_ paraphrases _/entity15_ : slightly superior to that of _entity16_ hand-produced sets _/entity16_ . The _entity17_ paraphrase _/entity17_ sets produced by this method thus seem adequate as _entity18_ reference sets _/entity18_ to be used for _entity19_ MT evaluation _/entity19_ .	NONE entity12 entity10
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ _C_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ _P_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity22 entity20
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ _C_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ _P_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity15 entity12
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ _P_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ _C_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity23 entity25
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ _C_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ _P_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity16 entity15
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ _P_ learning methodology _/entity2_ applicable in _entity3_ _C_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity2 entity3
In this paper , we introduce a _entity1_ generative probabilistic optical character recognition ( OCR ) model _/entity1_ that describes an end-to-end process in the _entity2_ noisy channel framework _/entity2_ , progressing from generation of _entity3_ true text _/entity3_ through its transformation into the _entity4_ noisy output _/entity4_ of an _entity5_ OCR system _/entity5_ . The _entity6_ _P_ model _/entity6_ is designed for use in _entity7_ error correction _/entity7_ , with a focus on _entity8_ post-processing _/entity8_ the _entity9_ _C_ output _/entity9_ of black-box _entity10_ OCR systems _/entity10_ in order to make it more useful for _entity11_ NLP tasks _/entity11_ . We present an implementation of the _entity12_ model _/entity12_ based on _entity13_ finite-state models _/entity13_ , demonstrate the _entity14_ model _/entity14_ 's ability to significantly reduce _entity15_ character and word error rate _/entity15_ , and provide evaluation results involving _entity16_ automatic extraction _/entity16_ of _entity17_ translation lexicons _/entity17_ from _entity18_ printed text _/entity18_ .	NONE entity6 entity9
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ _C_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ _P_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity6 entity4
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ _C_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ _P_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity7 entity5
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ _P_ ontology _/entity3_ . We apply our system to the task of _entity4_ _C_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity3 entity4
There are several approaches that model _entity1_ _P_ information extraction _/entity1_ as a _entity2_ token classification task _/entity2_ , using various _entity3_ tagging strategies _/entity3_ to combine multiple _entity4_ _C_ tokens _/entity4_ . We describe the _entity5_ tagging strategies _/entity5_ that can be found in the literature and evaluate their relative performances . We also introduce a new strategy , called _entity6_ Begin/After tagging _/entity6_ or _entity7_ BIA _/entity7_ , and show that it is competitive to the best other strategies .	NONE entity1 entity4
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ _C_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ _P_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity11 entity8
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ _C_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ _P_ natural language generation _/entity45_ .	NONE entity45 entity43
This paper concerns the _entity1_ _C_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ _P_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity3 entity1
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ _C_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ _P_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity9 entity6
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ _P_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ _C_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	COMPARE entity6 entity8
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ _P_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ _C_ WSD accuracy _/entity15_ .	NONE entity13 entity15
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ _P_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ _C_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity3 entity4
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ _C_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ _P_ logical form _/entity16_ .	NONE entity16 entity13
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ _P_ ambiguity _/entity13_ that a particular _entity14_ _C_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity13 entity14
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ _P_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ _C_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity12 entity14
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ _C_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ statistical ranking _/entity6_ than a previous approach to _entity7_ _P_ statistical generation _/entity7_ . An efficient _entity8_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ lattice-based approach _/entity9_ .	NONE entity7 entity5
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ _P_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ _C_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity10 entity12
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ _P_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ _C_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity24 entity25
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ sublanguage _/entity3_ , is proposed for identifying _entity4_ unknown words _/entity4_ , especially _entity5_ personal names _/entity5_ , in _entity6_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ adaptive dynamic word formation _/entity8_ , _entity9_ _C_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ _P_ WI systems _/entity12_ without the _entity13_ name identification _/entity13_ capability .	NONE entity12 entity9
A research program is described in which a particular _entity1_ representational format for meaning _/entity1_ is tested as broadly as possible . In this format , developed by the LNR research group at The University of California at San Diego , _entity2_ verbs _/entity2_ are represented as interconnected sets of _entity3_ subpredicates _/entity3_ . These _entity4_ _P_ subpredicates _/entity4_ may be thought of as the almost inevitable _entity5_ _C_ inferences _/entity5_ that a _entity6_ listener _/entity6_ makes when a _entity7_ verb _/entity7_ is used in a _entity8_ sentence _/entity8_ . They confer a _entity9_ meaning structure _/entity9_ on the _entity10_ sentence _/entity10_ in which the _entity11_ verb _/entity11_ is used .	NONE entity4 entity5
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ _C_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ _P_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity10 entity9
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ _P_ Discourse processing _/entity37_ requires recognizing how the _entity38_ _C_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity37 entity38
This paper discusses two problems that arise in the _entity1_ Generation _/entity1_ of _entity2_ Referring Expressions _/entity2_ : ( a ) _entity3_ _P_ numeric-valued attributes _/entity3_ , such as size or location ; ( b ) _entity4_ perspective-taking _/entity4_ in _entity5_ reference _/entity5_ . Both problems , it is argued , can be resolved if some structure is imposed on the available knowledge prior to _entity6_ _C_ content determination _/entity6_ . We describe a _entity7_ clustering algorithm _/entity7_ which is sufficiently general to be applied to these diverse problems , discuss its application , and evaluate its performance .	NONE entity3 entity6
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ _P_ formatted weather data _/entity2_ . Such _entity3_ _C_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ stereotyped text structure _/entity5_ . _entity6_ RAREAS _/entity6_ draws on several kinds of _entity7_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ bilingual or multi-lingual texts _/entity9_ .	NONE entity2 entity3
We suggest a new goal and _entity1_ evaluation criterion _/entity1_ for _entity2_ word similarity measures _/entity2_ . The new criterion _entity3_ meaning-entailing substitutability _/entity3_ fits the needs of _entity4_ _C_ semantic-oriented NLP applications _/entity4_ and can be evaluated directly ( independent of an application ) at a good level of _entity5_ human agreement _/entity5_ . Motivated by this _entity6_ _P_ semantic criterion _/entity6_ we analyze the empirical quality of _entity7_ distributional word feature vectors _/entity7_ and its impact on _entity8_ word similarity results _/entity8_ , proposing an objective measure for evaluating _entity9_ feature vector quality _/entity9_ . Finally , a novel _entity10_ feature weighting and selection function _/entity10_ is presented , which yields superior _entity11_ feature vectors _/entity11_ and better _entity12_ word similarity performance _/entity12_ .	NONE entity6 entity4
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ _P_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ _C_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity11 entity14
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ _C_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ _P_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity21 entity19
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ _P_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ _C_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity28 entity30
This paper describes a _entity1_ _P_ computational model _/entity1_ of _entity2_ word segmentation _/entity2_ and presents simulation results on _entity3_ _C_ realistic acquisition _/entity3_ . In particular , we explore the capacity and limitations of _entity4_ statistical learning mechanisms _/entity4_ that have recently gained prominence in _entity5_ cognitive psychology _/entity5_ and _entity6_ linguistics _/entity6_ .	NONE entity1 entity3
This article deals with the _entity1_ _C_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ _P_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity4 entity1
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ _C_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ _P_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity5 entity2
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ _P_ performance _/entity12_ of a _entity13_ _C_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity12 entity13
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ _P_ language model _/entity5_ , traditional _entity6_ _C_ statistical approaches _/entity6_ , which resolve _entity7_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity5 entity6
In this paper we explore a new _entity1_ _C_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ _P_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity4 entity1
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ _P_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ _C_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity5 entity8
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ delimiters _/entity3_ , which poses a challenge to _entity4_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ _P_ MT _/entity5_ , the widely used approach is to apply a _entity6_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ _C_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity5 entity8
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ _C_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ high-quality data _/entity7_ that allow the comparison of both _entity8_ _P_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ automatic metrics _/entity11_ used within _entity12_ MT _/entity12_ are also discussed in this regard .	NONE entity8 entity5
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ word sense _/entity3_ , a _entity4_ topic signature _/entity4_ is a set of _entity5_ words _/entity5_ that tend to co-occur with it . _entity6_ _P_ Topic signatures _/entity6_ can be useful in a number of _entity7_ _C_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ topic signatures _/entity15_ on a _entity16_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ standard WSD datasets _/entity18_ , with promising results .	USAGE entity6 entity7
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ _P_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ _C_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity17 entity19
`` To explain complex phenomena , an _entity1_ explanation system _/entity1_ must be able to select information from a formal representation of _entity2_ domain knowledge _/entity2_ , organize the selected information into _entity3_ multisentential discourse plans _/entity3_ , and realize the _entity4_ _C_ discourse plans _/entity4_ in text . Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for _entity5_ explanation _/entity5_ , empirical results have been limited . This paper reports on a seven-year effort to empirically study _entity6_ explanation generation _/entity6_ from _entity7_ _P_ semantically rich , large-scale knowledge bases _/entity7_ . In particular , it describes a _entity8_ robust explanation system _/entity8_ that constructs _entity9_ multisentential and multi-paragraph explanations _/entity9_ from the a _entity10_ large-scale knowledge base _/entity10_ in the domain of botanical anatomy , physiology , and development . We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system . In this evaluation , scored within `` '' half a grade '' '' of domain experts , and its performance exceeded that of one of the domain experts . ''	NONE entity7 entity4
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ sense-tagged training data _/entity4_ from _entity5_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ _C_ nouns _/entity6_ in the _entity7_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ _P_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ WSD programs _/entity14_ .	NONE entity9 entity6
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ _C_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ _P_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity10 entity7
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ corpus of sentences _/entity3_ in the domain of the _entity4_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ _C_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ _P_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity8 entity5
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ _C_ parse tree _/entity7_ into the _entity8_ _P_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity8 entity7
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ _P_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ _C_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity8 entity9
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ _C_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ LCS meaning components _/entity6_ . Our _entity7_ _P_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity7 entity4
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ _C_ language pairs _/entity5_ like _entity6_ _P_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity6 entity5
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ _C_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ _P_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity10 entity8
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ _C_ taxonomic similarity _/entity8_ and _entity9_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ dictionary-based word vectors _/entity10_ better reflect _entity11_ _P_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity11 entity8
The _entity1_ TAP-XL Automated Analyst 's Assistant _/entity1_ is an application designed to help an _entity2_ English _/entity2_ -speaking analyst write a _entity3_ _C_ topical report _/entity3_ , culling information from a large inflow of _entity4_ multilingual , multimedia data _/entity4_ . It gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other _entity5_ _P_ languages _/entity5_ by leveraging _entity6_ human language technology _/entity6_ .	NONE entity5 entity3
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ _C_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ _P_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	MODEL-FEATURE entity4 entity3
In this paper we present our recent work on harvesting _entity1_ English-Chinese bitexts _/entity1_ of the laws of Hong Kong from the _entity2_ Web _/entity2_ and aligning them to the _entity3_ subparagraph _/entity3_ level via utilizing the _entity4_ numbering system _/entity4_ in the _entity5_ legal text hierarchy _/entity5_ . Basic methodology and practical techniques are reported in detail . The resultant _entity6_ bilingual corpus _/entity6_ , 10.4M _entity7_ English words _/entity7_ and 18.3M _entity8_ Chinese characters _/entity8_ , is an authoritative and comprehensive _entity9_ _P_ text collection _/entity9_ covering the specific and special domain of HK laws . It is particularly valuable to _entity10_ _C_ empirical MT research _/entity10_ . This piece of work has also laid a foundation for exploring and harvesting _entity11_ English-Chinese bitexts _/entity11_ in a larger volume from the _entity12_ Web _/entity12_ .	NONE entity9 entity10
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ _P_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ _C_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity7 entity10
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ editor _/entity3_ was to provide an adequate tool to help _entity4_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ linguistic theory _/entity6_ . If we want valuable _entity7_ lexicons _/entity7_ and _entity8_ grammars _/entity8_ to achieve complex _entity9_ _P_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ _C_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity9 entity12
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ _P_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ _C_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity12 entity14
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ _C_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ _P_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity5 entity3
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ _C_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ _P_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity13 entity10
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ _C_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ _P_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity10 entity9
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ _P_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ _C_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity14 entity15
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ _C_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ _P_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity15 entity12
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ _P_ domains _/entity17_ easily and it is especially useful for _entity18_ _C_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity17 entity18
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ _P_ keywords _/entity7_ as the _entity8_ _C_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity7 entity8
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ editor _/entity3_ was to provide an adequate tool to help _entity4_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ linguistic theory _/entity6_ . If we want valuable _entity7_ lexicons _/entity7_ and _entity8_ grammars _/entity8_ to achieve complex _entity9_ _C_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ _P_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity12 entity9
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ _C_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ _P_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity9 entity6
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ _C_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ _P_ extensional reference _/entity25_ .	NONE entity25 entity24
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ _C_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ _P_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity18 entity16
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ human-computer interaction _/entity6_ . We analyzed _entity7_ eye gaze _/entity7_ , _entity8_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ _P_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ _C_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity12 entity13
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ _P_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ _C_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity6 entity9
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ _P_ large-scale database _/entity10_ that provides the _entity11_ _C_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity10 entity11
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ CCRs _/entity3_ are _entity4_ _P_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ _C_ categories _/entity5_ in _entity6_ local trees _/entity6_ which allow the _entity7_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	MODEL-FEATURE entity4 entity5
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ _C_ factored automata _/entity19_ , where _entity20_ _P_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity20 entity19
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ _C_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ _P_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity18 entity16
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ _C_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ _P_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity4 entity3
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ _C_ WordNet _/entity2_ , an on-line _entity3_ _P_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity3 entity2
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ statistical approaches _/entity6_ , which resolve _entity7_ _P_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ _C_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity7 entity9
The principle known as _entity1_ _P_ free indexation _/entity1_ plays an important role in the determination of the _entity2_ referential properties of noun phrases _/entity2_ in the _entity3_ principle-and-parameters language framework _/entity3_ . First , by investigating the combinatorics of _entity4_ _C_ free indexation _/entity4_ , we show that the problem of enumerating all possible _entity5_ indexings _/entity5_ requires _entity6_ exponential time _/entity6_ . Secondly , we exhibit a provably optimal _entity7_ free indexation algorithm _/entity7_ .	NONE entity1 entity4
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ _C_ language L _/entity16_ are directed by a _entity17_ _P_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity17 entity16
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ _P_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ _C_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity21 entity23
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ _P_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ _C_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity8 entity10
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ _P_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ _C_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity13 entity16
We describe a set of _entity1_ _C_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ _P_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity3 entity1
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ _P_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ _C_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity6 entity8
This paper ties up some loose ends in _entity1_ finite-state Optimality Theory _/entity1_ . First , it discusses how to perform _entity2_ comprehension _/entity2_ under _entity3_ Optimality Theory grammars _/entity3_ consisting of _entity4_ finite-state constraints _/entity4_ . _entity5_ Comprehension _/entity5_ has not been much studied in _entity6_ OT _/entity6_ ; we show that unlike _entity7_ production _/entity7_ , it does not always yield a regular set , making _entity8_ finite-state methods _/entity8_ inapplicable . However , after giving a suitably flexible presentation of _entity9_ OT _/entity9_ , we show carefully how to treat _entity10_ comprehension _/entity10_ under recent _entity11_ variants of OT _/entity11_ in which _entity12_ grammars _/entity12_ can be compiled into _entity13_ finite-state transducers _/entity13_ . We then unify these variants , showing that _entity14_ compilation _/entity14_ is possible if all components of the _entity15_ grammar _/entity15_ are _entity16_ _C_ regular relations _/entity16_ , including the _entity17_ harmony ordering _/entity17_ on _entity18_ _P_ scored candidates _/entity18_ .	NONE entity18 entity16
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ HPSG _/entity2_ into _entity3_ _P_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ _C_ HPSG _/entity5_ has a more elaborated _entity6_ principle-based theory _/entity6_ of possible _entity7_ phrase structures _/entity7_ , _entity8_ TAG _/entity8_ provides the means to represent _entity9_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity3 entity5
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ _C_ subset of logic _/entity12_ . The resulting _entity13_ _P_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity13 entity12
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ system combination _/entity3_ for _entity4_ _C_ unsupervised WSD _/entity4_ . We investigate several _entity5_ _P_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity5 entity4
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ _P_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ _C_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity12 entity14
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ _C_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ _P_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity9 entity6
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ sense-tagged training data _/entity4_ from _entity5_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ nouns _/entity6_ in the _entity7_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ _C_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ _P_ WSD programs _/entity14_ .	NONE entity14 entity12
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ _P_ word recognition experiments _/entity6_ were carried out with _entity7_ _C_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity6 entity7
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ _C_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ _P_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity6 entity4
This paper describes novel and practical _entity1_ _P_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ _C_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity1 entity3
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ _C_ punctuation _/entity22_ in transcribing _entity23_ _P_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity23 entity22
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ _C_ system _/entity13_ is evaluated using data from _entity14_ _P_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity14 entity13
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ _P_ procedures _/entity17_ used by the _entity18_ _C_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	USAGE entity17 entity18
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ _C_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ _P_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity7 entity5
This paper proposes a practical approach employing _entity1_ n-gram models _/entity1_ and _entity2_ error-correction rules _/entity2_ for _entity3_ Thai key prediction _/entity3_ and _entity4_ Thai-English language identification _/entity4_ . The paper also proposes _entity5_ rule-reduction algorithm _/entity5_ applying _entity6_ mutual information _/entity6_ to reduce the _entity7_ _P_ error-correction rules _/entity7_ . Our algorithm reported more than 99 % _entity8_ accuracy _/entity8_ in both _entity9_ _C_ language identification _/entity9_ and _entity10_ key prediction _/entity10_ .	NONE entity7 entity9
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ _C_ OT _/entity2_ . _entity3_ _P_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity3 entity2
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ _C_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ _P_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity9 entity7
A research program is described in which a particular _entity1_ _C_ representational format for meaning _/entity1_ is tested as broadly as possible . In this format , developed by the LNR research group at The University of California at San Diego , _entity2_ verbs _/entity2_ are represented as interconnected sets of _entity3_ _P_ subpredicates _/entity3_ . These _entity4_ subpredicates _/entity4_ may be thought of as the almost inevitable _entity5_ inferences _/entity5_ that a _entity6_ listener _/entity6_ makes when a _entity7_ verb _/entity7_ is used in a _entity8_ sentence _/entity8_ . They confer a _entity9_ meaning structure _/entity9_ on the _entity10_ sentence _/entity10_ in which the _entity11_ verb _/entity11_ is used .	NONE entity3 entity1
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ _P_ lexical choice _/entity2_ by certain _entity3_ _C_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity2 entity3
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ _P_ Bikel 's parser _/entity5_ achieves a higher _entity6_ _C_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	RESULT entity5 entity6
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ _C_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ _P_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity32 entity30
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ _C_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ _P_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity4 entity2
_entity1_ _C_ Systemic grammar _/entity1_ has been used for _entity2_ _P_ AI text generation _/entity2_ work in the past , but the _entity3_ implementations _/entity3_ have tended be ad hoc or inefficient . This paper presents an approach to systemic _entity4_ text generation _/entity4_ where _entity5_ AI problem solving techniques _/entity5_ are applied directly to an unadulterated _entity6_ systemic grammar _/entity6_ . This _entity7_ approach _/entity7_ is made possible by a special relationship between _entity8_ systemic grammar _/entity8_ and _entity9_ problem solving _/entity9_ : both are organized primarily as choosing from alternatives . The result is simple , efficient _entity10_ text generation _/entity10_ firmly based in a _entity11_ linguistic theory _/entity11_ .	NONE entity2 entity1
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ _P_ translation _/entity12_ of _entity13_ _C_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity12 entity13
`` To explain complex phenomena , an _entity1_ explanation system _/entity1_ must be able to select information from a formal representation of _entity2_ _C_ domain knowledge _/entity2_ , organize the selected information into _entity3_ multisentential discourse plans _/entity3_ , and realize the _entity4_ _P_ discourse plans _/entity4_ in text . Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for _entity5_ explanation _/entity5_ , empirical results have been limited . This paper reports on a seven-year effort to empirically study _entity6_ explanation generation _/entity6_ from _entity7_ semantically rich , large-scale knowledge bases _/entity7_ . In particular , it describes a _entity8_ robust explanation system _/entity8_ that constructs _entity9_ multisentential and multi-paragraph explanations _/entity9_ from the a _entity10_ large-scale knowledge base _/entity10_ in the domain of botanical anatomy , physiology , and development . We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system . In this evaluation , scored within `` '' half a grade '' '' of domain experts , and its performance exceeded that of one of the domain experts . ''	NONE entity4 entity2
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ _C_ poor syntactic construction _/entity12_ , _entity13_ _P_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity13 entity12
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ _C_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ _P_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity22 entity19
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ _C_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ _P_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity16 entity13
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ _C_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ _P_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity8 entity6
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ _C_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ _P_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity20 entity18
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ _P_ information _/entity2_ : 1. whether the _entity3_ _C_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity2 entity3
_entity1_ Pipelined Natural Language Generation ( NLG ) systems _/entity1_ have grown increasingly complex as _entity2_ architectural modules _/entity2_ were added to support _entity3_ language functionalities _/entity3_ such as _entity4_ referring expressions _/entity4_ , _entity5_ lexical choice _/entity5_ , and _entity6_ _P_ revision _/entity6_ . This has given rise to discussions about the relative placement of these new _entity7_ modules _/entity7_ in the overall _entity8_ _C_ architecture _/entity8_ . Recent work on another aspect of _entity9_ multi-paragraph text _/entity9_ , _entity10_ discourse markers _/entity10_ , indicates it is time to consider where a _entity11_ discourse marker insertion algorithm _/entity11_ fits in . We present examples which suggest that in a _entity12_ pipelined NLG architecture _/entity12_ , the best approach is to strongly tie it to a _entity13_ revision component _/entity13_ . Finally , we evaluate the approach in a working _entity14_ multi-page system _/entity14_ .	NONE entity6 entity8
We describe a method for identifying systematic _entity1_ patterns _/entity1_ in _entity2_ translation data _/entity2_ using _entity3_ part-of-speech tag sequences _/entity3_ . We incorporate this analysis into a _entity4_ _P_ diagnostic tool _/entity4_ intended for _entity5_ developers _/entity5_ of _entity6_ machine translation systems _/entity6_ , and demonstrate how our application can be used by _entity7_ _C_ developers _/entity7_ to explore _entity8_ patterns _/entity8_ in _entity9_ machine translation output _/entity9_ .	NONE entity4 entity7
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ _C_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ _P_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity12 entity10
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ _P_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ _C_ priming _/entity5_ of _entity6_ rules _/entity6_ between _entity7_ sentences _/entity7_ , within _entity8_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity2 entity5
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ phrase translations _/entity9_ in our _entity10_ _P_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ _C_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity10 entity12
_entity1_ Statistical language modeling _/entity1_ remains a challenging task , in particular for _entity2_ morphologically rich languages _/entity2_ . Recently , new approaches based on _entity3_ factored language models _/entity3_ have been developed to address this problem . These _entity4_ models _/entity4_ provide principled ways of including additional _entity5_ conditioning variables _/entity5_ other than the _entity6_ _C_ preceding words _/entity6_ , such as _entity7_ morphological or syntactic features _/entity7_ . However , the number of possible choices for _entity8_ _P_ model parameters _/entity8_ creates a _entity9_ large space of models _/entity9_ that can not be searched exhaustively . This paper presents an _entity10_ entirely data-driven model selection procedure _/entity10_ based on _entity11_ genetic search _/entity11_ , which is shown to outperform both _entity12_ knowledge-based and random selection procedures _/entity12_ on two different _entity13_ language modeling tasks _/entity13_ ( _entity14_ Arabic _/entity14_ and _entity15_ Turkish _/entity15_ ) .	NONE entity8 entity6
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ word-aligned corpora _/entity5_ is proposed . A _entity6_ statistical translation model _/entity6_ is also presented that deals such _entity7_ phrases _/entity7_ , as well as a _entity8_ _P_ training method _/entity8_ based on the maximization of _entity9_ _C_ translation accuracy _/entity9_ , as measured with the _entity10_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity8 entity9
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ _C_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ _P_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity34 entity31
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ _P_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ _C_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity18 entity21
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ _P_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ _C_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	NONE entity8 entity10
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ _C_ dependency tree kernels _/entity13_ on the 5 _entity14_ _P_ ACE relation major types _/entity14_ .	NONE entity14 entity13
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ _P_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ _C_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity12 entity14
The _entity1_ transfer phase _/entity1_ in _entity2_ machine translation ( MT ) systems _/entity2_ has been considered to be more complicated than _entity3_ _C_ analysis _/entity3_ and _entity4_ generation _/entity4_ , since it is inherently a conglomeration of individual _entity5_ lexical rules _/entity5_ . Currently some attempts are being made to use _entity6_ _P_ case-based reasoning _/entity6_ in _entity7_ machine translation _/entity7_ , that is , to make decisions on the basis of _entity8_ translation examples _/entity8_ at appropriate pints in _entity9_ MT _/entity9_ . This paper proposes a new type of _entity10_ transfer system _/entity10_ , called a _entity11_ Similarity-driven Transfer System ( SimTran ) _/entity11_ , for use in such _entity12_ case-based MT ( CBMT ) _/entity12_ .	NONE entity6 entity3
We discuss _entity1_ maximum a posteriori estimation _/entity1_ of _entity2_ continuous density hidden Markov models ( CDHMM ) _/entity2_ . The classical _entity3_ _C_ MLE reestimation algorithms _/entity3_ , namely the _entity4_ forward-backward algorithm _/entity4_ and the _entity5_ segmental k-means algorithm _/entity5_ , are expanded and _entity6_ _P_ reestimation formulas _/entity6_ are given for _entity7_ HMM with Gaussian mixture observation densities _/entity7_ . Because of its adaptive nature , _entity8_ Bayesian learning _/entity8_ serves as a unified approach for the following four _entity9_ speech recognition _/entity9_ applications , namely _entity10_ parameter smoothing _/entity10_ , _entity11_ speaker adaptation _/entity11_ , _entity12_ speaker group modeling _/entity12_ and _entity13_ corrective training _/entity13_ . New experimental results on all four applications are provided to show the effectiveness of the _entity14_ MAP estimation approach _/entity14_ .	NONE entity6 entity3
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ _C_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ _P_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity15 entity12
The _entity1_ transfer phase _/entity1_ in _entity2_ machine translation ( MT ) systems _/entity2_ has been considered to be more complicated than _entity3_ analysis _/entity3_ and _entity4_ generation _/entity4_ , since it is inherently a conglomeration of individual _entity5_ lexical rules _/entity5_ . Currently some attempts are being made to use _entity6_ case-based reasoning _/entity6_ in _entity7_ machine translation _/entity7_ , that is , to make decisions on the basis of _entity8_ translation examples _/entity8_ at appropriate pints in _entity9_ _C_ MT _/entity9_ . This paper proposes a new type of _entity10_ _P_ transfer system _/entity10_ , called a _entity11_ Similarity-driven Transfer System ( SimTran ) _/entity11_ , for use in such _entity12_ case-based MT ( CBMT ) _/entity12_ .	NONE entity10 entity9
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ multiple speakers _/entity5_ without _entity6_ explicit speaker enrollment _/entity6_ . Combining _entity7_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ _C_ segment-based approach _/entity12_ to _entity13_ _P_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity13 entity12
We propose a _entity1_ detection method _/entity1_ for orthographic variants caused by _entity2_ transliteration _/entity2_ in a large _entity3_ corpus _/entity3_ . The method employs two _entity4_ similarities _/entity4_ . One is _entity5_ string similarity _/entity5_ based on _entity6_ _C_ edit distance _/entity6_ . The other is _entity7_ contextual similarity _/entity7_ by a _entity8_ _P_ vector space model _/entity8_ . Experimental results show that the method performed a 0.889 _entity9_ F-measure _/entity9_ in an open test .	NONE entity8 entity6
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ _P_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ _C_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	RESULT entity17 entity18
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ _C_ approach _/entity21_ with the combination of _entity22_ _P_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity22 entity21
In this paper , we present a _entity1_ fully automated extraction system _/entity1_ , named _entity2_ _C_ IntEx _/entity2_ , to identify _entity3_ gene and protein interactions _/entity3_ in _entity4_ _P_ biomedical text _/entity4_ . Our approach is based on first splitting _entity5_ complex sentences _/entity5_ into _entity6_ simple clausal structures _/entity6_ made up of _entity7_ syntactic roles _/entity7_ . Then , tagging _entity8_ biological entities _/entity8_ with the help of _entity9_ biomedical and linguistic ontologies _/entity9_ . Finally , extracting _entity10_ complete interactions _/entity10_ by analyzing the matching contents of _entity11_ syntactic roles _/entity11_ and their linguistically significant combinations . Our _entity12_ extraction system _/entity12_ handles _entity13_ complex sentences _/entity13_ and extracts _entity14_ multiple and nested interactions _/entity14_ specified in a _entity15_ sentence _/entity15_ . Experimental evaluations with two other state of the art _entity16_ extraction systems _/entity16_ indicate that the _entity17_ IntEx system _/entity17_ achieves better _entity18_ performance _/entity18_ without the labor intensive _entity19_ pattern engineering requirement _/entity19_ .	NONE entity4 entity2
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ LFG _/entity4_ and _entity5_ UDRS _/entity5_ formalisms . It provides a _entity6_ model theoretic interpretation _/entity6_ and an _entity7_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ f-structures _/entity9_ through the _entity10_ _P_ translation images _/entity10_ of _entity11_ f-structures _/entity11_ as _entity12_ _C_ UDRSs _/entity12_ .	NONE entity10 entity12
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ _C_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ _P_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity13 entity11
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ _C_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ _P_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity7 entity5
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ _C_ phrase chunking _/entity5_ information is very effective for _entity6_ _P_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity6 entity5
Sources of _entity1_ _C_ training data _/entity1_ suitable for _entity2_ language modeling _/entity2_ of _entity3_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ _P_ training data _/entity4_ can be supplemented with _entity5_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	NONE entity4 entity1
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ _C_ verb semantics _/entity8_ and _entity9_ _P_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity9 entity8
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ _P_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ _C_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity5 entity8
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ _C_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ _P_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity13 entity11
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ _C_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ _P_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity24 entity22
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ _P_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ _C_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity5 entity8
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ _P_ traditional MT architectural designs _/entity4_ . A new _entity5_ semantic representation _/entity5_ is proposed that uses _entity6_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ _C_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ interlingua _/entity9_ within a new _entity10_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ direct approaches _/entity12_ into a single system .	NONE entity4 entity7
_entity1_ Taiwan Child Language Corpus _/entity1_ contains _entity2_ _P_ scripts _/entity2_ transcribed from about 330 hours of _entity3_ recordings _/entity3_ of fourteen young children from _entity4_ _C_ Southern Min Chinese _/entity4_ speaking families in Taiwan . The format of the _entity5_ corpus _/entity5_ adopts the _entity6_ Child Language Data Exchange System ( CHILDES ) _/entity6_ . The size of the _entity7_ corpus _/entity7_ is about 1.6 million _entity8_ words _/entity8_ . In this paper , we describe _entity9_ data collection _/entity9_ , _entity10_ transcription _/entity10_ , _entity11_ word segmentation _/entity11_ , and _entity12_ part-of-speech annotation _/entity12_ of this _entity13_ corpus _/entity13_ . Applications of the _entity14_ corpus _/entity14_ are also discussed .	NONE entity2 entity4
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ HPSG _/entity2_ into _entity3_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ HPSG _/entity5_ has a more elaborated _entity6_ _C_ principle-based theory _/entity6_ of possible _entity7_ phrase structures _/entity7_ , _entity8_ _P_ TAG _/entity8_ provides the means to represent _entity9_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity8 entity6
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ _C_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ _P_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity3 entity2
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ delimiters _/entity3_ , which poses a challenge to _entity4_ _P_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ MT _/entity5_ , the widely used approach is to apply a _entity6_ _C_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity4 entity6
In this paper a novel solution to automatic and _entity1_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ _P_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ _C_ WSI algorithm _/entity14_ .	NONE entity11 entity14
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ _C_ Training _/entity11_ and _entity12_ _P_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity12 entity11
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ _P_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ _C_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity13 entity16
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ _P_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ _C_ CTL translation _/entity15_ .	NONE entity12 entity15
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ _C_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ _P_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity17 entity14
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ _P_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ _C_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity7 entity10
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ _C_ text understanding _/entity5_ . Systems built with _entity6_ _P_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity6 entity5
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ _P_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ _C_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity3 entity5
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ _C_ TAGs _/entity14_ to be used beyond their role in _entity15_ _P_ syntax proper _/entity15_ . We discuss the application of _entity16_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity15 entity14
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ _C_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ _P_ language _/entity33_ of interest .	NONE entity33 entity30
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ _P_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ _C_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity20 entity23
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ pronoun resolution _/entity2_ in _entity3_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ features _/entity6_ designed for _entity7_ _P_ pronoun resolution _/entity7_ in _entity8_ _C_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	USAGE entity7 entity8
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ ungrammatical input _/entity8_ . While _entity9_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ passives _/entity11_ , _entity12_ relative clauses _/entity12_ and _entity13_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ _P_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ Plume _/entity16_ to handle _entity17_ _C_ passives _/entity17_ , _entity18_ relative clauses _/entity18_ , and _entity19_ interrogatives _/entity19_ in a general manner .	NONE entity15 entity17
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ _C_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ _P_ CF grammars _/entity16_ .	NONE entity16 entity14
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ _C_ source-language _/entity5_ _entity6_ _P_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity6 entity5
In this paper a system which understands and conceptualizes _entity1_ scenes descriptions in natural language _/entity1_ is presented . Specifically , the following components of the system are described : the _entity2_ syntactic analyzer _/entity2_ , based on a _entity3_ _C_ Procedural Systemic Grammar _/entity3_ , the _entity4_ semantic analyzer _/entity4_ relying on the _entity5_ Conceptual Dependency Theory _/entity5_ , and the _entity6_ _P_ dictionary _/entity6_ .	NONE entity6 entity3
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ _P_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ _C_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity4 entity7
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ _C_ truth-conditions _/entity7_ when the _entity8_ _P_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity8 entity7
We approximate _entity1_ _C_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ _P_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity3 entity1
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ _P_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ _C_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity12 entity15
In this paper , we present an _entity1_ unlexicalized parser _/entity1_ for _entity2_ German _/entity2_ which employs _entity3_ smoothing _/entity3_ and _entity4_ suffix analysis _/entity4_ to achieve a _entity5_ labelled bracket F-score _/entity5_ of 76.2 , higher than previously reported results on the _entity6_ NEGRA corpus _/entity6_ . In addition to the high _entity7_ accuracy _/entity7_ of the model , the use of _entity8_ _P_ smoothing _/entity8_ in an _entity9_ unlexicalized parser _/entity9_ allows us to better examine the interplay between _entity10_ smoothing _/entity10_ and _entity11_ _C_ parsing _/entity11_ results .	NONE entity8 entity11
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ _P_ verbs _/entity17_ with highly varied _entity18_ _C_ argument structures _/entity18_ .	MODEL-FEATURE entity17 entity18
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ _P_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ _C_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity5 entity7
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ _P_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ _C_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity13 entity16
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ _P_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ _C_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity3 entity5
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ _P_ parsing _/entity4_ . We compare two wide-coverage _entity5_ _C_ lexicalized grammars of English _/entity5_ , _entity6_ LEXSYS _/entity6_ and _entity7_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity4 entity5
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ _C_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ _P_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity8 entity6
In this paper , we present a _entity1_ fully automated extraction system _/entity1_ , named _entity2_ IntEx _/entity2_ , to identify _entity3_ gene and protein interactions _/entity3_ in _entity4_ _C_ biomedical text _/entity4_ . Our approach is based on first splitting _entity5_ _P_ complex sentences _/entity5_ into _entity6_ simple clausal structures _/entity6_ made up of _entity7_ syntactic roles _/entity7_ . Then , tagging _entity8_ biological entities _/entity8_ with the help of _entity9_ biomedical and linguistic ontologies _/entity9_ . Finally , extracting _entity10_ complete interactions _/entity10_ by analyzing the matching contents of _entity11_ syntactic roles _/entity11_ and their linguistically significant combinations . Our _entity12_ extraction system _/entity12_ handles _entity13_ complex sentences _/entity13_ and extracts _entity14_ multiple and nested interactions _/entity14_ specified in a _entity15_ sentence _/entity15_ . Experimental evaluations with two other state of the art _entity16_ extraction systems _/entity16_ indicate that the _entity17_ IntEx system _/entity17_ achieves better _entity18_ performance _/entity18_ without the labor intensive _entity19_ pattern engineering requirement _/entity19_ .	NONE entity5 entity4
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ _P_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ _C_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity8 entity11
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ _C_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ _P_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity10 entity7
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ _P_ ungrammatical segmentations _/entity11_ with a _entity12_ _C_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity11 entity12
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ rules _/entity6_ between _entity7_ sentences _/entity7_ , within _entity8_ _C_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ _P_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity11 entity8
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ _P_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ _C_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity4 entity6
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ _C_ other ( than ) _/entity4_ , _entity5_ _P_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity5 entity4
This paper presents a _entity1_ machine learning approach _/entity1_ to _entity2_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ corpus-based sample _/entity5_ and formulate them as _entity6_ _P_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ _C_ clauses _/entity7_ to create a set of _entity8_ domain independent features _/entity8_ to annotate an _entity9_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity6 entity7
In this paper a system which understands and conceptualizes _entity1_ scenes descriptions in natural language _/entity1_ is presented . Specifically , the following components of the system are described : the _entity2_ syntactic analyzer _/entity2_ , based on a _entity3_ Procedural Systemic Grammar _/entity3_ , the _entity4_ _C_ semantic analyzer _/entity4_ relying on the _entity5_ _P_ Conceptual Dependency Theory _/entity5_ , and the _entity6_ dictionary _/entity6_ .	USAGE entity5 entity4
This paper describes a system ( _entity1_ RAREAS _/entity1_ ) which synthesizes marine weather forecasts directly from _entity2_ formatted weather data _/entity2_ . Such _entity3_ synthesis _/entity3_ appears feasible in certain _entity4_ natural sublanguages _/entity4_ with _entity5_ stereotyped text structure _/entity5_ . _entity6_ RAREAS _/entity6_ draws on several kinds of _entity7_ _C_ linguistic and non-linguistic knowledge _/entity7_ and mirrors a forecaster 's apparent tendency to ascribe less precise _entity8_ _P_ temporal adverbs _/entity8_ to more remote meteorological events . The approach can easily be adapted to synthesize _entity9_ bilingual or multi-lingual texts _/entity9_ .	NONE entity8 entity7
Sources of _entity1_ training data _/entity1_ suitable for _entity2_ language modeling _/entity2_ of _entity3_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ _P_ text _/entity5_ from the _entity6_ _C_ web _/entity6_ filtered to match the _entity7_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	PART_WHOLE entity5 entity6
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ _P_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ _C_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity2 entity4
In this paper we describe and evaluate a _entity1_ Question Answering system _/entity1_ that goes beyond answering factoid questions . We focus on _entity2_ FAQ-like questions and answers _/entity2_ , and build our system around a _entity3_ _P_ noisy-channel architecture _/entity3_ which exploits both a _entity4_ language model _/entity4_ for _entity5_ _C_ answers _/entity5_ and a _entity6_ transformation model _/entity6_ for _entity7_ answer/question terms _/entity7_ , trained on a _entity8_ corpus _/entity8_ of 1 million _entity9_ question/answer pairs _/entity9_ collected from the Web .	NONE entity3 entity5
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ _C_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ _P_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity7 entity4
In the _entity1_ Chinese language _/entity1_ , a _entity2_ _C_ verb _/entity2_ may have its _entity3_ _P_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity3 entity2
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ sentence plans _/entity12_ for a given _entity13_ _P_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ _C_ plan _/entity16_ . The _entity17_ SPR _/entity17_ uses _entity18_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ top human-ranked sentence plan _/entity22_ .	NONE entity13 entity16
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ _C_ structure _/entity19_ of the _entity20_ _P_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity20 entity19
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ _P_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ _C_ ccg supertagging _/entity23_ .	NONE entity20 entity23
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ _P_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ _C_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity8 entity9
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ _C_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ _P_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity13 entity10
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ _C_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ _P_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity3 entity2
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ _C_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ _P_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity24 entity21
The _entity1_ interlingual approach to MT _/entity1_ has been repeatedly advocated by researchers originally interested in _entity2_ natural language understanding _/entity2_ who take _entity3_ machine translation _/entity3_ to be one possible application . However , not only the _entity4_ _P_ ambiguity _/entity4_ but also the vagueness which every _entity5_ _C_ natural language _/entity5_ inevitably has leads this approach into essential difficulties . In contrast , our project , the _entity6_ Mu-project _/entity6_ , adopts the _entity7_ transfer approach _/entity7_ as the basic framework of _entity8_ MT _/entity8_ . This paper describes the detailed construction of the _entity9_ transfer phase _/entity9_ of our system from _entity10_ Japanese _/entity10_ to _entity11_ English _/entity11_ , and gives some examples of problems which seem difficult to treat in the _entity12_ interlingual approach _/entity12_ . The basic design principles of the _entity13_ transfer phase _/entity13_ of our system have already been mentioned in ( 1 ) ( 2 ) . Some of the principles which are relevant to the topic of this paper are : ( a ) _entity14_ Multiple Layer of Grammars _/entity14_ ( b ) _entity15_ Multiple Layer Presentation _/entity15_ ( c ) _entity16_ Lexicon Driven Processing _/entity16_ ( d ) _entity17_ Form-Oriented Dictionary Description _/entity17_ . This paper also shows how these principles are realized in the current system .	MODEL-FEATURE entity4 entity5
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ _P_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ _C_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity11 entity13
_entity1_ Systemic grammar _/entity1_ has been used for _entity2_ AI text generation _/entity2_ work in the past , but the _entity3_ implementations _/entity3_ have tended be ad hoc or inefficient . This paper presents an approach to systemic _entity4_ text generation _/entity4_ where _entity5_ AI problem solving techniques _/entity5_ are applied directly to an unadulterated _entity6_ systemic grammar _/entity6_ . This _entity7_ approach _/entity7_ is made possible by a special relationship between _entity8_ systemic grammar _/entity8_ and _entity9_ problem solving _/entity9_ : both are organized primarily as choosing from alternatives . The result is simple , efficient _entity10_ _C_ text generation _/entity10_ firmly based in a _entity11_ _P_ linguistic theory _/entity11_ .	USAGE entity11 entity10
_entity1_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ language-independent system _/entity2_ for _entity3_ automatic discovery of text _/entity3_ in _entity4_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ _C_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ _P_ words _/entity9_ per _entity10_ language _/entity10_ .	NONE entity9 entity8
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ _P_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ _C_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity16 entity18
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ _P_ decoding _/entity8_ , we use a _entity9_ _C_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity8 entity9
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ _C_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ _P_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity4 entity2
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ _P_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ _C_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity9 entity12
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ _P_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ _C_ rules _/entity6_ between _entity7_ sentences _/entity7_ , within _entity8_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity3 entity6
This paper presents the results of automatically inducing a _entity1_ Combinatory Categorial Grammar ( CCG ) lexicon _/entity1_ from a _entity2_ Turkish dependency treebank _/entity2_ . The fact that _entity3_ Turkish _/entity3_ is an _entity4_ agglutinating free word order language _/entity4_ presents a challenge for _entity5_ language theories _/entity5_ . We explored possible ways to obtain a _entity6_ compact lexicon _/entity6_ , consistent with _entity7_ _C_ CCG principles _/entity7_ , from a _entity8_ _P_ treebank _/entity8_ which is an order of magnitude smaller than _entity9_ Penn WSJ _/entity9_ .	NONE entity8 entity7
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ _C_ scf types _/entity8_ which tests for the presence of _entity9_ _P_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity9 entity8
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ _C_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ _P_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity13 entity10
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ _C_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ _P_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity8 entity6
In the second year of _entity1_ evaluations _/entity1_ of the _entity2_ ARPA HLT Machine Translation ( MT ) Initiative _/entity2_ , methodologies developed and tested in 1992 were applied to the _entity3_ 1993 MT test runs _/entity3_ . The current methodology optimizes the inherently _entity4_ subjective judgments _/entity4_ on _entity5_ translation accuracy and quality _/entity5_ by channeling the _entity6_ judgments _/entity6_ of _entity7_ non-translators _/entity7_ into many _entity8_ data points _/entity8_ which reflect both the comparison of the _entity9_ performance _/entity9_ of the _entity10_ research MT systems _/entity10_ with _entity11_ production MT systems _/entity11_ and against the _entity12_ performance _/entity12_ of _entity13_ novice translators _/entity13_ . This paper discusses the three _entity14_ evaluation methods _/entity14_ used in the _entity15_ _C_ 1993 evaluation _/entity15_ , the results of the evaluations , and preliminary characterizations of the _entity16_ _P_ Winter 1994 evaluation _/entity16_ , now underway . The efforts under discussion focus on measuring the progress of _entity17_ core MT technology _/entity17_ and increasing the sensitivity and _entity18_ portability _/entity18_ of _entity19_ MT evaluation methodology _/entity19_ .	NONE entity16 entity15
In this paper , we present a novel _entity1_ training method _/entity1_ for a _entity2_ localized phrase-based prediction model _/entity2_ for _entity3_ statistical machine translation ( SMT ) _/entity3_ . The _entity4_ model _/entity4_ predicts _entity5_ blocks _/entity5_ with orientation to handle _entity6_ _C_ local phrase re-ordering _/entity6_ . We use a _entity7_ _P_ maximum likelihood criterion _/entity7_ to train a _entity8_ log-linear block bigram model _/entity8_ which uses _entity9_ real-valued features _/entity9_ ( e.g . a _entity10_ language model score _/entity10_ ) as well as _entity11_ binary features _/entity11_ based on the _entity12_ block _/entity12_ identities themselves , e.g . block bigram features . Our _entity13_ training algorithm _/entity13_ can easily handle millions of _entity14_ features _/entity14_ . The best system obtains a 18.6 % improvement over the _entity15_ baseline _/entity15_ on a standard _entity16_ Arabic-English translation task _/entity16_ .	NONE entity7 entity6
This paper discusses a _entity1_ decision-tree approach _/entity1_ to the problem of assigning _entity2_ probabilities _/entity2_ to _entity3_ words _/entity3_ following a given _entity4_ text _/entity4_ . In contrast with previous _entity5_ _P_ decision-tree language model attempts _/entity5_ , an algorithm for selecting _entity6_ nearly optimal questions _/entity6_ is considered . The model is to be tested on a standard task , _entity7_ The Wall Street Journal _/entity7_ , allowing a fair comparison with the well-known _entity8_ _C_ tri-gram model _/entity8_ .	NONE entity5 entity8
This paper addresses the problem of identifying likely _entity1_ topics _/entity1_ of _entity2_ texts _/entity2_ by their position in the _entity3_ text _/entity3_ . It describes the automated _entity4_ training _/entity4_ and evaluation of an _entity5_ Optimal Position Policy _/entity5_ , a method of locating the likely positions of _entity6_ topic-bearing sentences _/entity6_ based on _entity7_ genre-specific regularities _/entity7_ of _entity8_ discourse structure _/entity8_ . This method can be used in applications such as _entity9_ _P_ information retrieval _/entity9_ , _entity10_ routing _/entity10_ , and _entity11_ _C_ text summarization _/entity11_ .	NONE entity9 entity11
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ _C_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ _P_ speech understanding _/entity14_ .	NONE entity14 entity11
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ parsing _/entity4_ . We compare two wide-coverage _entity5_ lexicalized grammars of English _/entity5_ , _entity6_ _P_ LEXSYS _/entity6_ and _entity7_ _C_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	COMPARE entity6 entity7
We propose a method of organizing reading materials for _entity1_ vocabulary learning _/entity1_ . It enables us to select a concise set of reading _entity2_ texts _/entity2_ ( from a _entity3_ target corpus _/entity3_ ) that contains all the _entity4_ target vocabulary _/entity4_ to be learned . We used a specialized _entity5_ vocabulary _/entity5_ for an English certification test as the _entity6_ _C_ target vocabulary _/entity6_ and used _entity7_ English Wikipedia _/entity7_ , a free-content encyclopedia , as the _entity8_ target corpus _/entity8_ . The organized reading materials would enable learners not only to study the _entity9_ _P_ target vocabulary _/entity9_ efficiently but also to gain a variety of knowledge through reading . The reading materials are available on our web site .	NONE entity9 entity6
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ _P_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ _C_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity29 entity31
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to disambiguate various relations between _entity2_ named entities _/entity2_ by use of various _entity3_ lexical and syntactic features _/entity3_ from the _entity4_ contexts _/entity4_ . It works by calculating _entity5_ eigenvectors _/entity5_ of an _entity6_ adjacency graph _/entity6_ 's _entity7_ Laplacian _/entity7_ to recover a _entity8_ _C_ submanifold _/entity8_ of data from a _entity9_ _P_ high dimensionality space _/entity9_ and then performing _entity10_ cluster number estimation _/entity10_ on the _entity11_ eigenvectors _/entity11_ . Experiment results on _entity12_ ACE corpora _/entity12_ show that this _entity13_ spectral clustering based approach _/entity13_ outperforms the other _entity14_ clustering methods _/entity14_ .	NONE entity9 entity8
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ _C_ SVM _/entity4_ . Our study illustrates that the base _entity5_ _P_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity5 entity4
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ temporal expressions _/entity2_ in _entity3_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ temporal expressions _/entity4_ in a novel _entity5_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ expressions _/entity6_ fits well with our _entity7_ _C_ constraint-based representation _/entity7_ of time , _entity8_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ _P_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ baseline _/entity10_ , and compares favorably with some of the closely related work .	NONE entity9 entity7
Informally , a _entity1_ _C_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ _P_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity3 entity1
A _entity1_ _P_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ _C_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity1 entity3
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ _C_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ _P_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity9 entity6
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ _P_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ _C_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity17 entity19
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ _P_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ _C_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity17 entity19
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ _C_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ _P_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity22 entity20
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ _P_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ _C_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity2 entity4
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ _C_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ _P_ complexity _/entity15_ .	NONE entity15 entity12
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ _C_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ _P_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity4 entity2
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ _P_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ _C_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity3 entity6
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ _C_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ _P_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity9 entity8
This paper describes a recently collected _entity1_ spoken language corpus _/entity1_ for the _entity2_ ATIS ( Air Travel Information System ) domain _/entity2_ . This data collection effort has been co-ordinated by _entity3_ _P_ MADCOW ( Multi-site ATIS Data COllection Working group ) _/entity3_ . We summarize the motivation for this effort , the goals , the implementation of a _entity4_ _C_ multi-site data collection paradigm _/entity4_ , and the accomplishments of _entity5_ MADCOW _/entity5_ in monitoring the _entity6_ collection _/entity6_ and distribution of 12,000 _entity7_ utterances _/entity7_ of _entity8_ spontaneous speech _/entity8_ from five sites for use in a _entity9_ multi-site common evaluation of speech , natural language and spoken language _/entity9_ .	NONE entity3 entity4
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ _C_ superordinate substitution _/entity6_ , and definite _entity7_ _P_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity7 entity6
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ _C_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ _P_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity15 entity14
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ _P_ passive _/entity17_ and the _entity18_ _C_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity17 entity18
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ _P_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ _C_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity11 entity14
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ _P_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ _C_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity2 entity4
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ _P_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ _C_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity10 entity13
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ performance _/entity5_ of using _entity6_ ASR output _/entity6_ as opposed to _entity7_ human transcription _/entity7_ . Examination of the effect of _entity8_ features _/entity8_ shows that _entity9_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ _C_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ _P_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity12 entity10
This paper proposes _entity1_ document oriented preference sets ( DoPS ) _/entity1_ for the disambiguation of the _entity2_ _P_ dependency structure _/entity2_ of _entity3_ _C_ sentences _/entity3_ . The _entity4_ DoPS system _/entity4_ extracts preference knowledge from a _entity5_ target document _/entity5_ or other _entity6_ documents _/entity6_ automatically . _entity7_ Sentence ambiguities _/entity7_ can be resolved by using domain targeted preference knowledge without using complicated large _entity8_ knowledgebases _/entity8_ . _entity9_ Implementation _/entity9_ and _entity10_ empirical results _/entity10_ are described for the the analysis of _entity11_ dependency structures _/entity11_ of _entity12_ Japanese patent claim sentences _/entity12_ .	MODEL-FEATURE entity2 entity3
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ _C_ LCS meaning components _/entity6_ . Our _entity7_ _P_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity7 entity6
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ sublanguage _/entity3_ , is proposed for identifying _entity4_ unknown words _/entity4_ , especially _entity5_ personal names _/entity5_ , in _entity6_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ _P_ adaptive dynamic word formation _/entity8_ , _entity9_ _C_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ WI systems _/entity12_ without the _entity13_ name identification _/entity13_ capability .	NONE entity8 entity9
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ _P_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ _C_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity7 entity9
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ _P_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ _C_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity4 entity6
_entity1_ Chart parsing _/entity1_ is _entity2_ _C_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ chart _/entity3_ that works outward from _entity4_ islands _/entity4_ and makes sense of as much of the _entity5_ _P_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ fragments _/entity7_ occur in the _entity8_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ islands _/entity9_ , until possibly completely missing _entity10_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity5 entity2
This paper describes a new , _entity1_ _C_ large scale discourse-level annotation _/entity1_ project - the _entity2_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ discourse structure _/entity3_ that is based on identifying _entity4_ _P_ discourse connectives _/entity4_ and their _entity5_ arguments _/entity5_ . The _entity6_ PDTB _/entity6_ is being built directly on top of the _entity7_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ inter-annotator agreement _/entity11_ - both the _entity12_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity4 entity1
This paper explores the issue of using different _entity1_ co-occurrence similarities _/entity1_ between _entity2_ terms _/entity2_ for separating _entity3_ query terms _/entity3_ that are useful for _entity4_ retrieval _/entity4_ from those that are harmful . The hypothesis under examination is that _entity5_ useful terms _/entity5_ tend to be more similar to each other than to other _entity6_ query terms _/entity6_ . Preliminary experiments with similarities computed using _entity7_ first-order and second-order co-occurrence _/entity7_ seem to confirm the hypothesis . _entity8_ Term similarities _/entity8_ could then be used for determining which _entity9_ query terms _/entity9_ are useful and best reflect the user 's information need . A possible application would be to use this source of evidence for tuning the _entity10_ _P_ weights _/entity10_ of the _entity11_ _C_ query terms _/entity11_ .	MODEL-FEATURE entity10 entity11
A novel _entity1_ bootstrapping approach _/entity1_ to _entity2_ Named Entity ( NE ) tagging _/entity2_ using _entity3_ concept-based seeds _/entity3_ and _entity4_ _P_ successive learners _/entity4_ is presented . This approach only requires a few _entity5_ common noun _/entity5_ or _entity6_ pronoun _/entity6_ _entity7_ _C_ seeds _/entity7_ that correspond to the _entity8_ concept _/entity8_ for the targeted _entity9_ NE _/entity9_ , e.g . he/she/man/woman for _entity10_ PERSON NE _/entity10_ . The _entity11_ bootstrapping procedure _/entity11_ is implemented as training two _entity12_ successive learners _/entity12_ . First , _entity13_ decision list _/entity13_ is used to learn the _entity14_ parsing-based NE rules _/entity14_ . Then , a _entity15_ Hidden Markov Model _/entity15_ is trained on a _entity16_ corpus _/entity16_ automatically tagged by the first _entity17_ learner _/entity17_ . The resulting _entity18_ NE system _/entity18_ approaches _entity19_ supervised NE _/entity19_ performance for some _entity20_ NE types _/entity20_ .	NONE entity4 entity7
Past work of generating _entity1_ referring expressions _/entity1_ mainly utilized attributes of _entity2_ objects _/entity2_ and _entity3_ binary relations _/entity3_ between _entity4_ objects _/entity4_ . However , such an approach does not work well when there is no distinctive attribute among _entity5_ objects _/entity5_ . To overcome this limitation , this paper proposes a method utilizing the perceptual groups of _entity6_ objects _/entity6_ and _entity7_ _C_ n-ary relations _/entity7_ among them . The key is to identify groups of _entity8_ objects _/entity8_ that are naturally recognized by humans . We conducted psychological experiments with 42 subjects to collect _entity9_ referring expressions _/entity9_ in such situations , and built a _entity10_ _P_ generation algorithm _/entity10_ based on the results . The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions .	NONE entity10 entity7
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ _C_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ _P_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity15 entity14
We introduce a new _entity1_ _C_ interactive corpus exploration tool _/entity1_ called _entity2_ InfoMagnets _/entity2_ . _entity3_ InfoMagnets _/entity3_ aims at making _entity4_ _P_ exploratory corpus analysis _/entity4_ accessible to researchers who are not experts in _entity5_ text mining _/entity5_ . As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between _entity6_ language _/entity6_ and _entity7_ behavioral patterns _/entity7_ in two distinct domains : _entity8_ tutorial dialogue _/entity8_ ( Kumar et al. , submitted ) and _entity9_ on-line communities _/entity9_ ( Arguello et al. , 2006 ) . As an _entity10_ educational tool _/entity10_ , it has been used as part of a unit on _entity11_ protocol analysis _/entity11_ in an _entity12_ Educational Research Methods course _/entity12_ .	NONE entity4 entity1
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ _C_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ _P_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity7 entity4
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ _P_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ _C_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity14 entity17
_entity1_ Large-scale natural language generation _/entity1_ requires the integration of vast amounts of _entity2_ knowledge _/entity2_ : lexical , grammatical , and conceptual . A _entity3_ robust generator _/entity3_ must be able to operate well even when pieces of _entity4_ knowledge _/entity4_ are missing . It must also be robust against _entity5_ incomplete or inaccurate inputs _/entity5_ . To attack these problems , we have built a _entity6_ hybrid generator _/entity6_ , in which gaps in _entity7_ _C_ symbolic knowledge _/entity7_ are filled by _entity8_ _P_ statistical methods _/entity8_ . We describe algorithms and show experimental results . We also discuss how the _entity9_ hybrid generation model _/entity9_ can be used to simplify current _entity10_ generators _/entity10_ and enhance their _entity11_ portability _/entity11_ , even when perfect _entity12_ knowledge _/entity12_ is in principle obtainable .	NONE entity8 entity7
The purpose of this research is to test the efficacy of applying _entity1_ _C_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ _P_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity3 entity1
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ _C_ reference word string _/entity8_ and selects the _entity9_ _P_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity9 entity8
For _entity1_ intelligent interactive systems _/entity1_ to communicate with _entity2_ humans _/entity2_ in a natural manner , they must have knowledge about the _entity3_ system users _/entity3_ . This paper explores the role of _entity4_ user modeling _/entity4_ in such _entity5_ systems _/entity5_ . It begins with a characterization of what a _entity6_ user model _/entity6_ is and how it can be used . The types of information that a _entity7_ user model _/entity7_ may be required to keep about a _entity8_ user _/entity8_ are then identified and discussed . _entity9_ User models _/entity9_ themselves can vary greatly depending on the requirements of the situation and the implementation , so several dimensions along which they can be classified are presented . Since acquiring the knowledge for a _entity10_ user model _/entity10_ is a fundamental problem in _entity11_ user modeling _/entity11_ , a section is devoted to this topic . Next , the benefits and costs of implementing a _entity12_ user modeling component _/entity12_ for a system are weighed in light of several aspects of the _entity13_ interaction requirements _/entity13_ that may be imposed by the system . Finally , the current state of research in _entity14_ _P_ user modeling _/entity14_ is summarized , and future research topics that must be addressed in order to achieve powerful , general _entity15_ _C_ user modeling systems _/entity15_ are assessed .	NONE entity14 entity15
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ _P_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ _C_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity4 entity6
We present an operable definition of _entity1_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ discourse model _/entity3_ and _entity4_ _P_ knowledge store _/entity4_ is introduced enabling the _entity5_ decomposition _/entity5_ and _entity6_ _C_ formal representation _/entity6_ of its _entity7_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity4 entity6
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ _C_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ _P_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity12 entity10
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ _C_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ _P_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity15 entity13
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ _C_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ _P_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity6 entity3
This paper presents an _entity1_ _C_ unsupervised learning approach _/entity1_ to disambiguate various relations between _entity2_ _P_ named entities _/entity2_ by use of various _entity3_ lexical and syntactic features _/entity3_ from the _entity4_ contexts _/entity4_ . It works by calculating _entity5_ eigenvectors _/entity5_ of an _entity6_ adjacency graph _/entity6_ 's _entity7_ Laplacian _/entity7_ to recover a _entity8_ submanifold _/entity8_ of data from a _entity9_ high dimensionality space _/entity9_ and then performing _entity10_ cluster number estimation _/entity10_ on the _entity11_ eigenvectors _/entity11_ . Experiment results on _entity12_ ACE corpora _/entity12_ show that this _entity13_ spectral clustering based approach _/entity13_ outperforms the other _entity14_ clustering methods _/entity14_ .	NONE entity2 entity1
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ _P_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ _C_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity8 entity10
This paper proposes a practical approach employing _entity1_ n-gram models _/entity1_ and _entity2_ error-correction rules _/entity2_ for _entity3_ _C_ Thai key prediction _/entity3_ and _entity4_ Thai-English language identification _/entity4_ . The paper also proposes _entity5_ _P_ rule-reduction algorithm _/entity5_ applying _entity6_ mutual information _/entity6_ to reduce the _entity7_ error-correction rules _/entity7_ . Our algorithm reported more than 99 % _entity8_ accuracy _/entity8_ in both _entity9_ language identification _/entity9_ and _entity10_ key prediction _/entity10_ .	NONE entity5 entity3
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ _P_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ _C_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity4 entity6
This paper considers the problem of automatic assessment of _entity1_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ _C_ induced model _/entity10_ achieves significantly higher _entity11_ _P_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity11 entity10
The purpose of this research is to test the efficacy of applying _entity1_ _P_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ _C_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity1 entity2
In the past the evaluation of _entity1_ _P_ machine translation systems _/entity1_ has focused on single system evaluations because there were only few systems available . But now there are several commercial systems for the same _entity2_ language pair _/entity2_ . This requires new methods of comparative evaluation . In the paper we propose a _entity3_ _C_ black-box method _/entity3_ for comparing the _entity4_ lexical coverage _/entity4_ of _entity5_ MT systems _/entity5_ . The method is based on lists of _entity6_ words _/entity6_ from different _entity7_ frequency classes _/entity7_ . It is shown how these _entity8_ word lists _/entity8_ can be compiled and used for testing . We also present the results of using our method on 6 _entity9_ MT systems _/entity9_ that translate between _entity10_ English _/entity10_ and _entity11_ German _/entity11_ .	NONE entity1 entity3
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ _C_ text _/entity16_ and by the _entity17_ _P_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity17 entity16
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ _P_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ _C_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity20 entity22
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ _C_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ _P_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity13 entity11
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ _P_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ _C_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity5 entity7
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ _P_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ _C_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity13 entity16
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ _C_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ _P_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity8 entity6
We propose a draft scheme of the _entity1_ model _/entity1_ formalizing the _entity2_ _P_ structure of communicative context _/entity2_ in _entity3_ _C_ dialogue interaction _/entity3_ . The relationships between the interacting partners are considered as system of three automata representing the partners of the _entity4_ dialogue _/entity4_ and environment .	MODEL-FEATURE entity2 entity3
This paper proposes an _entity1_ alignment adaptation approach _/entity1_ to improve _entity2_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ alignment adaptation _/entity3_ is to use _entity4_ out-of-domain corpus _/entity4_ to improve _entity5_ _P_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ _C_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ precision _/entity11_ and _entity12_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity5 entity7
_entity1_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ language-independent system _/entity2_ for _entity3_ automatic discovery of text _/entity3_ in _entity4_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ _C_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ words _/entity9_ per _entity10_ _P_ language _/entity10_ .	NONE entity10 entity7
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ _C_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ _P_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity15 entity13
In this paper we discuss a proposed _entity1_ user knowledge modeling architecture _/entity1_ for the _entity2_ ICICLE system _/entity2_ , a _entity3_ language tutoring application _/entity3_ for deaf learners of _entity4_ written English _/entity4_ . The model will represent the _entity5_ language proficiency _/entity5_ of the user and is designed to be referenced during both _entity6_ writing analysis _/entity6_ and _entity7_ feedback production _/entity7_ . We motivate our _entity8_ model design _/entity8_ by citing relevant research on _entity9_ _C_ second language and cognitive skill acquisition _/entity9_ , and briefly discuss preliminary empirical evidence supporting the _entity10_ design _/entity10_ . We conclude by showing how our _entity11_ _P_ design _/entity11_ can provide a rich and _entity12_ robust information base _/entity12_ to a language assessment / correction application by modeling _entity13_ user proficiency _/entity13_ at a high level of granularity and specificity .	NONE entity11 entity9
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ _C_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ _P_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity10 entity8
This paper presents a _entity1_ _P_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ _C_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity1 entity4
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ _P_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ _C_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity8 entity11
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ _P_ paraphrase probability _/entity10_ that allows _entity11_ _C_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity10 entity11
This paper proposes an approach to _entity1_ full parsing _/entity1_ suitable for _entity2_ _C_ Information Extraction _/entity2_ from _entity3_ texts _/entity3_ . Sequences of cascades of _entity4_ rules _/entity4_ deterministically analyze the _entity5_ _P_ text _/entity5_ , building _entity6_ unambiguous structures _/entity6_ . Initially basic _entity7_ chunks _/entity7_ are analyzed ; then _entity8_ argumental relations _/entity8_ are recognized ; finally _entity9_ modifier attachment _/entity9_ is performed and the _entity10_ global parse tree _/entity10_ is built . The approach was proven to work for three _entity11_ languages _/entity11_ and different _entity12_ domains _/entity12_ . It was implemented in the _entity13_ IE module _/entity13_ of _entity14_ FACILE , a EU project for multilingual text classification and IE _/entity14_ .	NONE entity5 entity2
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ _P_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ _C_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity2 entity4
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ _C_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ _P_ question phrases _/entity5_ in _entity6_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ MRR _/entity14_ .	NONE entity5 entity4
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ _P_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ _C_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity11 entity14
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ _C_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ _P_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity14 entity13
This paper describes to what extent _entity1_ deep processing _/entity1_ may benefit from _entity2_ shallow techniques _/entity2_ and it presents a _entity3_ NLP system _/entity3_ which integrates a _entity4_ linguistic PoS tagger and chunker _/entity4_ as a preprocessing module of a _entity5_ broad coverage unification based grammar of Spanish _/entity5_ . Experiments show that the _entity6_ _C_ efficiency _/entity6_ of the overall analysis improves significantly and that our system also provides _entity7_ robustness _/entity7_ to the _entity8_ linguistic processing _/entity8_ while maintaining both the _entity9_ _P_ accuracy _/entity9_ and the _entity10_ precision _/entity10_ of the _entity11_ grammar _/entity11_ .	NONE entity9 entity6
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ _C_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ _P_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity7 entity5
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ _P_ accuracy _/entity7_ of the _entity8_ _C_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity7 entity8
This paper reports on two contributions to _entity1_ _P_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ _C_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity1 entity2
We have developed a _entity1_ _C_ computational model _/entity1_ of the process of describing the layout of an apartment or house , a much-studied _entity2_ _P_ discourse task _/entity2_ first characterized linguistically by Linde ( 1974 ) . The _entity3_ model _/entity3_ is embodied in a program , _entity4_ APT _/entity4_ , that can reproduce segments of actual tape-recorded descriptions , using _entity5_ organizational and discourse strategies _/entity5_ derived through analysis of our _entity6_ corpus _/entity6_ .	NONE entity2 entity1
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ natural language interface _/entity2_ to _entity3_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ sentences _/entity6_ in _entity7_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ _C_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ _P_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity12 entity9
We present an application of _entity1_ _P_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ _C_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity1 entity3
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ _P_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ _C_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	NONE entity4 entity7
In this paper , we present a _entity1_ fully automated extraction system _/entity1_ , named _entity2_ IntEx _/entity2_ , to identify _entity3_ gene and protein interactions _/entity3_ in _entity4_ biomedical text _/entity4_ . Our approach is based on first splitting _entity5_ complex sentences _/entity5_ into _entity6_ simple clausal structures _/entity6_ made up of _entity7_ syntactic roles _/entity7_ . Then , tagging _entity8_ biological entities _/entity8_ with the help of _entity9_ biomedical and linguistic ontologies _/entity9_ . Finally , extracting _entity10_ complete interactions _/entity10_ by analyzing the matching contents of _entity11_ syntactic roles _/entity11_ and their linguistically significant combinations . Our _entity12_ extraction system _/entity12_ handles _entity13_ _C_ complex sentences _/entity13_ and extracts _entity14_ multiple and nested interactions _/entity14_ specified in a _entity15_ _P_ sentence _/entity15_ . Experimental evaluations with two other state of the art _entity16_ extraction systems _/entity16_ indicate that the _entity17_ IntEx system _/entity17_ achieves better _entity18_ performance _/entity18_ without the labor intensive _entity19_ pattern engineering requirement _/entity19_ .	NONE entity15 entity13
In this study , we propose a _entity1_ knowledge-independent method _/entity1_ for aligning _entity2_ terms _/entity2_ and thus extracting _entity3_ translations _/entity3_ from a _entity4_ small , domain-specific corpus _/entity4_ consisting of _entity5_ parallel English and Chinese court judgments _/entity5_ from Hong Kong . With a _entity6_ sentence-aligned corpus _/entity6_ , _entity7_ translation equivalences _/entity7_ are suggested by analysing the _entity8_ frequency profiles _/entity8_ of _entity9_ parallel concordances _/entity9_ . The method overcomes the limitations of _entity10_ conventional statistical methods _/entity10_ which require _entity11_ large corpora _/entity11_ to be effective , and _entity12_ lexical approaches _/entity12_ which depend on existing _entity13_ bilingual dictionaries _/entity13_ . Pilot testing on a _entity14_ parallel corpus _/entity14_ of about 113K _entity15_ Chinese words _/entity15_ and 120K _entity16_ English words _/entity16_ gives an encouraging 85 % _entity17_ precision _/entity17_ and 45 % _entity18_ recall _/entity18_ . Future work includes fine-tuning the _entity19_ algorithm _/entity19_ upon the analysis of the errors , and acquiring a _entity20_ _P_ translation lexicon _/entity20_ for _entity21_ _C_ legal terminology _/entity21_ by filtering out _entity22_ general terms _/entity22_ .	NONE entity20 entity21
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ _C_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ _P_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity17 entity16
We describe a method for identifying systematic _entity1_ patterns _/entity1_ in _entity2_ translation data _/entity2_ using _entity3_ part-of-speech tag sequences _/entity3_ . We incorporate this analysis into a _entity4_ diagnostic tool _/entity4_ intended for _entity5_ developers _/entity5_ of _entity6_ _P_ machine translation systems _/entity6_ , and demonstrate how our application can be used by _entity7_ developers _/entity7_ to explore _entity8_ patterns _/entity8_ in _entity9_ _C_ machine translation output _/entity9_ .	NONE entity6 entity9
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ _C_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ _P_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity22 entity20
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ _P_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ _C_ disambiguation tool _/entity14_ .	USAGE entity12 entity14
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ _P_ hybrid architecture _/entity7_ has lead to a _entity8_ _C_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity7 entity8
This paper examines the properties of _entity1_ _C_ feature-based partial descriptions _/entity1_ built on top of _entity2_ _P_ Halliday 's systemic networks _/entity2_ . We show that the crucial operation of _entity3_ consistency checking _/entity3_ for such descriptions is NP-complete , and therefore probably intractable , but proceed to develop _entity4_ algorithms _/entity4_ which can sometimes alleviate the unpleasant consequences of this _entity5_ intractability _/entity5_ .	NONE entity2 entity1
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ _C_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ _P_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ TAGs _/entity14_ to be used beyond their role in _entity15_ syntax proper _/entity15_ . We discuss the application of _entity16_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity8 entity5
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ _C_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ _P_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity22 entity19
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ _C_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ _P_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity6 entity5
We present a new _entity1_ part-of-speech tagger _/entity1_ that demonstrates the following ideas : ( i ) explicit use of both preceding and following _entity2_ tag contexts _/entity2_ via a _entity3_ _C_ dependency network representation _/entity3_ , ( ii ) broad use of _entity4_ _P_ lexical features _/entity4_ , including _entity5_ jointly conditioning on multiple consecutive words _/entity5_ , ( iii ) effective use of _entity6_ priors _/entity6_ in _entity7_ conditional loglinear models _/entity7_ , and ( iv ) fine-grained modeling of _entity8_ unknown word features _/entity8_ . Using these ideas together , the resulting _entity9_ tagger _/entity9_ gives a 97.24 % _entity10_ accuracy _/entity10_ on the _entity11_ Penn Treebank WSJ _/entity11_ , an _entity12_ error reduction _/entity12_ of 4.4 % on the best previous single automatically learned _entity13_ tagging _/entity13_ result .	NONE entity4 entity3
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ _C_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ _P_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity12 entity9
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ _C_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ _P_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity19 entity17
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ _C_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ _P_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity14 entity11
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ _C_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ _P_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity15 entity12
We propose a method of organizing reading materials for _entity1_ vocabulary learning _/entity1_ . It enables us to select a concise set of reading _entity2_ texts _/entity2_ ( from a _entity3_ target corpus _/entity3_ ) that contains all the _entity4_ target vocabulary _/entity4_ to be learned . We used a specialized _entity5_ vocabulary _/entity5_ for an English certification test as the _entity6_ target vocabulary _/entity6_ and used _entity7_ _P_ English Wikipedia _/entity7_ , a free-content encyclopedia , as the _entity8_ target corpus _/entity8_ . The organized reading materials would enable learners not only to study the _entity9_ _C_ target vocabulary _/entity9_ efficiently but also to gain a variety of knowledge through reading . The reading materials are available on our web site .	NONE entity7 entity9
This paper presents a novel _entity1_ ensemble learning approach _/entity1_ to resolving _entity2_ German pronouns _/entity2_ . _entity3_ Boosting _/entity3_ , the method in question , combines the moderately accurate _entity4_ hypotheses _/entity4_ of several _entity5_ classifiers _/entity5_ to form a highly accurate one . Experiments show that this approach is superior to a single _entity6_ _C_ decision-tree classifier _/entity6_ . Furthermore , we present a _entity7_ standalone system _/entity7_ that resolves _entity8_ pronouns _/entity8_ in _entity9_ _P_ unannotated text _/entity9_ by using a fully automatic sequence of _entity10_ preprocessing modules _/entity10_ that mimics the _entity11_ manual annotation process _/entity11_ . Although the system performs well within a limited _entity12_ textual domain _/entity12_ , further research is needed to make it effective for _entity13_ open-domain question answering _/entity13_ and _entity14_ text summarisation _/entity14_ .	NONE entity9 entity6
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ _C_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ _P_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity13 entity10
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ training and test data _/entity5_ with respect to _entity6_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ training data _/entity7_ labeled with _entity8_ _C_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ domain _/entity9_ , _entity10_ _P_ topic _/entity10_ and time .	NONE entity10 entity8
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ CCRs _/entity3_ are _entity4_ _P_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ categories _/entity5_ in _entity6_ local trees _/entity6_ which allow the _entity7_ _C_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	NONE entity4 entity7
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ _C_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ categories _/entity4_ in a published _entity5_ _P_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity5 entity3
Recently , we initiated a project to develop a _entity1_ phonetically-based spoken language understanding system _/entity1_ called _entity2_ SUMMIT _/entity2_ . In contrast to many of the past efforts that make use of _entity3_ heuristic rules _/entity3_ whose development requires intense _entity4_ knowledge engineering _/entity4_ , our approach attempts to express the _entity5_ speech knowledge _/entity5_ within a formal framework using well-defined mathematical tools . In our system , _entity6_ _C_ features _/entity6_ and _entity7_ _P_ decision strategies _/entity7_ are discovered and trained automatically , using a large body of _entity8_ speech data _/entity8_ . This paper describes the system , and documents its current performance .	NONE entity7 entity6
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ _C_ vectors _/entity4_ is then calculated . The _entity5_ _P_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity5 entity4
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ _C_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ _P_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity17 entity15
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ _C_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ _P_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity11 entity9
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ _C_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ _P_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity10 entity9
This paper presents a _entity1_ machine learning approach _/entity1_ to _entity2_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ corpus-based sample _/entity5_ and formulate them as _entity6_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ clauses _/entity7_ to create a set of _entity8_ domain independent features _/entity8_ to annotate an _entity9_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ _P_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ _C_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity13 entity16
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ _P_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ _C_ implementation _/entity20_ .	NONE entity17 entity20
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ _C_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ _P_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity24 entity21
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ _C_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ _P_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity14 entity12
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ _C_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ _P_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity15 entity13
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ _P_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ _C_ natural language generation _/entity45_ .	NONE entity42 entity45
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ _C_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ _P_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity30 entity28
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ _P_ laboratory study _/entity9_ using the _entity10_ _C_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity9 entity10
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ _C_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ _P_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity14 entity13
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ _P_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ _C_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity8 entity10
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ _C_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ _P_ TAGs _/entity14_ to be used beyond their role in _entity15_ syntax proper _/entity15_ . We discuss the application of _entity16_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity14 entity13
This paper describes the status of the _entity1_ MIT ATIS system _/entity1_ as of February 1992 , focusing especially on the changes made to the _entity2_ SUMMIT recognizer _/entity2_ . These include _entity3_ _P_ context-dependent phonetic modelling _/entity3_ , the use of a _entity4_ bigram language model _/entity4_ in conjunction with a _entity5_ _C_ probabilistic LR parser _/entity5_ , and refinements made to the _entity6_ lexicon _/entity6_ . Together with the use of a larger _entity7_ training set _/entity7_ , these modifications combined to reduce the _entity8_ speech recognition word and sentence error rates _/entity8_ by a factor of 2.5 and 1.6 , respectively , on the _entity9_ October '91 test set _/entity9_ . The weighted error for the entire _entity10_ spoken language system _/entity10_ on the same _entity11_ test set _/entity11_ is 49.3 % . Similar results were also obtained on the _entity12_ February '92 benchmark evaluation _/entity12_ .	NONE entity3 entity5
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ _C_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ _P_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity10 entity8
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ _P_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ _C_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity17 entity20
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ _P_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ _C_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity12 entity15
Reducing _entity1_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ LM pruning _/entity3_ . They are probability , _entity4_ rank _/entity4_ , and _entity5_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ pruning criteria _/entity6_ in a real application of _entity7_ Chinese text input _/entity7_ in terms of _entity8_ _C_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ _P_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ CER _/entity13_ .	NONE entity11 entity8
In this paper , we describe a search procedure for _entity1_ _P_ statistical machine translation ( MT ) _/entity1_ based on _entity2_ dynamic programming ( DP ) _/entity2_ . Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible _entity3_ word reordering _/entity3_ between _entity4_ _C_ source and target language _/entity4_ in order to achieve an efficient search algorithm . A search restriction especially useful for the translation direction from German to English is presented . The experimental tests are carried out on the _entity5_ Verbmobil task _/entity5_ ( German-English , 8000-word vocabulary ) , which is a _entity6_ limited-domain spoken-language task _/entity6_ .	NONE entity1 entity4
This paper describes a new , _entity1_ large scale discourse-level annotation _/entity1_ project - the _entity2_ _P_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ _C_ discourse structure _/entity3_ that is based on identifying _entity4_ discourse connectives _/entity4_ and their _entity5_ arguments _/entity5_ . The _entity6_ PDTB _/entity6_ is being built directly on top of the _entity7_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ inter-annotator agreement _/entity11_ - both the _entity12_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity2 entity3
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ _C_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ _P_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity18 entity16
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ _P_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ _C_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity27 entity29
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ _P_ 66 % recall rate _/entity21_ . A new _entity22_ _C_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity21 entity22
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ _P_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ _C_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity8 entity10
_entity1_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ language-independent system _/entity2_ for _entity3_ automatic discovery of text _/entity3_ in _entity4_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ _P_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ _C_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ words _/entity9_ per _entity10_ language _/entity10_ .	NONE entity7 entity8
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ _C_ performance _/entity5_ of using _entity6_ _P_ ASR output _/entity6_ as opposed to _entity7_ human transcription _/entity7_ . Examination of the effect of _entity8_ features _/entity8_ shows that _entity9_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity6 entity5
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ _P_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ _C_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ phrase translations _/entity9_ in our _entity10_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity4 entity7
An attempt has been made to use an _entity1_ Augmented Transition Network _/entity1_ as a procedural _entity2_ dialog model _/entity2_ . The development of such a _entity3_ model _/entity3_ appears to be important in several respects : as a device to represent and to use different _entity4_ dialog schemata _/entity4_ proposed in empirical _entity5_ conversation analysis _/entity5_ ; as a device to represent and to use _entity6_ models of verbal interaction _/entity6_ ; as a device combining knowledge about _entity7_ dialog schemata _/entity7_ and about _entity8_ verbal interaction _/entity8_ with knowledge about _entity9_ task-oriented and goal-directed dialogs _/entity9_ . A standard _entity10_ _P_ ATN _/entity10_ should be further developed in order to account for the _entity11_ _C_ verbal interactions _/entity11_ of _entity12_ task-oriented dialogs _/entity12_ .	NONE entity10 entity11
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ _C_ GLOSSER _/entity4_ : _entity5_ _P_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity5 entity4
In order to boost the _entity1_ translation quality _/entity1_ of _entity2_ _P_ EBMT _/entity2_ based on a small-sized _entity3_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ _C_ language model _/entity5_ of an in-domain _entity6_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ BLEU score _/entity9_ and the _entity10_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity2 entity5
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ word-aligned corpora _/entity5_ is proposed . A _entity6_ statistical translation model _/entity6_ is also presented that deals such _entity7_ phrases _/entity7_ , as well as a _entity8_ training method _/entity8_ based on the maximization of _entity9_ translation accuracy _/entity9_ , as measured with the _entity10_ _C_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ _P_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity12 entity10
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ _C_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ _P_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity14 entity12
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ _C_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ _P_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity13 entity10
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ _P_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ _C_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity3 entity4
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ _C_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ _P_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity18 entity15
_entity1_ Determiners _/entity1_ play an important role in conveying the _entity2_ meaning _/entity2_ of an _entity3_ utterance _/entity3_ , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the _entity4_ global meaning _/entity4_ of a _entity5_ sentence _/entity5_ , even if not in a precise way . Another problem with _entity6_ determiners _/entity6_ is their inherent _entity7_ ambiguity _/entity7_ . In this paper we propose a _entity8_ _P_ logical formalism _/entity8_ , which , among other things , is suitable for representing _entity9_ _C_ determiners _/entity9_ without forcing a particular _entity10_ interpretation _/entity10_ when their _entity11_ meaning _/entity11_ is still not clear .	NONE entity8 entity9
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ _C_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ _P_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity21 entity19
We present results on _entity1_ addressee identification _/entity1_ in _entity2_ four-participants face-to-face meetings _/entity2_ using _entity3_ Bayesian Network _/entity3_ and _entity4_ Naive Bayes classifiers _/entity4_ . First , we investigate how well the _entity5_ addressee _/entity5_ of a _entity6_ dialogue act _/entity6_ can be predicted based on _entity7_ gaze _/entity7_ , _entity8_ utterance _/entity8_ and _entity9_ conversational context features _/entity9_ . Then , we explore whether information about _entity10_ meeting context _/entity10_ can aid _entity11_ classifiers _/entity11_ ' _entity12_ performances _/entity12_ . Both _entity13_ classifiers _/entity13_ perform the best when _entity14_ conversational context _/entity14_ and _entity15_ utterance features _/entity15_ are combined with _entity16_ _P_ speaker 's gaze information _/entity16_ . The _entity17_ classifiers _/entity17_ show little _entity18_ gain _/entity18_ from information about _entity19_ _C_ meeting context _/entity19_ .	NONE entity16 entity19
This paper describes three relatively _entity1_ domain-independent capabilities _/entity1_ recently added to the _entity2_ Paramax spoken language understanding system _/entity2_ : _entity3_ non-monotonic reasoning _/entity3_ , _entity4_ implicit reference resolution _/entity4_ , and _entity5_ database query paraphrase _/entity5_ . In addition , we discuss the results of the _entity6_ February 1992 ATIS benchmark tests _/entity6_ . We describe a variation on the _entity7_ standard evaluation metric _/entity7_ which provides a more tightly controlled measure of progress . Finally , we briefly describe an experiment which we have done in extending the _entity8_ _P_ n-best speech/language integration architecture _/entity8_ to improving _entity9_ _C_ OCR _/entity9_ _entity10_ accuracy _/entity10_ .	NONE entity8 entity9
Motivated by the success of _entity1_ _C_ ensemble methods _/entity1_ in _entity2_ _P_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity2 entity1
In this paper , we describe the _entity1_ pronominal anaphora resolution module _/entity1_ of _entity2_ Lucy _/entity2_ , a portable _entity3_ English understanding system _/entity3_ . The design of this module was motivated by the observation that , although there exist many theories of _entity4_ _C_ anaphora resolution _/entity4_ , no one of these theories is complete . Thus we have implemented a _entity5_ _P_ blackboard-like architecture _/entity5_ in which individual _entity6_ partial theories _/entity6_ can be encoded as separate modules that can interact to propose candidate _entity7_ antecedents _/entity7_ and to evaluate each other 's proposals .	NONE entity5 entity4
A new approach for _entity1_ _C_ Interactive Machine Translation _/entity1_ where the _entity2_ _P_ author _/entity2_ interacts during the creation or the modification of the _entity3_ document _/entity3_ is proposed . The explanation of an _entity4_ ambiguity _/entity4_ or an error for the purposes of correction does not use any concepts of the underlying _entity5_ linguistic theory _/entity5_ : it is a reformulation of the erroneous or ambiguous _entity6_ sentence _/entity6_ . The interaction is limited to the analysis step of the _entity7_ translation process _/entity7_ . This paper presents a new _entity8_ interactive disambiguation scheme _/entity8_ based on the _entity9_ paraphrasing _/entity9_ of a _entity10_ parser _/entity10_ 's multiple output . Some examples of _entity11_ paraphrasing _/entity11_ ambiguous _entity12_ sentences _/entity12_ are presented .	NONE entity2 entity1
This paper presents the results of automatically inducing a _entity1_ Combinatory Categorial Grammar ( CCG ) lexicon _/entity1_ from a _entity2_ Turkish dependency treebank _/entity2_ . The fact that _entity3_ Turkish _/entity3_ is an _entity4_ agglutinating free word order language _/entity4_ presents a challenge for _entity5_ language theories _/entity5_ . We explored possible ways to obtain a _entity6_ compact lexicon _/entity6_ , consistent with _entity7_ CCG principles _/entity7_ , from a _entity8_ _C_ treebank _/entity8_ which is an order of magnitude smaller than _entity9_ _P_ Penn WSJ _/entity9_ .	NONE entity9 entity8
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ _P_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ _C_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity15 entity18
This paper discusses the application of _entity1_ Unification Categorial Grammar ( UCG ) _/entity1_ to the framework of _entity2_ Isomorphic Grammars _/entity2_ for _entity3_ Machine Translation _/entity3_ pioneered by Landsbergen . The _entity4_ Isomorphic Grammars approach to MT _/entity4_ involves developing the _entity5_ grammars _/entity5_ of the _entity6_ Source and Target languages _/entity6_ in parallel , in order to ensure that _entity7_ SL _/entity7_ and _entity8_ TL _/entity8_ expressions which stand in the _entity9_ translation relation _/entity9_ have _entity10_ isomorphic derivations _/entity10_ . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited , obviating the need for answers to _entity11_ semantic questions _/entity11_ that we do not yet have . _entity12_ Semantic _/entity12_ and other information may still be incorporated , but as constraints on the _entity13_ _C_ translation relation _/entity13_ , not as levels of _entity14_ textual representation _/entity14_ . After introducing this approach to _entity15_ _P_ MT system _/entity15_ design , and the basics of _entity16_ monolingual UCG _/entity16_ , we will show how the two can be integrated , and present an example from an implemented _entity17_ bi-directional English-Spanish fragment _/entity17_ . Finally we will present some outstanding problems with the approach .	NONE entity15 entity13
This paper describes the understanding process of the _entity1_ spatial descriptions _/entity1_ in _entity2_ Japanese _/entity2_ . In order to understand the described _entity3_ world _/entity3_ , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space . It is done by an experimental _entity4_ computer program _/entity4_ _entity5_ SPRINT _/entity5_ , which takes _entity6_ natural language texts _/entity6_ and produces a _entity7_ model _/entity7_ of the described _entity8_ world _/entity8_ . To reconstruct the _entity9_ model _/entity9_ , the authors extract the _entity10_ qualitative spatial constraints _/entity10_ from the _entity11_ text _/entity11_ , and represent them as the _entity12_ numerical constraints _/entity12_ on the _entity13_ _P_ spatial attributes _/entity13_ of the _entity14_ entities _/entity14_ . This makes it possible to express the vagueness of the _entity15_ _C_ spatial concepts _/entity15_ and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints . The interpretation reflects the _entity16_ temporary belief _/entity16_ about the _entity17_ world _/entity17_ .	NONE entity13 entity15
Despite much recent progress on accurate _entity1_ semantic role labeling _/entity1_ , previous work has largely used _entity2_ independent classifiers _/entity2_ , possibly combined with separate _entity3_ label sequence models _/entity3_ via _entity4_ _P_ Viterbi decoding _/entity4_ . This stands in stark contrast to the linguistic observation that a _entity5_ core argument frame _/entity5_ is a joint structure , with strong _entity6_ dependencies _/entity6_ between _entity7_ _C_ arguments _/entity7_ . We show how to build a _entity8_ joint model _/entity8_ of _entity9_ argument frames _/entity9_ , incorporating novel _entity10_ features _/entity10_ that model these interactions into _entity11_ discriminative log-linear models _/entity11_ . This system achieves an _entity12_ error reduction _/entity12_ of 22 % on all _entity13_ arguments _/entity13_ and 32 % on _entity14_ core arguments _/entity14_ over a state-of-the art independent _entity15_ classifier _/entity15_ for _entity16_ gold-standard parse trees _/entity16_ on _entity17_ PropBank _/entity17_ .	NONE entity4 entity7
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ _C_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ _P_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity17 entity16
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ _P_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ _C_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity13 entity15
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ _P_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ _C_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity11 entity13
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ _C_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ _P_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity12 entity10
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ _C_ senses _/entity10_ of _entity11_ _P_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity11 entity10
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ _C_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ _P_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity10 entity8
The reality of _entity1_ analogies between words _/entity1_ is refuted by noone ( e.g. , I walked is to to walk as I laughed is to to laugh , noted I walked : to walk : : I laughed : to laugh ) . But _entity2_ computational linguists _/entity2_ seem to be quite dubious about _entity3_ analogies between sentences _/entity3_ : they would not be enough numerous to be of any use . We report experiments conducted on a _entity4_ multilingual corpus _/entity4_ to estimate the number of _entity5_ analogies _/entity5_ among the _entity6_ sentences _/entity6_ that it contains . We give two estimates , a lower one and a higher one . As an _entity7_ analogy _/entity7_ must be valid on the level of _entity8_ _C_ form _/entity8_ as well as on the level of _entity9_ meaning _/entity9_ , we relied on the idea that _entity10_ _P_ translation _/entity10_ should preserve _entity11_ meaning _/entity11_ to test for similar _entity12_ meanings _/entity12_ .	NONE entity10 entity8
Despite much recent progress on accurate _entity1_ semantic role labeling _/entity1_ , previous work has largely used _entity2_ independent classifiers _/entity2_ , possibly combined with separate _entity3_ label sequence models _/entity3_ via _entity4_ Viterbi decoding _/entity4_ . This stands in stark contrast to the linguistic observation that a _entity5_ core argument frame _/entity5_ is a joint structure , with strong _entity6_ dependencies _/entity6_ between _entity7_ arguments _/entity7_ . We show how to build a _entity8_ joint model _/entity8_ of _entity9_ argument frames _/entity9_ , incorporating novel _entity10_ features _/entity10_ that model these interactions into _entity11_ discriminative log-linear models _/entity11_ . This system achieves an _entity12_ error reduction _/entity12_ of 22 % on all _entity13_ arguments _/entity13_ and 32 % on _entity14_ _C_ core arguments _/entity14_ over a state-of-the art independent _entity15_ classifier _/entity15_ for _entity16_ gold-standard parse trees _/entity16_ on _entity17_ _P_ PropBank _/entity17_ .	NONE entity17 entity14
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ _C_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ _P_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity20 entity19
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ _C_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ _P_ lyric-based song sentiment classification task _/entity19_ .	NONE entity19 entity16
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ _C_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ _P_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity7 entity6
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ _P_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ _C_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity19 entity21
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ _P_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ _C_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity19 entity22
_entity1_ Large-scale natural language generation _/entity1_ requires the integration of vast amounts of _entity2_ knowledge _/entity2_ : lexical , grammatical , and conceptual . A _entity3_ robust generator _/entity3_ must be able to operate well even when pieces of _entity4_ knowledge _/entity4_ are missing . It must also be robust against _entity5_ _P_ incomplete or inaccurate inputs _/entity5_ . To attack these problems , we have built a _entity6_ hybrid generator _/entity6_ , in which gaps in _entity7_ _C_ symbolic knowledge _/entity7_ are filled by _entity8_ statistical methods _/entity8_ . We describe algorithms and show experimental results . We also discuss how the _entity9_ hybrid generation model _/entity9_ can be used to simplify current _entity10_ generators _/entity10_ and enhance their _entity11_ portability _/entity11_ , even when perfect _entity12_ knowledge _/entity12_ is in principle obtainable .	NONE entity5 entity7
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ _P_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ _C_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity17 entity19
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ _P_ similarity _/entity7_ , i.e. , _entity8_ _C_ taxonomic similarity _/entity8_ and _entity9_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ dictionary-based word vectors _/entity10_ better reflect _entity11_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity7 entity8
In this paper _entity1_ discourse segments _/entity1_ are defined and a method for _entity2_ discourse segmentation _/entity2_ primarily based on _entity3_ abduction _/entity3_ of _entity4_ _C_ temporal relations _/entity4_ between _entity5_ _P_ segments _/entity5_ is proposed . This method is precise and _entity6_ computationally feasible _/entity6_ and is supported by previous work in the area of _entity7_ temporal anaphora resolution _/entity7_ .	NONE entity5 entity4
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ _C_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ _P_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity8 entity6
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ _P_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ _C_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity4 entity6
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ events _/entity2_ are defined as _entity3_ _C_ event terms _/entity3_ and _entity4_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ _P_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ documents _/entity9_ . Experimental results are encouraging .	NONE entity6 entity3
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ _P_ parsing _/entity8_ of _entity9_ _C_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	USAGE entity8 entity9
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ _C_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ _P_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity10 entity8
A novel _entity1_ bootstrapping approach _/entity1_ to _entity2_ Named Entity ( NE ) tagging _/entity2_ using _entity3_ concept-based seeds _/entity3_ and _entity4_ successive learners _/entity4_ is presented . This approach only requires a few _entity5_ common noun _/entity5_ or _entity6_ pronoun _/entity6_ _entity7_ seeds _/entity7_ that correspond to the _entity8_ concept _/entity8_ for the targeted _entity9_ NE _/entity9_ , e.g . he/she/man/woman for _entity10_ PERSON NE _/entity10_ . The _entity11_ bootstrapping procedure _/entity11_ is implemented as training two _entity12_ successive learners _/entity12_ . First , _entity13_ decision list _/entity13_ is used to learn the _entity14_ parsing-based NE rules _/entity14_ . Then , a _entity15_ Hidden Markov Model _/entity15_ is trained on a _entity16_ corpus _/entity16_ automatically tagged by the first _entity17_ _C_ learner _/entity17_ . The resulting _entity18_ NE system _/entity18_ approaches _entity19_ supervised NE _/entity19_ performance for some _entity20_ _P_ NE types _/entity20_ .	NONE entity20 entity17
We present results on _entity1_ addressee identification _/entity1_ in _entity2_ four-participants face-to-face meetings _/entity2_ using _entity3_ Bayesian Network _/entity3_ and _entity4_ Naive Bayes classifiers _/entity4_ . First , we investigate how well the _entity5_ _C_ addressee _/entity5_ of a _entity6_ _P_ dialogue act _/entity6_ can be predicted based on _entity7_ gaze _/entity7_ , _entity8_ utterance _/entity8_ and _entity9_ conversational context features _/entity9_ . Then , we explore whether information about _entity10_ meeting context _/entity10_ can aid _entity11_ classifiers _/entity11_ ' _entity12_ performances _/entity12_ . Both _entity13_ classifiers _/entity13_ perform the best when _entity14_ conversational context _/entity14_ and _entity15_ utterance features _/entity15_ are combined with _entity16_ speaker 's gaze information _/entity16_ . The _entity17_ classifiers _/entity17_ show little _entity18_ gain _/entity18_ from information about _entity19_ meeting context _/entity19_ .	NONE entity6 entity5
This paper discusses two problems that arise in the _entity1_ _P_ Generation _/entity1_ of _entity2_ _C_ Referring Expressions _/entity2_ : ( a ) _entity3_ numeric-valued attributes _/entity3_ , such as size or location ; ( b ) _entity4_ perspective-taking _/entity4_ in _entity5_ reference _/entity5_ . Both problems , it is argued , can be resolved if some structure is imposed on the available knowledge prior to _entity6_ content determination _/entity6_ . We describe a _entity7_ clustering algorithm _/entity7_ which is sufficiently general to be applied to these diverse problems , discuss its application , and evaluate its performance .	NONE entity1 entity2
We present an efficient algorithm for the _entity1_ redundancy elimination problem _/entity1_ : Given an _entity2_ underspecified semantic representation ( USR ) _/entity2_ of a _entity3_ _P_ scope ambiguity _/entity3_ , compute an _entity4_ USR _/entity4_ with fewer mutually _entity5_ _C_ equivalent readings _/entity5_ . The algorithm operates on _entity6_ underspecified chart representations _/entity6_ which are derived from _entity7_ dominance graphs _/entity7_ ; it can be applied to the _entity8_ USRs _/entity8_ computed by _entity9_ large-scale grammars _/entity9_ . We evaluate the algorithm on a _entity10_ corpus _/entity10_ , and show that it reduces the degree of _entity11_ ambiguity _/entity11_ significantly while taking negligible runtime .	NONE entity3 entity5
A purely functional implementation of _entity1_ _P_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ _C_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity1 entity4
The _entity1_ JAVELIN system _/entity1_ integrates a flexible , _entity2_ planning-based architecture _/entity2_ with a variety of _entity3_ language processing modules _/entity3_ to provide an _entity4_ open-domain question answering capability _/entity4_ on _entity5_ free text _/entity5_ . The demonstration will focus on how _entity6_ JAVELIN _/entity6_ processes _entity7_ questions _/entity7_ and retrieves the most likely _entity8_ _C_ answer candidates _/entity8_ from the given _entity9_ text corpus _/entity9_ . The operation of the system will be explained in depth through browsing the _entity10_ _P_ repository _/entity10_ of _entity11_ data objects _/entity11_ created by the system during each _entity12_ question answering session _/entity12_ .	NONE entity10 entity8
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ _P_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ _C_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity10 entity13
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ _C_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ _P_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity11 entity9
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ _P_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ _C_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity19 entity22
This paper reports a completed stage of ongoing research at the University of York . Landsbergen 's advocacy of _entity1_ analytical inverses _/entity1_ for _entity2_ compositional syntax rules _/entity2_ encourages the application of _entity3_ Definite Clause Grammar techniques _/entity3_ to the construction of a _entity4_ parser _/entity4_ returning _entity5_ Montague analysis trees _/entity5_ . A _entity6_ parser MDCC _/entity6_ is presented which implements an _entity7_ augmented Friedman - Warren algorithm _/entity7_ permitting _entity8_ post referencing _/entity8_ * and interfaces with a language of _entity9_ intenslonal logic translator LILT _/entity9_ so as to display the _entity10_ _P_ derivational history _/entity10_ of corresponding _entity11_ reduced IL formulae _/entity11_ . Some familiarity with _entity12_ Montague 's PTQ _/entity12_ and the _entity13_ _C_ basic DCG mechanism _/entity13_ is assumed .	NONE entity10 entity13
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ _P_ events _/entity2_ are defined as _entity3_ _C_ event terms _/entity3_ and _entity4_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ documents _/entity9_ . Experimental results are encouraging .	NONE entity2 entity3
_entity1_ Techniques for automatically training _/entity1_ modules of a _entity2_ natural language generator _/entity2_ have recently been proposed , but a fundamental concern is whether the _entity3_ _P_ quality _/entity3_ of _entity4_ utterances _/entity4_ produced with _entity5_ trainable components _/entity5_ can compete with _entity6_ _C_ hand-crafted template-based or rule-based approaches _/entity6_ . In this paper We experimentally evaluate a _entity7_ trainable sentence planner _/entity7_ for a _entity8_ spoken dialogue system _/entity8_ by eliciting _entity9_ subjective human judgments _/entity9_ . In order to perform an exhaustive comparison , we also evaluate a _entity10_ hand-crafted template-based generation component _/entity10_ , two _entity11_ rule-based sentence planners _/entity11_ , and two _entity12_ baseline sentence planners _/entity12_ . We show that the _entity13_ trainable sentence planner _/entity13_ performs better than the _entity14_ rule-based systems _/entity14_ and the _entity15_ baselines _/entity15_ , and as well as the _entity16_ hand-crafted system _/entity16_ .	NONE entity3 entity6
_entity1_ Techniques for automatically training _/entity1_ modules of a _entity2_ natural language generator _/entity2_ have recently been proposed , but a fundamental concern is whether the _entity3_ quality _/entity3_ of _entity4_ utterances _/entity4_ produced with _entity5_ trainable components _/entity5_ can compete with _entity6_ hand-crafted template-based or rule-based approaches _/entity6_ . In this paper We experimentally evaluate a _entity7_ _C_ trainable sentence planner _/entity7_ for a _entity8_ spoken dialogue system _/entity8_ by eliciting _entity9_ subjective human judgments _/entity9_ . In order to perform an exhaustive comparison , we also evaluate a _entity10_ _P_ hand-crafted template-based generation component _/entity10_ , two _entity11_ rule-based sentence planners _/entity11_ , and two _entity12_ baseline sentence planners _/entity12_ . We show that the _entity13_ trainable sentence planner _/entity13_ performs better than the _entity14_ rule-based systems _/entity14_ and the _entity15_ baselines _/entity15_ , and as well as the _entity16_ hand-crafted system _/entity16_ .	NONE entity10 entity7
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ _P_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ _C_ word-trigram recognition _/entity7_ requiring _entity8_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity6 entity7
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ _C_ Turkish _/entity12_ has _entity13_ _P_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity13 entity12
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ word sense _/entity3_ , a _entity4_ topic signature _/entity4_ is a set of _entity5_ words _/entity5_ that tend to co-occur with it . _entity6_ _P_ Topic signatures _/entity6_ can be useful in a number of _entity7_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ _C_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ topic signatures _/entity15_ on a _entity16_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ standard WSD datasets _/entity18_ , with promising results .	NONE entity6 entity8
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ _P_ disambiguated morphological analysis _/entity11_ and _entity12_ _C_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity11 entity12
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ _C_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ _P_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity15 entity13
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ _C_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ _P_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity5 entity4
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ class of languages _/entity2_ obtained by adding _entity3_ reduplication _/entity3_ to _entity4_ context-free languages _/entity4_ . The _entity5_ model _/entity5_ is a _entity6_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ _C_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ _P_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ model _/entity12_ appears capable of accommodating the sort of _entity13_ reduplications _/entity13_ that have been observed to occur in _entity14_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity10 entity9
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ rules _/entity6_ between _entity7_ _P_ sentences _/entity7_ , within _entity8_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ _C_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity7 entity10
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ system combination _/entity3_ for _entity4_ unsupervised WSD _/entity4_ . We investigate several _entity5_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ _P_ SemCor _/entity10_ and _entity11_ _C_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity10 entity11
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ _P_ accuracy _/entity16_ of _entity17_ _C_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity16 entity17
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ _C_ wordspotter _/entity11_ for _entity12_ _P_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity12 entity11
This paper proposes a _entity1_ Hidden Markov Model ( HMM ) _/entity1_ and an _entity2_ HMM-based chunk tagger _/entity2_ , from which a _entity3_ named entity ( NE ) recognition ( NER ) system _/entity3_ is built to recognize and classify _entity4_ names _/entity4_ , _entity5_ times and numerical quantities _/entity5_ . Through the _entity6_ HMM _/entity6_ , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the _entity7_ words _/entity7_ , such as _entity8_ capitalization _/entity8_ and digitalization ; 2 ) _entity9_ internal semantic feature _/entity9_ of important triggers ; 3 ) _entity10_ internal gazetteer feature _/entity10_ ; 4 ) _entity11_ _P_ external macro context feature _/entity11_ . In this way , the _entity12_ NER problem _/entity12_ can be resolved effectively . Evaluation of our _entity13_ system _/entity13_ on _entity14_ _C_ MUC-6 and MUC-7 English NE tasks _/entity14_ achieves _entity15_ F-measures _/entity15_ of 96.6 % and 94.1 % respectively . It shows that the performance is significantly better than reported by any other _entity16_ machine-learning system _/entity16_ . Moreover , the _entity17_ performance _/entity17_ is even consistently better than those based on _entity18_ handcrafted rules _/entity18_ .	NONE entity11 entity14
We present results on _entity1_ addressee identification _/entity1_ in _entity2_ four-participants face-to-face meetings _/entity2_ using _entity3_ Bayesian Network _/entity3_ and _entity4_ Naive Bayes classifiers _/entity4_ . First , we investigate how well the _entity5_ addressee _/entity5_ of a _entity6_ _C_ dialogue act _/entity6_ can be predicted based on _entity7_ gaze _/entity7_ , _entity8_ _P_ utterance _/entity8_ and _entity9_ conversational context features _/entity9_ . Then , we explore whether information about _entity10_ meeting context _/entity10_ can aid _entity11_ classifiers _/entity11_ ' _entity12_ performances _/entity12_ . Both _entity13_ classifiers _/entity13_ perform the best when _entity14_ conversational context _/entity14_ and _entity15_ utterance features _/entity15_ are combined with _entity16_ speaker 's gaze information _/entity16_ . The _entity17_ classifiers _/entity17_ show little _entity18_ gain _/entity18_ from information about _entity19_ meeting context _/entity19_ .	NONE entity8 entity6
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ classification-based representation techniques _/entity7_ can be applied to _entity8_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ _C_ semantic interpretation _/entity13_ was first explored in the _entity14_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ _P_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	NONE entity15 entity13
This paper proposes an approach to _entity1_ full parsing _/entity1_ suitable for _entity2_ Information Extraction _/entity2_ from _entity3_ texts _/entity3_ . Sequences of cascades of _entity4_ rules _/entity4_ deterministically analyze the _entity5_ text _/entity5_ , building _entity6_ _P_ unambiguous structures _/entity6_ . Initially basic _entity7_ chunks _/entity7_ are analyzed ; then _entity8_ _C_ argumental relations _/entity8_ are recognized ; finally _entity9_ modifier attachment _/entity9_ is performed and the _entity10_ global parse tree _/entity10_ is built . The approach was proven to work for three _entity11_ languages _/entity11_ and different _entity12_ domains _/entity12_ . It was implemented in the _entity13_ IE module _/entity13_ of _entity14_ FACILE , a EU project for multilingual text classification and IE _/entity14_ .	NONE entity6 entity8
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ _C_ sense-tagged training data _/entity4_ from _entity5_ _P_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ nouns _/entity6_ in the _entity7_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ WSD programs _/entity14_ .	NONE entity5 entity4
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ _P_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ _C_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity5 entity7
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ high-quality data _/entity7_ that allow the comparison of both _entity8_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ _C_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ automatic metrics _/entity11_ used within _entity12_ _P_ MT _/entity12_ are also discussed in this regard .	NONE entity12 entity9
The applicability of many current _entity1_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ _P_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ _C_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ supervised methods _/entity13_ on 50 _entity14_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity10 entity12
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ _P_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ _C_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity8 entity10
In this paper we introduce a _entity1_ _C_ modal language LT _/entity1_ for imposing _entity2_ constraints _/entity2_ on _entity3_ _P_ trees _/entity3_ , and an extension _entity4_ LT ( LF ) _/entity4_ for imposing _entity5_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ GPSG _/entity9_ can be captured in _entity10_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	NONE entity3 entity1
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ _P_ German corpus _/entity11_ of 2.284 _entity12_ _C_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity11 entity12
Although every _entity1_ natural language system _/entity1_ needs a _entity2_ computational lexicon _/entity2_ , each system puts different amounts and types of information into its _entity3_ lexicon _/entity3_ according to its individual needs . However , some of the information needed across systems is shared or identical information . This paper presents our experience in planning and building _entity4_ _P_ COMPLEX _/entity4_ , a _entity5_ _C_ computational lexicon _/entity5_ designed to be a repository of _entity6_ shared lexical information _/entity6_ for use by _entity7_ Natural Language Processing ( NLP ) systems _/entity7_ . We have drawn primarily on explicit and implicit information from _entity8_ machine-readable dictionaries ( MRD 's ) _/entity8_ to create a _entity9_ broad coverage lexicon _/entity9_ .	NONE entity4 entity5
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ _C_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ _P_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ phrases _/entity12_ longer than three _entity13_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity7 entity5
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ _P_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ _C_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity20 entity23
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ _P_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ _C_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ dictionary-based word vectors _/entity10_ better reflect _entity11_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity5 entity6
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ _P_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ _C_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity20 entity22
Following recent developments in the _entity1_ automatic evaluation _/entity1_ of _entity2_ machine translation _/entity2_ and _entity3_ _C_ document summarization _/entity3_ , we present a similar approach , implemented in a measure called _entity4_ POURPRE _/entity4_ , for _entity5_ automatically evaluating answers to definition questions _/entity5_ . Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response . The lack of automatic methods for _entity6_ _P_ scoring system output _/entity6_ is an impediment to progress in the field , which we address with this work . Experiments with the _entity7_ TREC 2003 and TREC 2004 QA tracks _/entity7_ indicate that _entity8_ rankings _/entity8_ produced by our metric correlate highly with _entity9_ official rankings _/entity9_ , and that _entity10_ POURPRE _/entity10_ outperforms direct application of existing metrics .	NONE entity6 entity3
This paper describes a recently collected _entity1_ spoken language corpus _/entity1_ for the _entity2_ _C_ ATIS ( Air Travel Information System ) domain _/entity2_ . This data collection effort has been co-ordinated by _entity3_ MADCOW ( Multi-site ATIS Data COllection Working group ) _/entity3_ . We summarize the motivation for this effort , the goals , the implementation of a _entity4_ multi-site data collection paradigm _/entity4_ , and the accomplishments of _entity5_ _P_ MADCOW _/entity5_ in monitoring the _entity6_ collection _/entity6_ and distribution of 12,000 _entity7_ utterances _/entity7_ of _entity8_ spontaneous speech _/entity8_ from five sites for use in a _entity9_ multi-site common evaluation of speech , natural language and spoken language _/entity9_ .	NONE entity5 entity2
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ surface representation _/entity3_ of _entity4_ _C_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ _P_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity6 entity4
In this paper , we describe a search procedure for _entity1_ statistical machine translation ( MT ) _/entity1_ based on _entity2_ _C_ dynamic programming ( DP ) _/entity2_ . Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible _entity3_ word reordering _/entity3_ between _entity4_ _P_ source and target language _/entity4_ in order to achieve an efficient search algorithm . A search restriction especially useful for the translation direction from German to English is presented . The experimental tests are carried out on the _entity5_ Verbmobil task _/entity5_ ( German-English , 8000-word vocabulary ) , which is a _entity6_ limited-domain spoken-language task _/entity6_ .	NONE entity4 entity2
In this paper , we describe the research using _entity1_ machine learning techniques _/entity1_ to build a _entity2_ comma checker _/entity2_ to be integrated in a _entity3_ grammar checker _/entity3_ for _entity4_ Basque _/entity4_ . After several experiments , and trained with a little _entity5_ corpus _/entity5_ of 100,000 _entity6_ words _/entity6_ , the system guesses correctly not placing _entity7_ commas _/entity7_ with a _entity8_ precision _/entity8_ of 96 % and a _entity9_ recall _/entity9_ of 98 % . It also gets a _entity10_ precision _/entity10_ of 70 % and a _entity11_ recall _/entity11_ of 49 % in the task of placing _entity12_ _C_ commas _/entity12_ . Finally , we have shown that these results can be improved using a bigger and a more homogeneous _entity13_ corpus _/entity13_ to train , that is , a bigger _entity14_ corpus _/entity14_ written by one unique _entity15_ _P_ author _/entity15_ .	NONE entity15 entity12
This paper proposes an _entity1_ alignment adaptation approach _/entity1_ to improve _entity2_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ alignment adaptation _/entity3_ is to use _entity4_ out-of-domain corpus _/entity4_ to improve _entity5_ _C_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ _P_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ precision _/entity11_ and _entity12_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity7 entity5
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ _C_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ _P_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity7 entity4
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ _C_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ _P_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity9 entity7
This paper introduces a method for _entity1_ _C_ computational analysis of move structures _/entity1_ in _entity2_ abstracts _/entity2_ of _entity3_ _P_ research articles _/entity3_ . In our approach , _entity4_ sentences _/entity4_ in a given _entity5_ abstract _/entity5_ are analyzed and labeled with a specific _entity6_ move _/entity6_ in light of various _entity7_ rhetorical functions _/entity7_ . The method involves automatically gathering a large number of _entity8_ abstracts _/entity8_ from the _entity9_ Web _/entity9_ and building a _entity10_ language model _/entity10_ of _entity11_ abstract moves _/entity11_ . We also present a prototype _entity12_ concordancer _/entity12_ , _entity13_ CARE _/entity13_ , which exploits the _entity14_ move-tagged abstracts _/entity14_ for _entity15_ digital learning _/entity15_ . This system provides a promising approach to _entity16_ Web-based computer-assisted academic writing _/entity16_ .	NONE entity3 entity1
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ _C_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ _P_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity5 entity2
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ _C_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ _P_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity21 entity18
When people use _entity1_ natural language _/entity1_ in natural settings , they often use it ungrammatically , missing out or repeating words , breaking-off and restarting , speaking in fragments , etc.. Their _entity2_ human listeners _/entity2_ are usually able to cope with these deviations with little difficulty . If a _entity3_ computer system _/entity3_ wishes to accept _entity4_ natural language input _/entity4_ from its _entity5_ users _/entity5_ on a routine basis , it must display a similar indifference . In this paper , we outline a set of _entity6_ _P_ parsing flexibilities _/entity6_ that such a system should provide . We go , on to describe _entity7_ _C_ FlexP _/entity7_ , a _entity8_ bottom-up pattern-matching parser _/entity8_ that we have designed and implemented to provide these flexibilities for _entity9_ restricted natural language _/entity9_ input to a limited-domain computer system .	NONE entity6 entity7
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ _C_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ _P_ WSD accuracy _/entity15_ .	NONE entity15 entity12
This paper describes a method of _entity1_ interactively visualizing and directing the process _/entity1_ of _entity2_ translating a sentence _/entity2_ . The method allows a _entity3_ user _/entity3_ to explore a _entity4_ _C_ model _/entity4_ of _entity5_ syntax-based statistical machine translation ( MT ) _/entity5_ , to understand the _entity6_ model _/entity6_ 's strengths and weaknesses , and to compare it to other _entity7_ _P_ MT systems _/entity7_ . Using this _entity8_ visualization method _/entity8_ , we can find and address conceptual and practical problems in an _entity9_ MT system _/entity9_ . In our demonstration at _entity10_ ACL _/entity10_ , new _entity11_ users _/entity11_ of our tool will drive a _entity12_ syntax-based decoder _/entity12_ for themselves .	NONE entity7 entity4
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ _P_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ _C_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity5 entity8
We describe a method for identifying systematic _entity1_ patterns _/entity1_ in _entity2_ translation data _/entity2_ using _entity3_ part-of-speech tag sequences _/entity3_ . We incorporate this analysis into a _entity4_ diagnostic tool _/entity4_ intended for _entity5_ developers _/entity5_ of _entity6_ _P_ machine translation systems _/entity6_ , and demonstrate how our application can be used by _entity7_ _C_ developers _/entity7_ to explore _entity8_ patterns _/entity8_ in _entity9_ machine translation output _/entity9_ .	NONE entity6 entity7
Interpreting _entity1_ metaphors _/entity1_ is an integral and inescapable process in _entity2_ human understanding of natural language _/entity2_ . This paper discusses a _entity3_ method of analyzing metaphors _/entity3_ based on the existence of a small number of _entity4_ generalized metaphor mappings _/entity4_ . Each _entity5_ generalized metaphor _/entity5_ contains a _entity6_ recognition network _/entity6_ , a _entity7_ basic mapping _/entity7_ , additional _entity8_ _C_ transfer mappings _/entity8_ , and an _entity9_ _P_ implicit intention component _/entity9_ . It is argued that the method reduces _entity10_ metaphor interpretation _/entity10_ from a _entity11_ reconstruction _/entity11_ to a _entity12_ recognition task _/entity12_ . Implications towards automating certain aspects of _entity13_ language learning _/entity13_ are also discussed .	NONE entity9 entity8
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ _P_ CCRs _/entity3_ are _entity4_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ categories _/entity5_ in _entity6_ _C_ local trees _/entity6_ which allow the _entity7_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	NONE entity3 entity6
We present an operable definition of _entity1_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ _P_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ discourse model _/entity3_ and _entity4_ knowledge store _/entity4_ is introduced enabling the _entity5_ _C_ decomposition _/entity5_ and _entity6_ formal representation _/entity6_ of its _entity7_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity2 entity5
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ _C_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ _P_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity22 entity19
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ _P_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ _C_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity17 entity20
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ _P_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ _C_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity9 entity11
This paper proposes the _entity1_ Hierarchical Directed Acyclic Graph ( HDAG ) Kernel _/entity1_ for _entity2_ structured natural language data _/entity2_ . The _entity3_ HDAG Kernel _/entity3_ directly accepts several levels of both _entity4_ chunks _/entity4_ and their _entity5_ relations _/entity5_ , and then efficiently computes the _entity6_ weighed sum _/entity6_ of the number of common _entity7_ attribute sequences _/entity7_ of the _entity8_ HDAGs _/entity8_ . We applied the proposed method to _entity9_ question classification _/entity9_ and _entity10_ sentence alignment tasks _/entity10_ to evaluate its performance as a _entity11_ similarity measure _/entity11_ and a _entity12_ kernel function _/entity12_ . The results of the experiments demonstrate that the _entity13_ _P_ HDAG Kernel _/entity13_ is superior to other _entity14_ _C_ kernel functions _/entity14_ and _entity15_ baseline methods _/entity15_ .	COMPARE entity13 entity14
In this paper we show how two standard outputs from _entity1_ information extraction ( IE ) systems _/entity1_ - _entity2_ named entity annotations _/entity2_ and _entity3_ scenario templates _/entity3_ - can be used to enhance access to _entity4_ text collections _/entity4_ via a standard _entity5_ text browser _/entity5_ . We describe how this information is used in a _entity6_ prototype system _/entity6_ designed to support _entity7_ _C_ information workers _/entity7_ ' access to a _entity8_ pharmaceutical news archive _/entity8_ as part of their _entity9_ industry watch _/entity9_ function . We also report results of a preliminary , _entity10_ _P_ qualitative user evaluation _/entity10_ of the system , which while broadly positive indicates further work needs to be done on the _entity11_ interface _/entity11_ to make _entity12_ users _/entity12_ aware of the increased potential of _entity13_ IE-enhanced text browsers _/entity13_ .	NONE entity10 entity7
This paper describes the understanding process of the _entity1_ spatial descriptions _/entity1_ in _entity2_ Japanese _/entity2_ . In order to understand the described _entity3_ world _/entity3_ , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space . It is done by an experimental _entity4_ computer program _/entity4_ _entity5_ SPRINT _/entity5_ , which takes _entity6_ natural language texts _/entity6_ and produces a _entity7_ model _/entity7_ of the described _entity8_ world _/entity8_ . To reconstruct the _entity9_ model _/entity9_ , the authors extract the _entity10_ qualitative spatial constraints _/entity10_ from the _entity11_ text _/entity11_ , and represent them as the _entity12_ numerical constraints _/entity12_ on the _entity13_ spatial attributes _/entity13_ of the _entity14_ entities _/entity14_ . This makes it possible to express the vagueness of the _entity15_ _C_ spatial concepts _/entity15_ and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints . The interpretation reflects the _entity16_ temporary belief _/entity16_ about the _entity17_ _P_ world _/entity17_ .	NONE entity17 entity15
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ _P_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ _C_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity13 entity16
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ _P_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ _C_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity6 entity8
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ _C_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ _P_ expressions of natural languages _/entity9_ to their associated _entity10_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ TAGs _/entity14_ to be used beyond their role in _entity15_ syntax proper _/entity15_ . We discuss the application of _entity16_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity9 entity6
The principle known as _entity1_ free indexation _/entity1_ plays an important role in the determination of the _entity2_ referential properties of noun phrases _/entity2_ in the _entity3_ _C_ principle-and-parameters language framework _/entity3_ . First , by investigating the combinatorics of _entity4_ _P_ free indexation _/entity4_ , we show that the problem of enumerating all possible _entity5_ indexings _/entity5_ requires _entity6_ exponential time _/entity6_ . Secondly , we exhibit a provably optimal _entity7_ free indexation algorithm _/entity7_ .	NONE entity4 entity3
_entity1_ _P_ Metagrammatical formalisms _/entity1_ that combine _entity2_ _C_ context-free phrase structure rules _/entity2_ and _entity3_ metarules ( MPS grammars ) _/entity3_ allow concise statement of generalizations about the _entity4_ syntax _/entity4_ of _entity5_ natural languages _/entity5_ . _entity6_ Unconstrained MPS grammars _/entity6_ , unfortunately , are not computationally safe . We evaluate several proposals for constraining them , basing our assessment on _entity7_ computational tractability and explanatory adequacy _/entity7_ . We show that none of them satisfies both criteria , and suggest new directions for research on alternative _entity8_ metagrammatical formalisms _/entity8_ .	NONE entity1 entity2
We describe a three-tiered approach for evaluation of _entity1_ spoken dialogue systems _/entity1_ . The three tiers measure _entity2_ _P_ user satisfaction _/entity2_ , _entity3_ _C_ system support of mission success _/entity3_ and _entity4_ component performance _/entity4_ . We describe our use of this approach in numerous fielded _entity5_ user studies _/entity5_ conducted with the U.S. military .	NONE entity2 entity3
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ _C_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ _P_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity8 entity5
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ _C_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ _P_ multiple speakers _/entity5_ without _entity6_ explicit speaker enrollment _/entity6_ . Combining _entity7_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity5 entity3
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ _P_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ _C_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity14 entity16
_entity1_ Automatic summarization _/entity1_ and _entity2_ _C_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ _P_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity5 entity2
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ _C_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ _P_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity12 entity10
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ _P_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ _C_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity6 entity9
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ morphological and syntactic information _/entity3_ . While the _entity4_ model _/entity4_ is more complex than those which have been employed for _entity5_ unsupervised learning _/entity5_ of _entity6_ POS tags in English _/entity6_ , which use only _entity7_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ morphology _/entity9_ as well . In many _entity10_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ Bulgarian _/entity15_ , a _entity16_ _C_ Slavic language _/entity16_ with relatively _entity17_ free word order _/entity17_ and _entity18_ _P_ rich morphology _/entity18_ .	NONE entity18 entity16
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ _C_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ _P_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity6 entity4
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ _C_ strings _/entity18_ . Overall _entity19_ _P_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity19 entity18
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ system combination _/entity3_ for _entity4_ unsupervised WSD _/entity4_ . We investigate several _entity5_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ _P_ unsupervised WSD systems _/entity6_ . Our _entity7_ _C_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity6 entity7
_entity1_ Chart parsing _/entity1_ is _entity2_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ _P_ chart _/entity3_ that works outward from _entity4_ _C_ islands _/entity4_ and makes sense of as much of the _entity5_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ fragments _/entity7_ occur in the _entity8_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ islands _/entity9_ , until possibly completely missing _entity10_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity3 entity4
In order to build robust _entity1_ automatic abstracting systems _/entity1_ , there is a need for better _entity2_ _P_ training resources _/entity2_ than are currently available . In this paper , we introduce an _entity3_ _C_ annotation scheme _/entity3_ for scientific articles which can be used to build such a _entity4_ resource _/entity4_ in a consistent way . The seven categories of the _entity5_ scheme _/entity5_ are based on _entity6_ rhetorical moves _/entity6_ of _entity7_ argumentation _/entity7_ . Our experimental results show that the _entity8_ scheme _/entity8_ is stable , reproducible and intuitive to use .	NONE entity2 entity3
The applicability of many current _entity1_ _P_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ _C_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ supervised methods _/entity13_ on 50 _entity14_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity1 entity2
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ _P_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ _C_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity7 entity9
Following recent developments in the _entity1_ automatic evaluation _/entity1_ of _entity2_ _P_ machine translation _/entity2_ and _entity3_ _C_ document summarization _/entity3_ , we present a similar approach , implemented in a measure called _entity4_ POURPRE _/entity4_ , for _entity5_ automatically evaluating answers to definition questions _/entity5_ . Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response . The lack of automatic methods for _entity6_ scoring system output _/entity6_ is an impediment to progress in the field , which we address with this work . Experiments with the _entity7_ TREC 2003 and TREC 2004 QA tracks _/entity7_ indicate that _entity8_ rankings _/entity8_ produced by our metric correlate highly with _entity9_ official rankings _/entity9_ , and that _entity10_ POURPRE _/entity10_ outperforms direct application of existing metrics .	NONE entity2 entity3
This paper describes the status of the _entity1_ MIT ATIS system _/entity1_ as of February 1992 , focusing especially on the changes made to the _entity2_ SUMMIT recognizer _/entity2_ . These include _entity3_ context-dependent phonetic modelling _/entity3_ , the use of a _entity4_ bigram language model _/entity4_ in conjunction with a _entity5_ probabilistic LR parser _/entity5_ , and refinements made to the _entity6_ lexicon _/entity6_ . Together with the use of a larger _entity7_ _P_ training set _/entity7_ , these modifications combined to reduce the _entity8_ speech recognition word and sentence error rates _/entity8_ by a factor of 2.5 and 1.6 , respectively , on the _entity9_ October '91 test set _/entity9_ . The weighted error for the entire _entity10_ _C_ spoken language system _/entity10_ on the same _entity11_ test set _/entity11_ is 49.3 % . Similar results were also obtained on the _entity12_ February '92 benchmark evaluation _/entity12_ .	NONE entity7 entity10
This paper proposes a novel method of building _entity1_ _P_ polarity-tagged corpus _/entity1_ from _entity2_ _C_ HTML documents _/entity2_ . The characteristics of this method is that it is fully automatic and can be applied to arbitrary _entity3_ HTML documents _/entity3_ . The idea behind our method is to utilize certain _entity4_ layout structures _/entity4_ and _entity5_ linguistic pattern _/entity5_ . By using them , we can automatically extract such _entity6_ sentences _/entity6_ that express opinion . In our experiment , the method could construct a _entity7_ corpus _/entity7_ consisting of 126,610 _entity8_ sentences _/entity8_ .	PART_WHOLE entity1 entity2
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ _P_ candidates _/entity13_ for _entity14_ _C_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	USAGE entity13 entity14
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ _P_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ _C_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity15 entity18
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ _C_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ _P_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity13 entity12
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ _P_ statistical ranking _/entity6_ than a previous approach to _entity7_ statistical generation _/entity7_ . An efficient _entity8_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ _C_ lattice-based approach _/entity9_ .	NONE entity6 entity9
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ _C_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ _P_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ phrase translations _/entity9_ in our _entity10_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity6 entity4
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ _P_ word strings _/entity9_ , _entity10_ _C_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity9 entity10
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ _C_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ _P_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity31 entity29
We address appropriate _entity1_ _C_ user modeling _/entity1_ in order to generate _entity2_ _P_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity2 entity1
A method for _entity1_ _C_ error correction _/entity1_ of _entity2_ _P_ ill-formed input _/entity2_ is described that acquires _entity3_ dialogue patterns _/entity3_ in typical usage and uses these _entity4_ patterns _/entity4_ to predict new inputs . _entity5_ Error correction _/entity5_ is done by strongly biasing _entity6_ parsing _/entity6_ toward expected _entity7_ meanings _/entity7_ unless clear evidence from the input shows the current _entity8_ sentence _/entity8_ is not expected . A _entity9_ dialogue acquisition and tracking algorithm _/entity9_ is presented along with a description of its _entity10_ implementation _/entity10_ in a _entity11_ voice interactive system _/entity11_ . A series of tests are described that show the power of the _entity12_ error correction methodology _/entity12_ when _entity13_ stereotypic dialogue _/entity13_ occurs .	NONE entity2 entity1
This paper proposes that _entity1_ sentence analysis _/entity1_ should be treated as _entity2_ defeasible reasoning _/entity2_ , and presents such a treatment for _entity3_ Japanese sentence analyses _/entity3_ using an _entity4_ argumentation system _/entity4_ by Konolige , which is a _entity5_ formalization _/entity5_ of _entity6_ _P_ defeasible reasoning _/entity6_ , that includes _entity7_ _C_ arguments _/entity7_ and _entity8_ defeat rules _/entity8_ that capture _entity9_ defeasibility _/entity9_ .	NONE entity6 entity7
A proper treatment of _entity1_ syntax _/entity1_ and _entity2_ semantics _/entity2_ in _entity3_ machine translation _/entity3_ is introduced and discussed from the empirical viewpoint . For _entity4_ English-Japanese machine translation _/entity4_ , the _entity5_ syntax directed approach _/entity5_ is effective where the _entity6_ _C_ Heuristic Parsing Model ( HPM ) _/entity6_ and the _entity7_ Syntactic Role System _/entity7_ play important roles . For _entity8_ Japanese-English translation _/entity8_ , the _entity9_ _P_ semantics directed approach _/entity9_ is powerful where the _entity10_ Conceptual Dependency Diagram ( CDD ) _/entity10_ and the _entity11_ Augmented Case Marker System _/entity11_ ( which is a kind of _entity12_ Semantic Role System _/entity12_ ) play essential roles . Some examples of the difference between _entity13_ Japanese sentence structure _/entity13_ and _entity14_ English sentence structure _/entity14_ , which is vital to _entity15_ machine translation _/entity15_ are also discussed together with various interesting _entity16_ ambiguities _/entity16_ .	NONE entity9 entity6
In this paper we describe and evaluate a _entity1_ Question Answering system _/entity1_ that goes beyond answering factoid questions . We focus on _entity2_ FAQ-like questions and answers _/entity2_ , and build our system around a _entity3_ _C_ noisy-channel architecture _/entity3_ which exploits both a _entity4_ _P_ language model _/entity4_ for _entity5_ answers _/entity5_ and a _entity6_ transformation model _/entity6_ for _entity7_ answer/question terms _/entity7_ , trained on a _entity8_ corpus _/entity8_ of 1 million _entity9_ question/answer pairs _/entity9_ collected from the Web .	NONE entity4 entity3
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ _P_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ _C_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity15 entity18
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ _P_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ _C_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity3 entity5
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ _C_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ _P_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity18 entity16
This paper describes an implemented program that takes a _entity1_ tagged text corpus _/entity1_ and generates a partial list of the _entity2_ subcategorization frames _/entity2_ in which each _entity3_ verb _/entity3_ occurs . The completeness of the output list increases monotonically with the total _entity4_ occurrences _/entity4_ of each _entity5_ verb _/entity5_ in the _entity6_ training corpus _/entity6_ . _entity7_ False positive rates _/entity7_ are one to three percent . Five _entity8_ subcategorization frames _/entity8_ are currently detected and we foresee no impediment to detecting many more . Ultimately , we expect to provide a large _entity9_ subcategorization dictionary _/entity9_ to the _entity10_ NLP community _/entity10_ and to train _entity11_ _P_ dictionaries _/entity11_ for specific _entity12_ _C_ corpora _/entity12_ .	NONE entity11 entity12
_entity1_ _C_ Coedition _/entity1_ of a _entity2_ _P_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity2 entity1
We consider the problem of computing the _entity1_ _P_ Kullback-Leibler distance _/entity1_ , also called the _entity2_ relative entropy _/entity2_ , between a _entity3_ probabilistic context-free grammar _/entity3_ and a _entity4_ _C_ probabilistic finite automaton _/entity4_ . We show that there is a _entity5_ closed-form ( analytical ) solution _/entity5_ for one part of the _entity6_ Kullback-Leibler distance _/entity6_ , viz . the _entity7_ cross-entropy _/entity7_ . We discuss several applications of the result to the problem of _entity8_ distributional approximation _/entity8_ of _entity9_ probabilistic context-free grammars _/entity9_ by means of _entity10_ probabilistic finite automata _/entity10_ .	NONE entity1 entity4
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ _C_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ _P_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity21 entity18
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ _P_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ _C_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity8 entity11
_entity1_ Topical blog post retrieval _/entity1_ is the task of ranking _entity2_ blog posts _/entity2_ with respect to their _entity3_ relevance _/entity3_ for a given _entity4_ topic _/entity4_ . To improve _entity5_ topical blog post retrieval _/entity5_ we incorporate _entity6_ textual credibility indicators _/entity6_ in the _entity7_ retrieval process _/entity7_ . We consider two groups of _entity8_ _C_ indicators _/entity8_ : post level ( determined using information about individual _entity9_ _P_ blog posts _/entity9_ only ) and blog level ( determined using information from the underlying _entity10_ blogs _/entity10_ ) . We describe how to estimate these _entity11_ indicators _/entity11_ and how to integrate them into a _entity12_ retrieval approach _/entity12_ based on _entity13_ language models _/entity13_ . Experiments on the _entity14_ TREC Blog track test set _/entity14_ show that both groups of _entity15_ credibility indicators _/entity15_ significantly improve _entity16_ retrieval effectiveness _/entity16_ ; the best performance is achieved when combining them .	NONE entity9 entity8
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ _C_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ _P_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity15 entity13
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ _P_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ _C_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity3 entity5
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ _P_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ _C_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity13 entity15
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ electronic discussions _/entity5_ that influence the _entity6_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ _P_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ _C_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity9 entity11
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ _C_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ _P_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity24 entity21
This paper proposes to use a _entity1_ _C_ convolution kernel _/entity1_ over _entity2_ _P_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity2 entity1
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ _C_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ _P_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity3 entity2
_entity1_ Taiwan Child Language Corpus _/entity1_ contains _entity2_ scripts _/entity2_ transcribed from about 330 hours of _entity3_ recordings _/entity3_ of fourteen young children from _entity4_ _P_ Southern Min Chinese _/entity4_ speaking families in Taiwan . The format of the _entity5_ _C_ corpus _/entity5_ adopts the _entity6_ Child Language Data Exchange System ( CHILDES ) _/entity6_ . The size of the _entity7_ corpus _/entity7_ is about 1.6 million _entity8_ words _/entity8_ . In this paper , we describe _entity9_ data collection _/entity9_ , _entity10_ transcription _/entity10_ , _entity11_ word segmentation _/entity11_ , and _entity12_ part-of-speech annotation _/entity12_ of this _entity13_ corpus _/entity13_ . Applications of the _entity14_ corpus _/entity14_ are also discussed .	NONE entity4 entity5
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ _P_ newspaper stories _/entity3_ and other _entity4_ _C_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity3 entity4
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ events _/entity2_ are defined as _entity3_ event terms _/entity3_ and _entity4_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ _C_ contents _/entity5_ by frequency of _entity6_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ _P_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ documents _/entity9_ . Experimental results are encouraging .	NONE entity7 entity5
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ context-free parsing _/entity2_ is presented . The _entity3_ _C_ parser _/entity3_ computes a compact _entity4_ _P_ parse forest representation _/entity4_ of the complete set of possible _entity5_ analyses for large treebank grammars _/entity5_ and long _entity6_ input sentences _/entity6_ . The _entity7_ parser _/entity7_ uses _entity8_ bit-vector operations _/entity8_ to parallelise the _entity9_ basic parsing operations _/entity9_ . The _entity10_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	NONE entity4 entity3
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ _C_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ _P_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity7 entity5
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ _C_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ _P_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity24 entity22
_entity1_ Graph unification _/entity1_ remains the most expensive part of _entity2_ unification-based grammar parsing _/entity2_ . We focus on one speed-up element in the design of _entity3_ _P_ unification algorithms _/entity3_ : avoidance of _entity4_ copying _/entity4_ of _entity5_ _C_ unmodified subgraphs _/entity5_ . We propose a method of attaining such a design through a method of _entity6_ structure-sharing _/entity6_ which avoids _entity7_ log ( d ) overheads _/entity7_ often associated with _entity8_ structure-sharing of graphs _/entity8_ without any use of costly _entity9_ dependency pointers _/entity9_ . The proposed scheme eliminates _entity10_ redundant copying _/entity10_ while maintaining the _entity11_ quasi-destructive scheme 's ability _/entity11_ to avoid _entity12_ over copying _/entity12_ and _entity13_ early copying _/entity13_ combined with its ability to handle _entity14_ cyclic structures _/entity14_ without algorithmic additions .	NONE entity3 entity5
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ _P_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ _C_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity7 entity9
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ statistical approaches _/entity6_ , which resolve _entity7_ _P_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ _C_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity7 entity8
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ _C_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ _P_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity12 entity11
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ _C_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ _P_ formal intersections of FSAs _/entity21_ .	NONE entity21 entity19
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ _C_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ _P_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity25 entity22
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ HPSG _/entity2_ into _entity3_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ HPSG _/entity5_ has a more elaborated _entity6_ _P_ principle-based theory _/entity6_ of possible _entity7_ _C_ phrase structures _/entity7_ , _entity8_ TAG _/entity8_ provides the means to represent _entity9_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity6 entity7
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ _C_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ _P_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity9 entity6
In this paper we present a _entity1_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ _P_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ _C_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ states _/entity9_ which handles some additional cases of multiple _entity10_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity3 entity4
In this paper we present a _entity1_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ _P_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ states _/entity9_ which handles some additional cases of multiple _entity10_ _C_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity7 entity10
A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature , practical applications rather than `` toy '' implementations . Evaluation is also crucial to assessing competing claims and identifying promising technical approaches . While work in _entity1_ speech recognition ( SR ) _/entity1_ has a history of evaluation methodologies that permit comparison among various systems , until recently no methodology existed for either developers of _entity2_ natural language ( NL ) interfaces _/entity2_ or researchers in _entity3_ speech understanding ( SU ) _/entity3_ to evaluate and compare the systems they developed . Recently considerable progress has been made by a number of groups involved in the _entity4_ DARPA Spoken Language Systems ( SLS ) program _/entity4_ to agree on a methodology for comparative evaluation of _entity5_ SLS systems _/entity5_ , and that methodology has been put into practice several times in comparative tests of several _entity6_ SLS systems _/entity6_ . These evaluations are probably the only _entity7_ _C_ NL evaluations _/entity7_ other than the series of _entity8_ Message Understanding Conferences _/entity8_ ( Sundheim , 1989 ; Sundheim , 1991 ) to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems ( Palmer et al. , 1989 ; Neal et al. , 1991 ) . This paper describes a practical _entity9_ _P_ `` black-box '' methodology _/entity9_ for automatic evaluation of _entity10_ question-answering NL systems _/entity10_ . While each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and it can be used with either _entity11_ speech or text input _/entity11_ . The particular characteristics of the approach are described in the following section : subsequent sections present its implementation in the _entity12_ DARPA SLS community _/entity12_ , and some problems and directions for future development .	NONE entity9 entity7
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ _P_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ _C_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity9 entity12
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ _C_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ performance _/entity5_ of using _entity6_ ASR output _/entity6_ as opposed to _entity7_ _P_ human transcription _/entity7_ . Examination of the effect of _entity8_ features _/entity8_ shows that _entity9_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity7 entity4
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ _C_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ _P_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity24 entity23
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ _C_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ _P_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity13 entity11
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ electronic discussions _/entity5_ that influence the _entity6_ _C_ clustering process _/entity6_ , and offer a _entity7_ _P_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity7 entity6
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ _P_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ _C_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity9 entity12
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ _C_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ _P_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity5 entity3
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ _C_ besides _/entity6_ . These _entity7_ _P_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity7 entity6
The _entity1_ interlingual approach to MT _/entity1_ has been repeatedly advocated by researchers originally interested in _entity2_ natural language understanding _/entity2_ who take _entity3_ machine translation _/entity3_ to be one possible application . However , not only the _entity4_ ambiguity _/entity4_ but also the vagueness which every _entity5_ natural language _/entity5_ inevitably has leads this approach into essential difficulties . In contrast , our project , the _entity6_ Mu-project _/entity6_ , adopts the _entity7_ transfer approach _/entity7_ as the basic framework of _entity8_ MT _/entity8_ . This paper describes the detailed construction of the _entity9_ transfer phase _/entity9_ of our system from _entity10_ Japanese _/entity10_ to _entity11_ English _/entity11_ , and gives some examples of problems which seem difficult to treat in the _entity12_ interlingual approach _/entity12_ . The basic design principles of the _entity13_ transfer phase _/entity13_ of our system have already been mentioned in ( 1 ) ( 2 ) . Some of the principles which are relevant to the topic of this paper are : ( a ) _entity14_ Multiple Layer of Grammars _/entity14_ ( b ) _entity15_ Multiple Layer Presentation _/entity15_ ( c ) _entity16_ _C_ Lexicon Driven Processing _/entity16_ ( d ) _entity17_ _P_ Form-Oriented Dictionary Description _/entity17_ . This paper also shows how these principles are realized in the current system .	NONE entity17 entity16
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ _C_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ _P_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity9 entity6
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ _P_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ _C_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity13 entity15
_entity1_ Combination methods _/entity1_ are an effective way of improving _entity2_ system performance _/entity2_ . This paper examines the benefits of _entity3_ _P_ system combination _/entity3_ for _entity4_ unsupervised WSD _/entity4_ . We investigate several _entity5_ _C_ voting- and arbiter-based combination strategies _/entity5_ over a diverse pool of _entity6_ unsupervised WSD systems _/entity6_ . Our _entity7_ combination methods _/entity7_ rely on _entity8_ predominant senses _/entity8_ which are derived automatically from _entity9_ raw text _/entity9_ . Experiments using the _entity10_ SemCor _/entity10_ and _entity11_ Senseval-3 data sets _/entity11_ demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .	NONE entity3 entity5
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ _C_ speakers _/entity7_ . In addition , combination of the _entity8_ _P_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity8 entity7
The paper provides an overview of the research conducted at _entity1_ _C_ LIMSI _/entity1_ in the field of _entity2_ _P_ speech processing _/entity2_ , but also in the related areas of _entity3_ Human-Machine Communication _/entity3_ , including _entity4_ Natural Language Processing _/entity4_ , _entity5_ Non Verbal and Multimodal Communication _/entity5_ . Also presented are the commercial applications of some of the research projects . When applicable , the discussion is placed in the framework of international collaborations .	NONE entity2 entity1
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ _C_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ _P_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity21 entity18
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ _P_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ _C_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity3 entity5
We discuss _entity1_ maximum a posteriori estimation _/entity1_ of _entity2_ _C_ continuous density hidden Markov models ( CDHMM ) _/entity2_ . The classical _entity3_ MLE reestimation algorithms _/entity3_ , namely the _entity4_ forward-backward algorithm _/entity4_ and the _entity5_ _P_ segmental k-means algorithm _/entity5_ , are expanded and _entity6_ reestimation formulas _/entity6_ are given for _entity7_ HMM with Gaussian mixture observation densities _/entity7_ . Because of its adaptive nature , _entity8_ Bayesian learning _/entity8_ serves as a unified approach for the following four _entity9_ speech recognition _/entity9_ applications , namely _entity10_ parameter smoothing _/entity10_ , _entity11_ speaker adaptation _/entity11_ , _entity12_ speaker group modeling _/entity12_ and _entity13_ corrective training _/entity13_ . New experimental results on all four applications are provided to show the effectiveness of the _entity14_ MAP estimation approach _/entity14_ .	NONE entity5 entity2
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ _P_ classifier _/entity13_ for each _entity14_ _C_ noun _/entity14_ is limited to the _entity15_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	MODEL-FEATURE entity13 entity14
In this paper , we introduce a _entity1_ generative probabilistic optical character recognition ( OCR ) model _/entity1_ that describes an end-to-end process in the _entity2_ noisy channel framework _/entity2_ , progressing from generation of _entity3_ true text _/entity3_ through its transformation into the _entity4_ noisy output _/entity4_ of an _entity5_ OCR system _/entity5_ . The _entity6_ model _/entity6_ is designed for use in _entity7_ _C_ error correction _/entity7_ , with a focus on _entity8_ _P_ post-processing _/entity8_ the _entity9_ output _/entity9_ of black-box _entity10_ OCR systems _/entity10_ in order to make it more useful for _entity11_ NLP tasks _/entity11_ . We present an implementation of the _entity12_ model _/entity12_ based on _entity13_ finite-state models _/entity13_ , demonstrate the _entity14_ model _/entity14_ 's ability to significantly reduce _entity15_ character and word error rate _/entity15_ , and provide evaluation results involving _entity16_ automatic extraction _/entity16_ of _entity17_ translation lexicons _/entity17_ from _entity18_ printed text _/entity18_ .	NONE entity8 entity7
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ _P_ retrieval performance _/entity4_ of a _entity5_ _C_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity4 entity5
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ _P_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ _C_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity5 entity7
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ _C_ sequential models _/entity11_ , in this paper we focus on _entity12_ _P_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity12 entity11
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ ungrammatical input _/entity8_ . While _entity9_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ _C_ passives _/entity11_ , _entity12_ relative clauses _/entity12_ and _entity13_ _P_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ Plume _/entity16_ to handle _entity17_ passives _/entity17_ , _entity18_ relative clauses _/entity18_ , and _entity19_ interrogatives _/entity19_ in a general manner .	NONE entity13 entity11
Current _entity1_ _C_ natural language interfaces _/entity1_ have concentrated largely on determining the literal _entity2_ meaning _/entity2_ of _entity3_ _P_ input _/entity3_ from their _entity4_ users _/entity4_ . While such _entity5_ decoding _/entity5_ is an essential underpinning , much recent work suggests that _entity6_ natural language interfaces _/entity6_ will never appear cooperative or graceful unless they also incorporate numerous _entity7_ non-literal aspects of communication _/entity7_ , such as robust _entity8_ communication procedures _/entity8_ . This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these _entity9_ non-literal aspects of communication _/entity9_ ; that the new technology of powerful _entity10_ personal computers _/entity10_ with integral _entity11_ graphics displays _/entity11_ offers techniques superior to those of humans for these aspects , while still satisfying _entity12_ human communication needs _/entity12_ . The paper proposes _entity13_ interfaces _/entity13_ based on a judicious mixture of these techniques and the still valuable methods of more traditional _entity14_ natural language interfaces _/entity14_ .	NONE entity3 entity1
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ _C_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ _P_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity11 entity9
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ _P_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ _C_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity9 entity12
A research program is described in which a particular _entity1_ representational format for meaning _/entity1_ is tested as broadly as possible . In this format , developed by the LNR research group at The University of California at San Diego , _entity2_ verbs _/entity2_ are represented as interconnected sets of _entity3_ subpredicates _/entity3_ . These _entity4_ _C_ subpredicates _/entity4_ may be thought of as the almost inevitable _entity5_ inferences _/entity5_ that a _entity6_ _P_ listener _/entity6_ makes when a _entity7_ verb _/entity7_ is used in a _entity8_ sentence _/entity8_ . They confer a _entity9_ meaning structure _/entity9_ on the _entity10_ sentence _/entity10_ in which the _entity11_ verb _/entity11_ is used .	NONE entity6 entity4
Multimodal interfaces require effective _entity1_ parsing _/entity1_ and understanding of _entity2_ utterances _/entity2_ whose content is distributed across multiple input modes . Johnston 1998 presents an approach in which strategies for _entity3_ _C_ multimodal integration _/entity3_ are stated declaratively using a _entity4_ unification-based grammar _/entity4_ that is used by a _entity5_ multidimensional chart parser _/entity5_ to compose inputs . This approach is highly expressive and supports a broad class of _entity6_ _P_ interfaces _/entity6_ , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input . In this paper , we present an alternative approach in which _entity7_ multimodal parsing and understanding _/entity7_ are achieved using a _entity8_ weighted finite-state device _/entity8_ which takes _entity9_ speech and gesture streams _/entity9_ as inputs and outputs their joint interpretation . This approach is significantly more efficient , enables tight-coupling of multimodal understanding with _entity10_ speech recognition _/entity10_ , and provides a general probabilistic framework for _entity11_ multimodal ambiguity resolution _/entity11_ .	NONE entity6 entity3
This paper presents an _entity1_ _P_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ _C_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity1 entity4
This paper presents an analysis of _entity1_ temporal anaphora _/entity1_ in _entity2_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ _P_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ _C_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity6 entity8
A _entity1_ _C_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ _P_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity4 entity1
In our current research into the design of _entity1_ _C_ cognitively well-motivated interfaces _/entity1_ relying primarily on the _entity2_ display of graphical information _/entity2_ , we have observed that _entity3_ _P_ graphical information _/entity3_ alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations . This can occur due to too much _entity4_ information _/entity4_ being requested , too little , _entity5_ information _/entity5_ of the wrong kind , etc . To solve this problem , we are working towards the integration of _entity6_ natural language generation _/entity6_ to augment the _entity7_ interaction _/entity7_	NONE entity3 entity1
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ _C_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ _P_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity18 entity15
In the _entity1_ _P_ Chinese language _/entity1_ , a _entity2_ _C_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity1 entity2
The applicability of many current _entity1_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ _P_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ _C_ supervised methods _/entity13_ on 50 _entity14_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity12 entity13
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ _C_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ _P_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity8 entity7
In this paper , we present a _entity1_ fully automated extraction system _/entity1_ , named _entity2_ IntEx _/entity2_ , to identify _entity3_ gene and protein interactions _/entity3_ in _entity4_ biomedical text _/entity4_ . Our approach is based on first splitting _entity5_ complex sentences _/entity5_ into _entity6_ simple clausal structures _/entity6_ made up of _entity7_ syntactic roles _/entity7_ . Then , tagging _entity8_ biological entities _/entity8_ with the help of _entity9_ biomedical and linguistic ontologies _/entity9_ . Finally , extracting _entity10_ complete interactions _/entity10_ by analyzing the matching contents of _entity11_ syntactic roles _/entity11_ and their linguistically significant combinations . Our _entity12_ extraction system _/entity12_ handles _entity13_ complex sentences _/entity13_ and extracts _entity14_ _C_ multiple and nested interactions _/entity14_ specified in a _entity15_ sentence _/entity15_ . Experimental evaluations with two other state of the art _entity16_ extraction systems _/entity16_ indicate that the _entity17_ _P_ IntEx system _/entity17_ achieves better _entity18_ performance _/entity18_ without the labor intensive _entity19_ pattern engineering requirement _/entity19_ .	NONE entity17 entity14
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ _P_ mechanical translation _/entity46_ , _entity47_ _C_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity46 entity47
This paper describes a new , _entity1_ large scale discourse-level annotation _/entity1_ project - the _entity2_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ discourse structure _/entity3_ that is based on identifying _entity4_ discourse connectives _/entity4_ and their _entity5_ arguments _/entity5_ . The _entity6_ PDTB _/entity6_ is being built directly on top of the _entity7_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ _C_ inter-annotator agreement _/entity11_ - both the _entity12_ _P_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity12 entity11
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ _P_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ _C_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity3 entity5
This paper describes _entity1_ FERRET _/entity1_ , an _entity2_ interactive question-answering ( Q/A ) system _/entity2_ designed to address the challenges of integrating _entity3_ automatic Q/A _/entity3_ applications into real-world environments . _entity4_ _C_ FERRET _/entity4_ utilizes a novel approach to _entity5_ Q/A _/entity5_ known as _entity6_ predictive questioning _/entity6_ which attempts to identify the _entity7_ _P_ questions _/entity7_ ( and _entity8_ answers _/entity8_ ) that _entity9_ users _/entity9_ need by analyzing how a _entity10_ user _/entity10_ interacts with a system while gathering information related to a particular scenario .	NONE entity7 entity4
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ electronic discussions _/entity5_ that influence the _entity6_ _P_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ _C_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity6 entity9
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ _C_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ _P_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity16 entity13
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ _P_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ _C_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	RESULT entity13 entity14
_entity1_ _C_ Chart parsing _/entity1_ is _entity2_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ _P_ chart _/entity3_ that works outward from _entity4_ islands _/entity4_ and makes sense of as much of the _entity5_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ fragments _/entity7_ occur in the _entity8_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ islands _/entity9_ , until possibly completely missing _entity10_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity3 entity1
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ _C_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ _P_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity10 entity7
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ _P_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ _C_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity10 entity12
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ _C_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ _P_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity4 entity3
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ _C_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ _P_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity12 entity9
This paper describes the status of the _entity1_ MIT ATIS system _/entity1_ as of February 1992 , focusing especially on the changes made to the _entity2_ SUMMIT recognizer _/entity2_ . These include _entity3_ context-dependent phonetic modelling _/entity3_ , the use of a _entity4_ bigram language model _/entity4_ in conjunction with a _entity5_ probabilistic LR parser _/entity5_ , and refinements made to the _entity6_ lexicon _/entity6_ . Together with the use of a larger _entity7_ training set _/entity7_ , these modifications combined to reduce the _entity8_ speech recognition word and sentence error rates _/entity8_ by a factor of 2.5 and 1.6 , respectively , on the _entity9_ _C_ October '91 test set _/entity9_ . The weighted error for the entire _entity10_ spoken language system _/entity10_ on the same _entity11_ _P_ test set _/entity11_ is 49.3 % . Similar results were also obtained on the _entity12_ February '92 benchmark evaluation _/entity12_ .	NONE entity11 entity9
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ _P_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ _C_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity3 entity5
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ _C_ sentence plans _/entity12_ for a given _entity13_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ _P_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ plan _/entity16_ . The _entity17_ SPR _/entity17_ uses _entity18_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ top human-ranked sentence plan _/entity22_ .	NONE entity15 entity12
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ _C_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ _P_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity15 entity13
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ word sense _/entity3_ , a _entity4_ topic signature _/entity4_ is a set of _entity5_ words _/entity5_ that tend to co-occur with it . _entity6_ Topic signatures _/entity6_ can be useful in a number of _entity7_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ _P_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ _C_ topic signatures _/entity15_ on a _entity16_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ standard WSD datasets _/entity18_ , with promising results .	NONE entity14 entity15
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ _P_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ _C_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity4 entity6
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ _C_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ _P_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity5 entity3
The paper provides an overview of the research conducted at _entity1_ LIMSI _/entity1_ in the field of _entity2_ _C_ speech processing _/entity2_ , but also in the related areas of _entity3_ Human-Machine Communication _/entity3_ , including _entity4_ _P_ Natural Language Processing _/entity4_ , _entity5_ Non Verbal and Multimodal Communication _/entity5_ . Also presented are the commercial applications of some of the research projects . When applicable , the discussion is placed in the framework of international collaborations .	NONE entity4 entity2
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ _P_ logic _/entity6_ . With the aid of a _entity7_ _C_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity6 entity7
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ _P_ dialogue _/entity9_ that would make a _entity10_ _C_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity9 entity10
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ _P_ modifiers _/entity11_ around the _entity12_ _C_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity11 entity12
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ _P_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ _C_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity13 entity16
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ _P_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ _C_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity13 entity16
In this paper we introduce a _entity1_ modal language LT _/entity1_ for imposing _entity2_ constraints _/entity2_ on _entity3_ trees _/entity3_ , and an extension _entity4_ LT ( LF ) _/entity4_ for imposing _entity5_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ GPSG _/entity9_ can be captured in _entity10_ _P_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ _C_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	NONE entity10 entity12
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ _P_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ _C_ spoken language system domains _/entity17_ .	NONE entity16 entity17
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ _C_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ _P_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity5 entity2
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ _P_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ _C_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity9 entity11
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ _C_ parser _/entity3_ for _entity4_ _P_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ word error rates _/entity11_ competitive with the _entity12_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity4 entity3
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ _P_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ _C_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity4 entity6
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ events _/entity2_ are defined as _entity3_ event terms _/entity3_ and _entity4_ _C_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ _P_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ documents _/entity9_ . Experimental results are encouraging .	NONE entity6 entity4
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ _P_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ _C_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity11 entity12
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ _P_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ _C_ substitutions _/entity12_ for these elements .	MODEL-FEATURE entity10 entity12
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ _P_ grammatically correct input _/entity18_ . Several _entity19_ _C_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity18 entity19
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ _C_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ _P_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity8 entity5
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ _C_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ _P_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity47 entity44
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ _C_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ _P_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity17 entity14
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ _P_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ _C_ sublanguage _/entity3_ , is proposed for identifying _entity4_ unknown words _/entity4_ , especially _entity5_ personal names _/entity5_ , in _entity6_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ adaptive dynamic word formation _/entity8_ , _entity9_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ WI systems _/entity12_ without the _entity13_ name identification _/entity13_ capability .	NONE entity2 entity3
The _entity1_ transfer phase _/entity1_ in _entity2_ machine translation ( MT ) systems _/entity2_ has been considered to be more complicated than _entity3_ analysis _/entity3_ and _entity4_ generation _/entity4_ , since it is inherently a conglomeration of individual _entity5_ lexical rules _/entity5_ . Currently some attempts are being made to use _entity6_ case-based reasoning _/entity6_ in _entity7_ machine translation _/entity7_ , that is , to make decisions on the basis of _entity8_ translation examples _/entity8_ at appropriate pints in _entity9_ MT _/entity9_ . This paper proposes a new type of _entity10_ transfer system _/entity10_ , called a _entity11_ _P_ Similarity-driven Transfer System ( SimTran ) _/entity11_ , for use in such _entity12_ _C_ case-based MT ( CBMT ) _/entity12_ .	USAGE entity11 entity12
There are several approaches that model _entity1_ _C_ information extraction _/entity1_ as a _entity2_ _P_ token classification task _/entity2_ , using various _entity3_ tagging strategies _/entity3_ to combine multiple _entity4_ tokens _/entity4_ . We describe the _entity5_ tagging strategies _/entity5_ that can be found in the literature and evaluate their relative performances . We also introduce a new strategy , called _entity6_ Begin/After tagging _/entity6_ or _entity7_ BIA _/entity7_ , and show that it is competitive to the best other strategies .	NONE entity2 entity1
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ _C_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ _P_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity25 entity24
For _entity1_ intelligent interactive systems _/entity1_ to communicate with _entity2_ humans _/entity2_ in a natural manner , they must have knowledge about the _entity3_ system users _/entity3_ . This paper explores the role of _entity4_ user modeling _/entity4_ in such _entity5_ systems _/entity5_ . It begins with a characterization of what a _entity6_ user model _/entity6_ is and how it can be used . The types of information that a _entity7_ user model _/entity7_ may be required to keep about a _entity8_ user _/entity8_ are then identified and discussed . _entity9_ User models _/entity9_ themselves can vary greatly depending on the requirements of the situation and the implementation , so several dimensions along which they can be classified are presented . Since acquiring the knowledge for a _entity10_ _C_ user model _/entity10_ is a fundamental problem in _entity11_ _P_ user modeling _/entity11_ , a section is devoted to this topic . Next , the benefits and costs of implementing a _entity12_ user modeling component _/entity12_ for a system are weighed in light of several aspects of the _entity13_ interaction requirements _/entity13_ that may be imposed by the system . Finally , the current state of research in _entity14_ user modeling _/entity14_ is summarized , and future research topics that must be addressed in order to achieve powerful , general _entity15_ user modeling systems _/entity15_ are assessed .	NONE entity11 entity10
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ _C_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ _P_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity9 entity8
In this paper we present a _entity1_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ _C_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ states _/entity9_ which handles some additional cases of multiple _entity10_ _P_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity10 entity8
We present a new _entity1_ part-of-speech tagger _/entity1_ that demonstrates the following ideas : ( i ) explicit use of both preceding and following _entity2_ tag contexts _/entity2_ via a _entity3_ _P_ dependency network representation _/entity3_ , ( ii ) broad use of _entity4_ lexical features _/entity4_ , including _entity5_ jointly conditioning on multiple consecutive words _/entity5_ , ( iii ) effective use of _entity6_ _C_ priors _/entity6_ in _entity7_ conditional loglinear models _/entity7_ , and ( iv ) fine-grained modeling of _entity8_ unknown word features _/entity8_ . Using these ideas together , the resulting _entity9_ tagger _/entity9_ gives a 97.24 % _entity10_ accuracy _/entity10_ on the _entity11_ Penn Treebank WSJ _/entity11_ , an _entity12_ error reduction _/entity12_ of 4.4 % on the best previous single automatically learned _entity13_ tagging _/entity13_ result .	NONE entity3 entity6
A proper treatment of _entity1_ _P_ syntax _/entity1_ and _entity2_ _C_ semantics _/entity2_ in _entity3_ machine translation _/entity3_ is introduced and discussed from the empirical viewpoint . For _entity4_ English-Japanese machine translation _/entity4_ , the _entity5_ syntax directed approach _/entity5_ is effective where the _entity6_ Heuristic Parsing Model ( HPM ) _/entity6_ and the _entity7_ Syntactic Role System _/entity7_ play important roles . For _entity8_ Japanese-English translation _/entity8_ , the _entity9_ semantics directed approach _/entity9_ is powerful where the _entity10_ Conceptual Dependency Diagram ( CDD ) _/entity10_ and the _entity11_ Augmented Case Marker System _/entity11_ ( which is a kind of _entity12_ Semantic Role System _/entity12_ ) play essential roles . Some examples of the difference between _entity13_ Japanese sentence structure _/entity13_ and _entity14_ English sentence structure _/entity14_ , which is vital to _entity15_ machine translation _/entity15_ are also discussed together with various interesting _entity16_ ambiguities _/entity16_ .	NONE entity1 entity2
While _entity1_ _C_ paraphrasing _/entity1_ is critical both for _entity2_ interpretation and generation of natural language _/entity2_ , current systems use manual or semi-automatic methods to collect _entity3_ _P_ paraphrases _/entity3_ . We present an _entity4_ unsupervised learning algorithm _/entity4_ for _entity5_ identification of paraphrases _/entity5_ from a _entity6_ corpus of multiple English translations _/entity6_ of the same _entity7_ source text _/entity7_ . Our approach yields _entity8_ phrasal and single word lexical paraphrases _/entity8_ as well as _entity9_ syntactic paraphrases _/entity9_ .	NONE entity3 entity1
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ _P_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ _C_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ classification-based representation techniques _/entity7_ can be applied to _entity8_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ semantic interpretation _/entity13_ was first explored in the _entity14_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	COMPARE entity4 entity6
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ _C_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ multiple speakers _/entity5_ without _entity6_ _P_ explicit speaker enrollment _/entity6_ . Combining _entity7_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity6 entity3
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ _P_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ _C_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity6 entity9
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ _C_ predicate _/entity14_ , calibrating the _entity15_ _P_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity15 entity14
In this paper , we describe the _entity1_ pronominal anaphora resolution module _/entity1_ of _entity2_ Lucy _/entity2_ , a portable _entity3_ _C_ English understanding system _/entity3_ . The design of this module was motivated by the observation that , although there exist many theories of _entity4_ _P_ anaphora resolution _/entity4_ , no one of these theories is complete . Thus we have implemented a _entity5_ blackboard-like architecture _/entity5_ in which individual _entity6_ partial theories _/entity6_ can be encoded as separate modules that can interact to propose candidate _entity7_ antecedents _/entity7_ and to evaluate each other 's proposals .	NONE entity4 entity3
This paper presents a _entity1_ maximum entropy word alignment algorithm _/entity1_ for _entity2_ Arabic-English _/entity2_ based on _entity3_ supervised training data _/entity3_ . We demonstrate that it is feasible to create _entity4_ training material _/entity4_ for problems in _entity5_ machine translation _/entity5_ and that a mixture of _entity6_ supervised and unsupervised methods _/entity6_ yields superior _entity7_ _P_ performance _/entity7_ . The _entity8_ _C_ probabilistic model _/entity8_ used in the _entity9_ alignment _/entity9_ directly models the _entity10_ link decisions _/entity10_ . Significant improvement over traditional _entity11_ word alignment techniques _/entity11_ is shown as well as improvement on several _entity12_ machine translation tests _/entity12_ . Performance of the algorithm is contrasted with _entity13_ human annotation performance _/entity13_ .	NONE entity7 entity8
A method for _entity1_ error correction _/entity1_ of _entity2_ _C_ ill-formed input _/entity2_ is described that acquires _entity3_ dialogue patterns _/entity3_ in typical usage and uses these _entity4_ patterns _/entity4_ to predict new inputs . _entity5_ _P_ Error correction _/entity5_ is done by strongly biasing _entity6_ parsing _/entity6_ toward expected _entity7_ meanings _/entity7_ unless clear evidence from the input shows the current _entity8_ sentence _/entity8_ is not expected . A _entity9_ dialogue acquisition and tracking algorithm _/entity9_ is presented along with a description of its _entity10_ implementation _/entity10_ in a _entity11_ voice interactive system _/entity11_ . A series of tests are described that show the power of the _entity12_ error correction methodology _/entity12_ when _entity13_ stereotypic dialogue _/entity13_ occurs .	NONE entity5 entity2
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ _P_ semantic similarity measures _/entity11_ and _entity12_ _C_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity11 entity12
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ _C_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ _P_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity20 entity17
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ _C_ natural language interface _/entity2_ to _entity3_ _P_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ sentences _/entity6_ in _entity7_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity3 entity2
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ _P_ tenses _/entity27_ is fixed by the _entity28_ _C_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity27 entity28
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ _C_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ _P_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity14 entity11
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ _C_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ _P_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity23 entity20
This paper describes to what extent _entity1_ deep processing _/entity1_ may benefit from _entity2_ shallow techniques _/entity2_ and it presents a _entity3_ _P_ NLP system _/entity3_ which integrates a _entity4_ linguistic PoS tagger and chunker _/entity4_ as a preprocessing module of a _entity5_ broad coverage unification based grammar of Spanish _/entity5_ . Experiments show that the _entity6_ _C_ efficiency _/entity6_ of the overall analysis improves significantly and that our system also provides _entity7_ robustness _/entity7_ to the _entity8_ linguistic processing _/entity8_ while maintaining both the _entity9_ accuracy _/entity9_ and the _entity10_ precision _/entity10_ of the _entity11_ grammar _/entity11_ .	NONE entity3 entity6
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ _P_ theory _/entity4_ is expressed in a _entity5_ _C_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity4 entity5
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ _P_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ _C_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity6 entity7
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ _P_ confidence _/entity6_ the system has in the correctness of each _entity7_ _C_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	MODEL-FEATURE entity6 entity7
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ _C_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ _P_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity14 entity11
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ _C_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ _P_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity5 entity4
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ _P_ dictionary _/entity4_ using existing _entity5_ _C_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity4 entity5
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ _P_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ _C_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity3 entity5
_entity1_ Techniques for automatically training _/entity1_ modules of a _entity2_ natural language generator _/entity2_ have recently been proposed , but a fundamental concern is whether the _entity3_ quality _/entity3_ of _entity4_ utterances _/entity4_ produced with _entity5_ trainable components _/entity5_ can compete with _entity6_ hand-crafted template-based or rule-based approaches _/entity6_ . In this paper We experimentally evaluate a _entity7_ _P_ trainable sentence planner _/entity7_ for a _entity8_ spoken dialogue system _/entity8_ by eliciting _entity9_ _C_ subjective human judgments _/entity9_ . In order to perform an exhaustive comparison , we also evaluate a _entity10_ hand-crafted template-based generation component _/entity10_ , two _entity11_ rule-based sentence planners _/entity11_ , and two _entity12_ baseline sentence planners _/entity12_ . We show that the _entity13_ trainable sentence planner _/entity13_ performs better than the _entity14_ rule-based systems _/entity14_ and the _entity15_ baselines _/entity15_ , and as well as the _entity16_ hand-crafted system _/entity16_ .	NONE entity7 entity9
An empirical comparison of _entity1_ _P_ CFG filtering techniques _/entity1_ for _entity2_ LTAG _/entity2_ and _entity3_ HPSG _/entity3_ is presented . We demonstrate that an approximation of _entity4_ _C_ HPSG _/entity4_ produces a more effective _entity5_ CFG filter _/entity5_ than that of _entity6_ LTAG _/entity6_ . We also investigate the reason for that difference .	NONE entity1 entity4
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ natural language interface _/entity2_ to _entity3_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ _P_ sentences _/entity6_ in _entity7_ _C_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity6 entity7
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ _P_ semantic network _/entity16_ using a variant of a _entity17_ _C_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity16 entity17
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ _P_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ _C_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity10 entity12
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ _C_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ _P_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity24 entity22
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ _P_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ _C_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity27 entity29
Previous research has demonstrated the utility of _entity1_ clustering _/entity1_ in inducing _entity2_ semantic verb classes _/entity2_ from undisambiguated _entity3_ corpus data _/entity3_ . We describe a new approach which involves clustering _entity4_ subcategorization frame ( SCF ) _/entity4_ distributions using the _entity5_ Information Bottleneck _/entity5_ and _entity6_ _P_ nearest neighbour _/entity6_ methods . In contrast to previous work , we particularly focus on clustering _entity7_ polysemic verbs _/entity7_ . A novel _entity8_ evaluation scheme _/entity8_ is proposed which accounts for the effect of _entity9_ _C_ polysemy _/entity9_ on the _entity10_ clusters _/entity10_ , offering us a good insight into the potential and limitations of _entity11_ semantically classifying _/entity11_ _entity12_ undisambiguated SCF data _/entity12_ .	NONE entity6 entity9
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ _P_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ _C_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity2 entity4
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ _C_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ _P_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity14 entity12
The _entity1_ JAVELIN system _/entity1_ integrates a flexible , _entity2_ planning-based architecture _/entity2_ with a variety of _entity3_ language processing modules _/entity3_ to provide an _entity4_ _C_ open-domain question answering capability _/entity4_ on _entity5_ _P_ free text _/entity5_ . The demonstration will focus on how _entity6_ JAVELIN _/entity6_ processes _entity7_ questions _/entity7_ and retrieves the most likely _entity8_ answer candidates _/entity8_ from the given _entity9_ text corpus _/entity9_ . The operation of the system will be explained in depth through browsing the _entity10_ repository _/entity10_ of _entity11_ data objects _/entity11_ created by the system during each _entity12_ question answering session _/entity12_ .	NONE entity5 entity4
The _entity1_ interlingual approach to MT _/entity1_ has been repeatedly advocated by researchers originally interested in _entity2_ natural language understanding _/entity2_ who take _entity3_ _C_ machine translation _/entity3_ to be one possible application . However , not only the _entity4_ ambiguity _/entity4_ but also the vagueness which every _entity5_ natural language _/entity5_ inevitably has leads this approach into essential difficulties . In contrast , our project , the _entity6_ _P_ Mu-project _/entity6_ , adopts the _entity7_ transfer approach _/entity7_ as the basic framework of _entity8_ MT _/entity8_ . This paper describes the detailed construction of the _entity9_ transfer phase _/entity9_ of our system from _entity10_ Japanese _/entity10_ to _entity11_ English _/entity11_ , and gives some examples of problems which seem difficult to treat in the _entity12_ interlingual approach _/entity12_ . The basic design principles of the _entity13_ transfer phase _/entity13_ of our system have already been mentioned in ( 1 ) ( 2 ) . Some of the principles which are relevant to the topic of this paper are : ( a ) _entity14_ Multiple Layer of Grammars _/entity14_ ( b ) _entity15_ Multiple Layer Presentation _/entity15_ ( c ) _entity16_ Lexicon Driven Processing _/entity16_ ( d ) _entity17_ Form-Oriented Dictionary Description _/entity17_ . This paper also shows how these principles are realized in the current system .	NONE entity6 entity3
We describe a _entity1_ dialogue system _/entity1_ that works with its interlocutor to identify objects . Our contributions include a concise , _entity2_ modular architecture _/entity2_ with reversible processes of _entity3_ understanding _/entity3_ and _entity4_ generation _/entity4_ , an _entity5_ information-state model of reference _/entity5_ , and flexible links between _entity6_ _C_ semantics _/entity6_ and _entity7_ _P_ collaborative problem solving _/entity7_ .	NONE entity7 entity6
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ _C_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ _P_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity11 entity9
This paper considers the problem of automatic assessment of _entity1_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ _P_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ _C_ induced model _/entity10_ achieves significantly higher _entity11_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity8 entity10
_entity1_ Systemic grammar _/entity1_ has been used for _entity2_ AI text generation _/entity2_ work in the past , but the _entity3_ _C_ implementations _/entity3_ have tended be ad hoc or inefficient . This paper presents an approach to systemic _entity4_ text generation _/entity4_ where _entity5_ _P_ AI problem solving techniques _/entity5_ are applied directly to an unadulterated _entity6_ systemic grammar _/entity6_ . This _entity7_ approach _/entity7_ is made possible by a special relationship between _entity8_ systemic grammar _/entity8_ and _entity9_ problem solving _/entity9_ : both are organized primarily as choosing from alternatives . The result is simple , efficient _entity10_ text generation _/entity10_ firmly based in a _entity11_ linguistic theory _/entity11_ .	NONE entity5 entity3
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ _P_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ _C_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity21 entity22
There are several approaches that model _entity1_ information extraction _/entity1_ as a _entity2_ token classification task _/entity2_ , using various _entity3_ tagging strategies _/entity3_ to combine multiple _entity4_ tokens _/entity4_ . We describe the _entity5_ _P_ tagging strategies _/entity5_ that can be found in the literature and evaluate their relative performances . We also introduce a new strategy , called _entity6_ Begin/After tagging _/entity6_ or _entity7_ _C_ BIA _/entity7_ , and show that it is competitive to the best other strategies .	NONE entity5 entity7
The applicability of many current _entity1_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ _P_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ supervised methods _/entity13_ on 50 _entity14_ _C_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity12 entity14
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ _C_ human language learners _/entity2_ , to the _entity3_ _P_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity3 entity2
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ _C_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ _P_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity11 entity9
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ _P_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ _C_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity18 entity21
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ _P_ pronoun resolution _/entity2_ in _entity3_ _C_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ features _/entity6_ designed for _entity7_ pronoun resolution _/entity7_ in _entity8_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	NONE entity2 entity3
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ _C_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ _P_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity9 entity6
This paper investigates some _entity1_ computational problems _/entity1_ associated with _entity2_ _C_ probabilistic translation models _/entity2_ that have recently been adopted in the literature on _entity3_ machine translation _/entity3_ . These _entity4_ models _/entity4_ can be viewed as pairs of _entity5_ _P_ probabilistic context-free grammars _/entity5_ working in a 'synchronous ' way . Two _entity6_ hardness _/entity6_ results for the class _entity7_ NP _/entity7_ are reported , along with an _entity8_ exponential time lower-bound _/entity8_ for certain classes of algorithms that are currently used in the literature .	NONE entity5 entity2
We present a _entity1_ corpus-based study _/entity1_ of methods that have been proposed in the _entity2_ linguistics literature _/entity2_ for selecting the _entity3_ semantically unmarked term _/entity3_ out of a pair of _entity4_ antonymous adjectives _/entity4_ . Solutions to this problem are applicable to the more general task of selecting the positive _entity5_ term _/entity5_ from the pair . Using _entity6_ automatically collected data _/entity6_ , the _entity7_ accuracy _/entity7_ and applicability of each method is quantified , and a _entity8_ statistical analysis _/entity8_ of the significance of the results is performed . We show that some simple methods are indeed good indicators for the answer to the problem while other proposed methods fail to perform better than would be attributable to chance . In addition , one of the simplest methods , _entity9_ text frequency _/entity9_ , dominates all others . We also apply two _entity10_ _P_ generic statistical learning methods _/entity10_ for combining the indications of the individual methods , and compare their performance to the simple methods . The most sophisticated _entity11_ _C_ complex learning method _/entity11_ offers a small , but statistically significant , improvement over the original tests .	NONE entity10 entity11
We approximate _entity1_ _P_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ _C_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity1 entity3
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ _P_ entailment _/entity16_ . Our _entity17_ _C_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity16 entity17
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ _C_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ _P_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity12 entity10
The reality of _entity1_ analogies between words _/entity1_ is refuted by noone ( e.g. , I walked is to to walk as I laughed is to to laugh , noted I walked : to walk : : I laughed : to laugh ) . But _entity2_ computational linguists _/entity2_ seem to be quite dubious about _entity3_ analogies between sentences _/entity3_ : they would not be enough numerous to be of any use . We report experiments conducted on a _entity4_ multilingual corpus _/entity4_ to estimate the number of _entity5_ analogies _/entity5_ among the _entity6_ sentences _/entity6_ that it contains . We give two estimates , a lower one and a higher one . As an _entity7_ analogy _/entity7_ must be valid on the level of _entity8_ form _/entity8_ as well as on the level of _entity9_ _P_ meaning _/entity9_ , we relied on the idea that _entity10_ _C_ translation _/entity10_ should preserve _entity11_ meaning _/entity11_ to test for similar _entity12_ meanings _/entity12_ .	NONE entity9 entity10
When people use _entity1_ natural language _/entity1_ in natural settings , they often use it ungrammatically , missing out or repeating words , breaking-off and restarting , speaking in fragments , etc.. Their _entity2_ human listeners _/entity2_ are usually able to cope with these deviations with little difficulty . If a _entity3_ computer system _/entity3_ wishes to accept _entity4_ _P_ natural language input _/entity4_ from its _entity5_ users _/entity5_ on a routine basis , it must display a similar indifference . In this paper , we outline a set of _entity6_ _C_ parsing flexibilities _/entity6_ that such a system should provide . We go , on to describe _entity7_ FlexP _/entity7_ , a _entity8_ bottom-up pattern-matching parser _/entity8_ that we have designed and implemented to provide these flexibilities for _entity9_ restricted natural language _/entity9_ input to a limited-domain computer system .	NONE entity4 entity6
_entity1_ Pipelined Natural Language Generation ( NLG ) systems _/entity1_ have grown increasingly complex as _entity2_ architectural modules _/entity2_ were added to support _entity3_ language functionalities _/entity3_ such as _entity4_ referring expressions _/entity4_ , _entity5_ lexical choice _/entity5_ , and _entity6_ revision _/entity6_ . This has given rise to discussions about the relative placement of these new _entity7_ modules _/entity7_ in the overall _entity8_ architecture _/entity8_ . Recent work on another aspect of _entity9_ _C_ multi-paragraph text _/entity9_ , _entity10_ discourse markers _/entity10_ , indicates it is time to consider where a _entity11_ discourse marker insertion algorithm _/entity11_ fits in . We present examples which suggest that in a _entity12_ _P_ pipelined NLG architecture _/entity12_ , the best approach is to strongly tie it to a _entity13_ revision component _/entity13_ . Finally , we evaluate the approach in a working _entity14_ multi-page system _/entity14_ .	NONE entity12 entity9
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ news commentary texts _/entity7_ : _entity8_ Colon _/entity8_ , _entity9_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ _C_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ _P_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity15 entity12
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ _P_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ _C_ German corpus _/entity11_ of 2.284 _entity12_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ baseline _/entity13_ of 54.55 % ) .	NONE entity8 entity11
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ character sets _/entity6_ should be treated differently and the changes between _entity7_ character types _/entity7_ are very important because _entity8_ Japanese script _/entity8_ has both _entity9_ _P_ ideograms _/entity9_ like _entity10_ Chinese _/entity10_ ( _entity11_ _C_ kanji _/entity11_ ) and _entity12_ phonograms _/entity12_ like _entity13_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity9 entity11
This paper proposes a generic _entity1_ mathematical formalism _/entity1_ for the combination of various _entity2_ structures _/entity2_ : _entity3_ strings _/entity3_ , _entity4_ trees _/entity4_ , _entity5_ dags _/entity5_ , _entity6_ graphs _/entity6_ , and products of them . The _entity7_ polarization _/entity7_ of the objects of the _entity8_ elementary structures _/entity8_ controls the _entity9_ saturation _/entity9_ of the final _entity10_ structure _/entity10_ . This formalism is both elementary and powerful enough to strongly simulate many _entity11_ grammar formalisms _/entity11_ , such as _entity12_ rewriting systems _/entity12_ , _entity13_ dependency grammars _/entity13_ , _entity14_ _P_ TAG _/entity14_ , _entity15_ _C_ HPSG _/entity15_ and _entity16_ LFG _/entity16_ .	NONE entity14 entity15
Recently , we initiated a project to develop a _entity1_ _C_ phonetically-based spoken language understanding system _/entity1_ called _entity2_ SUMMIT _/entity2_ . In contrast to many of the past efforts that make use of _entity3_ heuristic rules _/entity3_ whose development requires intense _entity4_ _P_ knowledge engineering _/entity4_ , our approach attempts to express the _entity5_ speech knowledge _/entity5_ within a formal framework using well-defined mathematical tools . In our system , _entity6_ features _/entity6_ and _entity7_ decision strategies _/entity7_ are discovered and trained automatically , using a large body of _entity8_ speech data _/entity8_ . This paper describes the system , and documents its current performance .	NONE entity4 entity1
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ _C_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ _P_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity12 entity10
In order to boost the _entity1_ translation quality _/entity1_ of _entity2_ EBMT _/entity2_ based on a small-sized _entity3_ _P_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ language model _/entity5_ of an in-domain _entity6_ _C_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ BLEU score _/entity9_ and the _entity10_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity3 entity6
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ _C_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ _P_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity6 entity3
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ _P_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ _C_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity22 entity25
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ _C_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ _P_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity11 entity10
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ _C_ phrases _/entity15_ at any distance in the _entity16_ _P_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity16 entity15
This paper proposes a practical approach employing _entity1_ n-gram models _/entity1_ and _entity2_ _P_ error-correction rules _/entity2_ for _entity3_ _C_ Thai key prediction _/entity3_ and _entity4_ Thai-English language identification _/entity4_ . The paper also proposes _entity5_ rule-reduction algorithm _/entity5_ applying _entity6_ mutual information _/entity6_ to reduce the _entity7_ error-correction rules _/entity7_ . Our algorithm reported more than 99 % _entity8_ accuracy _/entity8_ in both _entity9_ language identification _/entity9_ and _entity10_ key prediction _/entity10_ .	USAGE entity2 entity3
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ _C_ word _/entity17_ from a _entity18_ _P_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity18 entity17
This paper reports recent research into methods for _entity1_ _C_ creating natural language text _/entity1_ . A new _entity2_ _P_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity2 entity1
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ _C_ training and testing factors _/entity10_ on _entity11_ _P_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity11 entity10
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ _C_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ _P_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity17 entity15
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ _C_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ _P_ phrases _/entity12_ longer than three _entity13_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity12 entity11
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ _P_ maximum entropy classifiers _/entity2_ , _entity3_ _C_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity2 entity3
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ _P_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ _C_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity18 entity20
In this paper , we present a novel _entity1_ _C_ training method _/entity1_ for a _entity2_ _P_ localized phrase-based prediction model _/entity2_ for _entity3_ statistical machine translation ( SMT ) _/entity3_ . The _entity4_ model _/entity4_ predicts _entity5_ blocks _/entity5_ with orientation to handle _entity6_ local phrase re-ordering _/entity6_ . We use a _entity7_ maximum likelihood criterion _/entity7_ to train a _entity8_ log-linear block bigram model _/entity8_ which uses _entity9_ real-valued features _/entity9_ ( e.g . a _entity10_ language model score _/entity10_ ) as well as _entity11_ binary features _/entity11_ based on the _entity12_ block _/entity12_ identities themselves , e.g . block bigram features . Our _entity13_ training algorithm _/entity13_ can easily handle millions of _entity14_ features _/entity14_ . The best system obtains a 18.6 % improvement over the _entity15_ baseline _/entity15_ on a standard _entity16_ Arabic-English translation task _/entity16_ .	NONE entity2 entity1
This paper considers the problem of automatic assessment of _entity1_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ _C_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ _P_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity11 entity8
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ _C_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ _P_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity4 entity2
In this paper , we improve an _entity1_ unsupervised learning method _/entity1_ using the _entity2_ Expectation-Maximization ( EM ) algorithm _/entity2_ proposed by Nigam et al . for _entity3_ text classification problems _/entity3_ in order to apply it to _entity4_ word sense disambiguation ( WSD ) problems _/entity4_ . The improved method stops the _entity5_ _P_ EM algorithm _/entity5_ at the _entity6_ optimum iteration number _/entity6_ . To estimate that number , we propose two methods . In experiments , we solved 50 _entity7_ noun WSD problems _/entity7_ in the _entity8_ _C_ Japanese Dictionary Task in SENSEVAL2 _/entity8_ . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for _entity9_ verb WSD problems _/entity9_ .	NONE entity5 entity8
_entity1_ Metagrammatical formalisms _/entity1_ that combine _entity2_ _P_ context-free phrase structure rules _/entity2_ and _entity3_ metarules ( MPS grammars ) _/entity3_ allow concise statement of generalizations about the _entity4_ syntax _/entity4_ of _entity5_ _C_ natural languages _/entity5_ . _entity6_ Unconstrained MPS grammars _/entity6_ , unfortunately , are not computationally safe . We evaluate several proposals for constraining them , basing our assessment on _entity7_ computational tractability and explanatory adequacy _/entity7_ . We show that none of them satisfies both criteria , and suggest new directions for research on alternative _entity8_ metagrammatical formalisms _/entity8_ .	NONE entity2 entity5
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ _C_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ _P_ grammar coverage problems _/entity3_ we use a _entity4_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity3 entity2
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ _P_ statistical approaches _/entity6_ , which resolve _entity7_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ _C_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity6 entity8
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ _C_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ _P_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity18 entity15
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ _C_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ _P_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity13 entity11
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ _C_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ _P_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity15 entity14
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ _C_ latent variables _/entity7_ . Finegrained _entity8_ _P_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity8 entity7
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ language independent modules _/entity3_ of the _entity4_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ _P_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ _C_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity5 entity6
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ _C_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ _P_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity11 entity8
In this paper a _entity1_ morphological component _/entity1_ with a limited capability to automatically interpret ( and generate ) _entity2_ _C_ derived words _/entity2_ is presented . The system combines an extended _entity3_ two-level morphology _/entity3_ [ Trost , 1991a ; Trost , 1991b ] with a _entity4_ feature-based word grammar _/entity4_ building on a _entity5_ _P_ hierarchical lexicon _/entity5_ . _entity6_ Polymorphemic stems _/entity6_ not explicitly stored in the _entity7_ lexicon _/entity7_ are given a _entity8_ compositional interpretation _/entity8_ . That way the system allows to minimize redundancy in the _entity9_ lexicon _/entity9_ because _entity10_ derived words _/entity10_ that are transparent need not to be stored explicitly . Also , _entity11_ words formed ad-hoc _/entity11_ can be recognized correctly . The system is implemented in CommonLisp and has been tested on examples from _entity12_ German derivation _/entity12_ .	NONE entity5 entity2
Sources of _entity1_ _P_ training data _/entity1_ suitable for _entity2_ _C_ language modeling _/entity2_ of _entity3_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	USAGE entity1 entity2
In this paper we discuss a proposed _entity1_ _P_ user knowledge modeling architecture _/entity1_ for the _entity2_ ICICLE system _/entity2_ , a _entity3_ language tutoring application _/entity3_ for deaf learners of _entity4_ _C_ written English _/entity4_ . The model will represent the _entity5_ language proficiency _/entity5_ of the user and is designed to be referenced during both _entity6_ writing analysis _/entity6_ and _entity7_ feedback production _/entity7_ . We motivate our _entity8_ model design _/entity8_ by citing relevant research on _entity9_ second language and cognitive skill acquisition _/entity9_ , and briefly discuss preliminary empirical evidence supporting the _entity10_ design _/entity10_ . We conclude by showing how our _entity11_ design _/entity11_ can provide a rich and _entity12_ robust information base _/entity12_ to a language assessment / correction application by modeling _entity13_ user proficiency _/entity13_ at a high level of granularity and specificity .	NONE entity1 entity4
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ _P_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ _C_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity10 entity13
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ _C_ words , conceptual mappings _/entity14_ , and _entity15_ _P_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity15 entity14
This paper proposes an automatic , essentially _entity1_ _P_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ _C_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity1 entity3
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ _P_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ _C_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity19 entity21
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ _C_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ _P_ CAS approach _/entity23_ .	NONE entity23 entity20
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ _C_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ _P_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity9 entity6
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ _C_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ _P_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity14 entity12
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ _C_ utterances _/entity6_ ( called the _entity7_ _P_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	MODEL-FEATURE entity7 entity6
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ _P_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ _C_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity3 entity5
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ _C_ unknown word _/entity12_ . The _entity13_ _P_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity13 entity12
In this paper a _entity1_ _P_ morphological component _/entity1_ with a limited capability to automatically interpret ( and generate ) _entity2_ derived words _/entity2_ is presented . The system combines an extended _entity3_ _C_ two-level morphology _/entity3_ [ Trost , 1991a ; Trost , 1991b ] with a _entity4_ feature-based word grammar _/entity4_ building on a _entity5_ hierarchical lexicon _/entity5_ . _entity6_ Polymorphemic stems _/entity6_ not explicitly stored in the _entity7_ lexicon _/entity7_ are given a _entity8_ compositional interpretation _/entity8_ . That way the system allows to minimize redundancy in the _entity9_ lexicon _/entity9_ because _entity10_ derived words _/entity10_ that are transparent need not to be stored explicitly . Also , _entity11_ words formed ad-hoc _/entity11_ can be recognized correctly . The system is implemented in CommonLisp and has been tested on examples from _entity12_ German derivation _/entity12_ .	NONE entity1 entity3
This paper proposes an _entity1_ _P_ alignment adaptation approach _/entity1_ to improve _entity2_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ _C_ alignment adaptation _/entity3_ is to use _entity4_ out-of-domain corpus _/entity4_ to improve _entity5_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ precision _/entity11_ and _entity12_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity1 entity3
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ _P_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ _C_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity13 entity16
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ _P_ anaphora references _/entity6_ and _entity7_ _C_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity6 entity7
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ _P_ meaning _/entity26_ of the _entity27_ _C_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	MODEL-FEATURE entity26 entity27
In this paper we present a _entity1_ _C_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ _P_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ states _/entity9_ which handles some additional cases of multiple _entity10_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity4 entity1
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ _P_ decision-tree classifier _/entity7_ for 30 _entity8_ _C_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity7 entity8
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ high-quality data _/entity7_ that allow the comparison of both _entity8_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ _P_ automatic metrics _/entity11_ used within _entity12_ _C_ MT _/entity12_ are also discussed in this regard .	USAGE entity11 entity12
In order to meet the needs of a publication of papers in English , many systems to run off texts have been developed . In this paper , we report a system _entity1_ FROFF _/entity1_ which can make a fair copy of not only texts but also graphs and tables indispensable to our papers . Its selection of _entity2_ fonts _/entity2_ , specification of _entity3_ character _/entity3_ size are dynamically changeable , and the _entity4_ _C_ typing location _/entity4_ can be also changed in lateral or longitudinal directions . Each _entity5_ character _/entity5_ has its own width and a line length is counted by the sum of each _entity6_ _P_ character _/entity6_ . By using commands or _entity7_ rules _/entity7_ which are defined to facilitate the construction of format expected or some _entity8_ mathematical expressions _/entity8_ , elaborate and pretty documents can be successfully obtained .	NONE entity6 entity4
In order to build robust _entity1_ automatic abstracting systems _/entity1_ , there is a need for better _entity2_ training resources _/entity2_ than are currently available . In this paper , we introduce an _entity3_ _P_ annotation scheme _/entity3_ for scientific articles which can be used to build such a _entity4_ resource _/entity4_ in a consistent way . The seven categories of the _entity5_ scheme _/entity5_ are based on _entity6_ _C_ rhetorical moves _/entity6_ of _entity7_ argumentation _/entity7_ . Our experimental results show that the _entity8_ scheme _/entity8_ is stable , reproducible and intuitive to use .	NONE entity3 entity6
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ _C_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ _P_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity13 entity11
How to obtain _entity1_ hierarchical relations _/entity1_ ( e.g . _entity2_ superordinate -hyponym relation _/entity2_ , _entity3_ _C_ synonym relation _/entity3_ ) is one of the most important problems for _entity4_ _P_ thesaurus construction _/entity4_ . A pilot system for extracting these _entity5_ relations _/entity5_ automatically from an ordinary _entity6_ Japanese language dictionary _/entity6_ ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given . The features of the _entity7_ definition sentences _/entity7_ in the _entity8_ dictionary _/entity8_ , the mechanical extraction of the _entity9_ hierarchical relations _/entity9_ and the estimation of the results are discussed .	NONE entity4 entity3
_entity1_ _P_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ _C_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity1 entity4
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ _C_ parsing _/entity4_ . We compare two wide-coverage _entity5_ lexicalized grammars of English _/entity5_ , _entity6_ LEXSYS _/entity6_ and _entity7_ _P_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity7 entity4
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ _C_ disjunctive values _/entity29_ is _entity30_ _P_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity30 entity29
We propose a solution to the challenge of the _entity1_ CoNLL 2008 shared task _/entity1_ that uses a _entity2_ generative history-based latent variable model _/entity2_ to predict the most likely _entity3_ derivation _/entity3_ of a _entity4_ synchronous dependency parser _/entity4_ for both _entity5_ _C_ syntactic and semantic dependencies _/entity5_ . The submitted _entity6_ model _/entity6_ yields 79.1 % _entity7_ macro-average F1 performance _/entity7_ , for the joint task , 86.9 % _entity8_ _P_ syntactic dependencies LAS _/entity8_ and 71.0 % _entity9_ semantic dependencies F1 _/entity9_ . A larger _entity10_ model _/entity10_ trained after the deadline achieves 80.5 % _entity11_ macro-average F1 _/entity11_ , 87.6 % _entity12_ syntactic dependencies LAS _/entity12_ , and 73.1 % _entity13_ semantic dependencies F1 _/entity13_ .	NONE entity8 entity5
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ _P_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ _C_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity14 entity16
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ _P_ model _/entity8_ then attempts to improve upon this initial _entity9_ _C_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity8 entity9
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ _P_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ _C_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity19 entity22
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ _C_ event _/entity3_ described in a _entity4_ _P_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity4 entity3
This paper describes the understanding process of the _entity1_ spatial descriptions _/entity1_ in _entity2_ Japanese _/entity2_ . In order to understand the described _entity3_ world _/entity3_ , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space . It is done by an experimental _entity4_ computer program _/entity4_ _entity5_ SPRINT _/entity5_ , which takes _entity6_ _P_ natural language texts _/entity6_ and produces a _entity7_ model _/entity7_ of the described _entity8_ _C_ world _/entity8_ . To reconstruct the _entity9_ model _/entity9_ , the authors extract the _entity10_ qualitative spatial constraints _/entity10_ from the _entity11_ text _/entity11_ , and represent them as the _entity12_ numerical constraints _/entity12_ on the _entity13_ spatial attributes _/entity13_ of the _entity14_ entities _/entity14_ . This makes it possible to express the vagueness of the _entity15_ spatial concepts _/entity15_ and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints . The interpretation reflects the _entity16_ temporary belief _/entity16_ about the _entity17_ world _/entity17_ .	NONE entity6 entity8
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ _C_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ _P_ are available . _/entity16_	NONE entity16 entity14
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ _P_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ _C_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity8 entity11
In this paper we present our recent work on harvesting _entity1_ English-Chinese bitexts _/entity1_ of the laws of Hong Kong from the _entity2_ Web _/entity2_ and aligning them to the _entity3_ subparagraph _/entity3_ level via utilizing the _entity4_ numbering system _/entity4_ in the _entity5_ legal text hierarchy _/entity5_ . Basic methodology and practical techniques are reported in detail . The resultant _entity6_ bilingual corpus _/entity6_ , 10.4M _entity7_ English words _/entity7_ and 18.3M _entity8_ _C_ Chinese characters _/entity8_ , is an authoritative and comprehensive _entity9_ text collection _/entity9_ covering the specific and special domain of HK laws . It is particularly valuable to _entity10_ _P_ empirical MT research _/entity10_ . This piece of work has also laid a foundation for exploring and harvesting _entity11_ English-Chinese bitexts _/entity11_ in a larger volume from the _entity12_ Web _/entity12_ .	NONE entity10 entity8
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ _P_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ _C_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity13 entity16
This paper shows how _entity1_ _P_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ _C_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity1 entity4
This paper describes the framework of a _entity1_ Korean phonological knowledge base system _/entity1_ using the _entity2_ unification-based grammar formalism _/entity2_ : _entity3_ Korean Phonology Structure Grammar ( KPSG ) _/entity3_ . The approach of _entity4_ _P_ KPSG _/entity4_ provides an explicit development model for constructing a computational _entity5_ phonological system _/entity5_ : _entity6_ _C_ speech recognition _/entity6_ and _entity7_ synthesis system _/entity7_ . We show that the proposed approach is more describable than other approaches such as those employing a traditional _entity8_ generative phonological approach _/entity8_ .	NONE entity4 entity6
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ _C_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ _P_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity9 entity6
We describe a method for identifying systematic _entity1_ patterns _/entity1_ in _entity2_ translation data _/entity2_ using _entity3_ part-of-speech tag sequences _/entity3_ . We incorporate this analysis into a _entity4_ _P_ diagnostic tool _/entity4_ intended for _entity5_ developers _/entity5_ of _entity6_ _C_ machine translation systems _/entity6_ , and demonstrate how our application can be used by _entity7_ developers _/entity7_ to explore _entity8_ patterns _/entity8_ in _entity9_ machine translation output _/entity9_ .	NONE entity4 entity6
We present the first application of the _entity1_ head-driven statistical parsing model _/entity1_ of Collins ( 1999 ) as a _entity2_ simultaneous language model _/entity2_ and _entity3_ parser _/entity3_ for _entity4_ large-vocabulary speech recognition _/entity4_ . The model is adapted to an _entity5_ online left to right chart-parser _/entity5_ for _entity6_ word lattices _/entity6_ , integrating acoustic , n-gram , and parser probabilities . The _entity7_ parser _/entity7_ uses _entity8_ structural and lexical dependencies _/entity8_ not considered by _entity9_ n-gram models _/entity9_ , conditioning recognition on more linguistically-grounded relationships . Experiments on the _entity10_ Wall Street Journal treebank _/entity10_ and lattice corpora show _entity11_ _P_ word error rates _/entity11_ competitive with the _entity12_ _C_ standard n-gram language model _/entity12_ while extracting additional _entity13_ structural information _/entity13_ useful for _entity14_ speech understanding _/entity14_ .	NONE entity11 entity12
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ _C_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ _P_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity24 entity22
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ _C_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ _P_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity6 entity3
The paper outlines a _entity1_ _C_ computational theory _/entity1_ of _entity2_ _P_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity2 entity1
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ _C_ deictic information _/entity8_ ) 2. whether the _entity9_ _P_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity9 entity8
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ _P_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ _C_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity14 entity16
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ _P_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ _C_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity8 entity11
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ _P_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ _C_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity19 entity21
In the second year of _entity1_ evaluations _/entity1_ of the _entity2_ ARPA HLT Machine Translation ( MT ) Initiative _/entity2_ , methodologies developed and tested in 1992 were applied to the _entity3_ 1993 MT test runs _/entity3_ . The current methodology optimizes the inherently _entity4_ subjective judgments _/entity4_ on _entity5_ translation accuracy and quality _/entity5_ by channeling the _entity6_ judgments _/entity6_ of _entity7_ non-translators _/entity7_ into many _entity8_ data points _/entity8_ which reflect both the comparison of the _entity9_ performance _/entity9_ of the _entity10_ research MT systems _/entity10_ with _entity11_ _C_ production MT systems _/entity11_ and against the _entity12_ performance _/entity12_ of _entity13_ _P_ novice translators _/entity13_ . This paper discusses the three _entity14_ evaluation methods _/entity14_ used in the _entity15_ 1993 evaluation _/entity15_ , the results of the evaluations , and preliminary characterizations of the _entity16_ Winter 1994 evaluation _/entity16_ , now underway . The efforts under discussion focus on measuring the progress of _entity17_ core MT technology _/entity17_ and increasing the sensitivity and _entity18_ portability _/entity18_ of _entity19_ MT evaluation methodology _/entity19_ .	NONE entity13 entity11
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ _C_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ _P_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity6 entity4
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ _P_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ _C_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity14 entity17
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ _P_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ _C_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity19 entity22
We introduce a new _entity1_ interactive corpus exploration tool _/entity1_ called _entity2_ InfoMagnets _/entity2_ . _entity3_ InfoMagnets _/entity3_ aims at making _entity4_ exploratory corpus analysis _/entity4_ accessible to researchers who are not experts in _entity5_ text mining _/entity5_ . As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between _entity6_ language _/entity6_ and _entity7_ behavioral patterns _/entity7_ in two distinct domains : _entity8_ _P_ tutorial dialogue _/entity8_ ( Kumar et al. , submitted ) and _entity9_ _C_ on-line communities _/entity9_ ( Arguello et al. , 2006 ) . As an _entity10_ educational tool _/entity10_ , it has been used as part of a unit on _entity11_ protocol analysis _/entity11_ in an _entity12_ Educational Research Methods course _/entity12_ .	NONE entity8 entity9
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ _C_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ _P_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity11 entity8
We present a new _entity1_ part-of-speech tagger _/entity1_ that demonstrates the following ideas : ( i ) explicit use of both preceding and following _entity2_ tag contexts _/entity2_ via a _entity3_ dependency network representation _/entity3_ , ( ii ) broad use of _entity4_ lexical features _/entity4_ , including _entity5_ jointly conditioning on multiple consecutive words _/entity5_ , ( iii ) effective use of _entity6_ priors _/entity6_ in _entity7_ conditional loglinear models _/entity7_ , and ( iv ) fine-grained modeling of _entity8_ unknown word features _/entity8_ . Using these ideas together , the resulting _entity9_ tagger _/entity9_ gives a 97.24 % _entity10_ _C_ accuracy _/entity10_ on the _entity11_ _P_ Penn Treebank WSJ _/entity11_ , an _entity12_ error reduction _/entity12_ of 4.4 % on the best previous single automatically learned _entity13_ tagging _/entity13_ result .	NONE entity11 entity10
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ _P_ English _/entity23_ , and the _entity24_ _C_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity23 entity24
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ duration model _/entity3_ has reduced the _entity4_ _C_ error rate _/entity4_ by about 10 % in both _entity5_ _P_ triphone and semiphone systems _/entity5_ . A new _entity6_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity5 entity4
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ _P_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ question phrases _/entity5_ in _entity6_ _C_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ MRR _/entity14_ .	NONE entity4 entity6
In this paper , we present a novel _entity1_ training method _/entity1_ for a _entity2_ localized phrase-based prediction model _/entity2_ for _entity3_ statistical machine translation ( SMT ) _/entity3_ . The _entity4_ model _/entity4_ predicts _entity5_ blocks _/entity5_ with orientation to handle _entity6_ local phrase re-ordering _/entity6_ . We use a _entity7_ _P_ maximum likelihood criterion _/entity7_ to train a _entity8_ _C_ log-linear block bigram model _/entity8_ which uses _entity9_ real-valued features _/entity9_ ( e.g . a _entity10_ language model score _/entity10_ ) as well as _entity11_ binary features _/entity11_ based on the _entity12_ block _/entity12_ identities themselves , e.g . block bigram features . Our _entity13_ training algorithm _/entity13_ can easily handle millions of _entity14_ features _/entity14_ . The best system obtains a 18.6 % improvement over the _entity15_ baseline _/entity15_ on a standard _entity16_ Arabic-English translation task _/entity16_ .	NONE entity7 entity8
In this paper , we present a novel _entity1_ training method _/entity1_ for a _entity2_ localized phrase-based prediction model _/entity2_ for _entity3_ statistical machine translation ( SMT ) _/entity3_ . The _entity4_ model _/entity4_ predicts _entity5_ blocks _/entity5_ with orientation to handle _entity6_ local phrase re-ordering _/entity6_ . We use a _entity7_ maximum likelihood criterion _/entity7_ to train a _entity8_ log-linear block bigram model _/entity8_ which uses _entity9_ _C_ real-valued features _/entity9_ ( e.g . a _entity10_ language model score _/entity10_ ) as well as _entity11_ _P_ binary features _/entity11_ based on the _entity12_ block _/entity12_ identities themselves , e.g . block bigram features . Our _entity13_ training algorithm _/entity13_ can easily handle millions of _entity14_ features _/entity14_ . The best system obtains a 18.6 % improvement over the _entity15_ baseline _/entity15_ on a standard _entity16_ Arabic-English translation task _/entity16_ .	NONE entity11 entity9
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ _C_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ _P_ noun phrases _/entity20_ .	NONE entity20 entity17
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ _P_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ _C_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity3 entity5
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ _P_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ _C_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity7 entity10
In this paper , we reported experiments of _entity1_ unsupervised automatic acquisition _/entity1_ of _entity2_ Italian and English verb subcategorization frames ( SCFs ) _/entity2_ from _entity3_ general and domain corpora _/entity3_ . The proposed technique operates on _entity4_ syntactically shallow-parsed corpora _/entity4_ on the basis of a limited number of _entity5_ search heuristics _/entity5_ not relying on any previous _entity6_ lexico-syntactic knowledge _/entity6_ about _entity7_ SCFs _/entity7_ . Although preliminary , reported results are in line with _entity8_ state-of-the-art lexical acquisition systems _/entity8_ . The issue of whether _entity9_ verbs _/entity9_ sharing similar _entity10_ _C_ SCFs distributions _/entity10_ happen to share _entity11_ similar semantic properties _/entity11_ as well was also explored by clustering _entity12_ _P_ verbs _/entity12_ that share _entity13_ frames _/entity13_ with the same _entity14_ distribution _/entity14_ using the _entity15_ Minimum Description Length Principle ( MDL ) _/entity15_ . First experiments in this direction were carried out on _entity16_ Italian verbs _/entity16_ with encouraging results .	NONE entity12 entity10
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ _P_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ _C_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity2 entity4
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ _C_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ _P_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity5 entity3
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ _C_ phrases _/entity4_ while simultaneously using less _entity5_ _P_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ phrase translations _/entity9_ in our _entity10_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity5 entity4
We present a _entity1_ practically unsupervised learning method _/entity1_ to produce _entity2_ single-snippet answers _/entity2_ to _entity3_ definition questions _/entity3_ in _entity4_ question answering systems _/entity4_ that supplement _entity5_ Web search engines _/entity5_ . The method exploits _entity6_ on-line encyclopedias and dictionaries _/entity6_ to generate automatically an arbitrarily large number of _entity7_ positive and negative definition examples _/entity7_ , which are then used to train an _entity8_ _P_ svm _/entity8_ to separate the two classes . We show experimentally that the proposed method is viable , that it outperforms the alternative of training the _entity9_ system _/entity9_ on _entity10_ _C_ questions _/entity10_ and _entity11_ news articles from trec _/entity11_ , and that it helps the _entity12_ search engine _/entity12_ handle _entity13_ definition questions _/entity13_ significantly better .	NONE entity8 entity10
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ _P_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ _C_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity10 entity13
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ _C_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ _P_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity9 entity6
In this paper , we introduce a _entity1_ generative probabilistic optical character recognition ( OCR ) model _/entity1_ that describes an end-to-end process in the _entity2_ _P_ noisy channel framework _/entity2_ , progressing from generation of _entity3_ _C_ true text _/entity3_ through its transformation into the _entity4_ noisy output _/entity4_ of an _entity5_ OCR system _/entity5_ . The _entity6_ model _/entity6_ is designed for use in _entity7_ error correction _/entity7_ , with a focus on _entity8_ post-processing _/entity8_ the _entity9_ output _/entity9_ of black-box _entity10_ OCR systems _/entity10_ in order to make it more useful for _entity11_ NLP tasks _/entity11_ . We present an implementation of the _entity12_ model _/entity12_ based on _entity13_ finite-state models _/entity13_ , demonstrate the _entity14_ model _/entity14_ 's ability to significantly reduce _entity15_ character and word error rate _/entity15_ , and provide evaluation results involving _entity16_ automatic extraction _/entity16_ of _entity17_ translation lexicons _/entity17_ from _entity18_ printed text _/entity18_ .	NONE entity2 entity3
In this paper , we present a novel _entity1_ training method _/entity1_ for a _entity2_ localized phrase-based prediction model _/entity2_ for _entity3_ statistical machine translation ( SMT ) _/entity3_ . The _entity4_ model _/entity4_ predicts _entity5_ blocks _/entity5_ with orientation to handle _entity6_ local phrase re-ordering _/entity6_ . We use a _entity7_ maximum likelihood criterion _/entity7_ to train a _entity8_ log-linear block bigram model _/entity8_ which uses _entity9_ real-valued features _/entity9_ ( e.g . a _entity10_ language model score _/entity10_ ) as well as _entity11_ binary features _/entity11_ based on the _entity12_ block _/entity12_ identities themselves , e.g . block bigram features . Our _entity13_ training algorithm _/entity13_ can easily handle millions of _entity14_ _C_ features _/entity14_ . The best system obtains a 18.6 % improvement over the _entity15_ _P_ baseline _/entity15_ on a standard _entity16_ Arabic-English translation task _/entity16_ .	NONE entity15 entity14
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ _C_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ _P_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity19 entity18
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ _C_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ _P_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity20 entity18
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ _C_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ _P_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity9 entity7
We describe a _entity1_ dialogue system _/entity1_ that works with its interlocutor to identify objects . Our contributions include a concise , _entity2_ _P_ modular architecture _/entity2_ with reversible processes of _entity3_ understanding _/entity3_ and _entity4_ generation _/entity4_ , an _entity5_ _C_ information-state model of reference _/entity5_ , and flexible links between _entity6_ semantics _/entity6_ and _entity7_ collaborative problem solving _/entity7_ .	NONE entity2 entity5
This paper introduces a method for _entity1_ computational analysis of move structures _/entity1_ in _entity2_ abstracts _/entity2_ of _entity3_ research articles _/entity3_ . In our approach , _entity4_ sentences _/entity4_ in a given _entity5_ abstract _/entity5_ are analyzed and labeled with a specific _entity6_ move _/entity6_ in light of various _entity7_ rhetorical functions _/entity7_ . The method involves automatically gathering a large number of _entity8_ abstracts _/entity8_ from the _entity9_ Web _/entity9_ and building a _entity10_ _P_ language model _/entity10_ of _entity11_ abstract moves _/entity11_ . We also present a prototype _entity12_ concordancer _/entity12_ , _entity13_ _C_ CARE _/entity13_ , which exploits the _entity14_ move-tagged abstracts _/entity14_ for _entity15_ digital learning _/entity15_ . This system provides a promising approach to _entity16_ Web-based computer-assisted academic writing _/entity16_ .	NONE entity10 entity13
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ _C_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ _P_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity4 entity2
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ _C_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ _P_ word senses _/entity14_ using a combination of online sources .	NONE entity14 entity12
In the past the evaluation of _entity1_ _P_ machine translation systems _/entity1_ has focused on single system evaluations because there were only few systems available . But now there are several commercial systems for the same _entity2_ language pair _/entity2_ . This requires new methods of comparative evaluation . In the paper we propose a _entity3_ black-box method _/entity3_ for comparing the _entity4_ _C_ lexical coverage _/entity4_ of _entity5_ MT systems _/entity5_ . The method is based on lists of _entity6_ words _/entity6_ from different _entity7_ frequency classes _/entity7_ . It is shown how these _entity8_ word lists _/entity8_ can be compiled and used for testing . We also present the results of using our method on 6 _entity9_ MT systems _/entity9_ that translate between _entity10_ English _/entity10_ and _entity11_ German _/entity11_ .	NONE entity1 entity4
_entity1_ _C_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ _P_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity2 entity1
In this paper we show how two standard outputs from _entity1_ information extraction ( IE ) systems _/entity1_ - _entity2_ named entity annotations _/entity2_ and _entity3_ scenario templates _/entity3_ - can be used to enhance access to _entity4_ _P_ text collections _/entity4_ via a standard _entity5_ text browser _/entity5_ . We describe how this information is used in a _entity6_ prototype system _/entity6_ designed to support _entity7_ _C_ information workers _/entity7_ ' access to a _entity8_ pharmaceutical news archive _/entity8_ as part of their _entity9_ industry watch _/entity9_ function . We also report results of a preliminary , _entity10_ qualitative user evaluation _/entity10_ of the system , which while broadly positive indicates further work needs to be done on the _entity11_ interface _/entity11_ to make _entity12_ users _/entity12_ aware of the increased potential of _entity13_ IE-enhanced text browsers _/entity13_ .	NONE entity4 entity7
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ _P_ manual transcription _/entity8_ . In our method , _entity9_ _C_ unsupervised training _/entity9_ is first used to train a _entity10_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity8 entity9
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ _P_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ _C_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity14 entity17
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ _C_ training _/entity10_ a _entity11_ _P_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity11 entity10
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ high-quality data _/entity7_ that allow the comparison of both _entity8_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ _C_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ _P_ automatic metrics _/entity11_ used within _entity12_ MT _/entity12_ are also discussed in this regard .	NONE entity11 entity9
This paper discusses the application of _entity1_ Unification Categorial Grammar ( UCG ) _/entity1_ to the framework of _entity2_ Isomorphic Grammars _/entity2_ for _entity3_ Machine Translation _/entity3_ pioneered by Landsbergen . The _entity4_ Isomorphic Grammars approach to MT _/entity4_ involves developing the _entity5_ _P_ grammars _/entity5_ of the _entity6_ Source and Target languages _/entity6_ in parallel , in order to ensure that _entity7_ _C_ SL _/entity7_ and _entity8_ TL _/entity8_ expressions which stand in the _entity9_ translation relation _/entity9_ have _entity10_ isomorphic derivations _/entity10_ . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited , obviating the need for answers to _entity11_ semantic questions _/entity11_ that we do not yet have . _entity12_ Semantic _/entity12_ and other information may still be incorporated , but as constraints on the _entity13_ translation relation _/entity13_ , not as levels of _entity14_ textual representation _/entity14_ . After introducing this approach to _entity15_ MT system _/entity15_ design , and the basics of _entity16_ monolingual UCG _/entity16_ , we will show how the two can be integrated , and present an example from an implemented _entity17_ bi-directional English-Spanish fragment _/entity17_ . Finally we will present some outstanding problems with the approach .	NONE entity5 entity7
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ _C_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ _P_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity9 entity7
We suggest a new goal and _entity1_ evaluation criterion _/entity1_ for _entity2_ word similarity measures _/entity2_ . The new criterion _entity3_ meaning-entailing substitutability _/entity3_ fits the needs of _entity4_ _C_ semantic-oriented NLP applications _/entity4_ and can be evaluated directly ( independent of an application ) at a good level of _entity5_ human agreement _/entity5_ . Motivated by this _entity6_ semantic criterion _/entity6_ we analyze the empirical quality of _entity7_ _P_ distributional word feature vectors _/entity7_ and its impact on _entity8_ word similarity results _/entity8_ , proposing an objective measure for evaluating _entity9_ feature vector quality _/entity9_ . Finally , a novel _entity10_ feature weighting and selection function _/entity10_ is presented , which yields superior _entity11_ feature vectors _/entity11_ and better _entity12_ word similarity performance _/entity12_ .	NONE entity7 entity4
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ _P_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ _C_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity13 entity16
This paper presents an _entity1_ _C_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ _P_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity4 entity1
We present an operable definition of _entity1_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ discourse model _/entity3_ and _entity4_ _C_ knowledge store _/entity4_ is introduced enabling the _entity5_ _P_ decomposition _/entity5_ and _entity6_ formal representation _/entity6_ of its _entity7_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity5 entity4
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ traditional MT architectural designs _/entity4_ . A new _entity5_ semantic representation _/entity5_ is proposed that uses _entity6_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ interlingua _/entity9_ within a new _entity10_ _P_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ _C_ direct approaches _/entity12_ into a single system .	NONE entity10 entity12
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ phrases _/entity12_ longer than three _entity13_ _P_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ _C_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity13 entity16
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ _C_ delimiters _/entity3_ , which poses a challenge to _entity4_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ _P_ MT _/entity5_ , the widely used approach is to apply a _entity6_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity5 entity3
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ _P_ log-likelihood _/entity24_ under a _entity25_ _C_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity24 entity25
This paper proposes a novel method of building _entity1_ polarity-tagged corpus _/entity1_ from _entity2_ _C_ HTML documents _/entity2_ . The characteristics of this method is that it is fully automatic and can be applied to arbitrary _entity3_ _P_ HTML documents _/entity3_ . The idea behind our method is to utilize certain _entity4_ layout structures _/entity4_ and _entity5_ linguistic pattern _/entity5_ . By using them , we can automatically extract such _entity6_ sentences _/entity6_ that express opinion . In our experiment , the method could construct a _entity7_ corpus _/entity7_ consisting of 126,610 _entity8_ sentences _/entity8_ .	NONE entity3 entity2
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ _P_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ _C_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity5 entity8
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ _P_ natural language interface _/entity2_ to _entity3_ _C_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ sentences _/entity6_ in _entity7_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity2 entity3
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ _C_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ _P_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ phrases _/entity12_ longer than three _entity13_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity11 entity9
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ _P_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ _C_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity8 entity11
This paper describes an _entity1_ _P_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ _C_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity1 entity4
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ _C_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ _P_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity43 entity41
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ _C_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ _P_ lemmatization _/entity32_ more adequate .	NONE entity32 entity30
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ _P_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ _C_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity20 entity22
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ _C_ lexical similarity _/entity15_ of _entity16_ _P_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity16 entity15
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ _C_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ _P_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity5 entity2
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ _P_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ _C_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity7 entity10
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ _C_ spoken corpora _/entity23_ simply in order to help _entity24_ _P_ parsers _/entity24_ .	NONE entity24 entity23
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ _P_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ _C_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity3 entity5
In this paper we present a _entity1_ statistical profile _/entity1_ of the _entity2_ Named Entity task _/entity2_ , a specific _entity3_ information extraction task _/entity3_ for which _entity4_ _C_ corpora _/entity4_ in several _entity5_ languages _/entity5_ are available . Using the _entity6_ results _/entity6_ of the _entity7_ _P_ statistical analysis _/entity7_ , we propose an _entity8_ algorithm _/entity8_ for _entity9_ lower bound estimation _/entity9_ for _entity10_ Named Entity corpora _/entity10_ and discuss the significance of the _entity11_ cross-lingual comparisons _/entity11_ provided by the _entity12_ analysis _/entity12_ .	NONE entity7 entity4
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ _C_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ _P_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity15 entity12
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ _P_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ _C_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity7 entity10
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ _C_ coherence _/entity7_ in _entity8_ _P_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity8 entity7
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ _P_ temporal expressions _/entity2_ in _entity3_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ _C_ temporal expressions _/entity4_ in a novel _entity5_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ expressions _/entity6_ fits well with our _entity7_ constraint-based representation _/entity7_ of time , _entity8_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ baseline _/entity10_ , and compares favorably with some of the closely related work .	NONE entity2 entity4
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ _P_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ _C_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity5 entity7
This paper reports a completed stage of ongoing research at the University of York . Landsbergen 's advocacy of _entity1_ analytical inverses _/entity1_ for _entity2_ compositional syntax rules _/entity2_ encourages the application of _entity3_ Definite Clause Grammar techniques _/entity3_ to the construction of a _entity4_ parser _/entity4_ returning _entity5_ Montague analysis trees _/entity5_ . A _entity6_ parser MDCC _/entity6_ is presented which implements an _entity7_ _C_ augmented Friedman - Warren algorithm _/entity7_ permitting _entity8_ _P_ post referencing _/entity8_ * and interfaces with a language of _entity9_ intenslonal logic translator LILT _/entity9_ so as to display the _entity10_ derivational history _/entity10_ of corresponding _entity11_ reduced IL formulae _/entity11_ . Some familiarity with _entity12_ Montague 's PTQ _/entity12_ and the _entity13_ basic DCG mechanism _/entity13_ is assumed .	NONE entity8 entity7
Computer programs so far have not fared well in _entity1_ _P_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ _C_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity1 entity4
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ _C_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ _P_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity22 entity20
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ _P_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ _C_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity19 entity21
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ _P_ Morphemes _/entity14_ added to a _entity15_ _C_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity14 entity15
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ _C_ sentences _/entity10_ that were randomly selected from the _entity11_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ _P_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity12 entity10
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ _P_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ _C_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity7 entity9
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ _P_ character sets _/entity6_ should be treated differently and the changes between _entity7_ character types _/entity7_ are very important because _entity8_ _C_ Japanese script _/entity8_ has both _entity9_ ideograms _/entity9_ like _entity10_ Chinese _/entity10_ ( _entity11_ kanji _/entity11_ ) and _entity12_ phonograms _/entity12_ like _entity13_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity6 entity8
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ _C_ representation tree _/entity12_ . In the process of _entity13_ _P_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity13 entity12
_entity1_ Chart parsing _/entity1_ is _entity2_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ chart _/entity3_ that works outward from _entity4_ islands _/entity4_ and makes sense of as much of the _entity5_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ _C_ fragments _/entity7_ occur in the _entity8_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ islands _/entity9_ , until possibly completely missing _entity10_ _P_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity10 entity7
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ _P_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ _C_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity8 entity10
We present preliminary results concerning robust techniques for _entity1_ _C_ resolving bridging definite descriptions _/entity1_ . We report our analysis of a collection of 20 _entity2_ Wall Street Journal articles _/entity2_ from the _entity3_ Penn Treebank Corpus _/entity3_ and our experiments with _entity4_ _P_ WordNet _/entity4_ to identify relations between _entity5_ bridging descriptions _/entity5_ and their _entity6_ antecedents _/entity6_ .	NONE entity4 entity1
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ _P_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ _C_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity17 entity20
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ _P_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ _C_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity22 entity24
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ _P_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ _C_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	PART_WHOLE entity6 entity7
This paper considers the problem of automatic assessment of _entity1_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ _P_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ _C_ ranking learning problem _/entity7_ and show that the proposed _entity8_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity4 entity7
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ _P_ insufficient training data _/entity3_ and _entity4_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ _C_ statistical approaches _/entity6_ , which resolve _entity7_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity3 entity6
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ _P_ NE items _/entity21_ . When a _entity22_ _C_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity21 entity22
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ _P_ ungrammatical input _/entity8_ . While _entity9_ _C_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ passives _/entity11_ , _entity12_ relative clauses _/entity12_ and _entity13_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ Plume _/entity16_ to handle _entity17_ passives _/entity17_ , _entity18_ relative clauses _/entity18_ , and _entity19_ interrogatives _/entity19_ in a general manner .	NONE entity8 entity9
In this paper , we describe the _entity1_ pronominal anaphora resolution module _/entity1_ of _entity2_ Lucy _/entity2_ , a portable _entity3_ English understanding system _/entity3_ . The design of this module was motivated by the observation that , although there exist many theories of _entity4_ anaphora resolution _/entity4_ , no one of these theories is complete . Thus we have implemented a _entity5_ _P_ blackboard-like architecture _/entity5_ in which individual _entity6_ _C_ partial theories _/entity6_ can be encoded as separate modules that can interact to propose candidate _entity7_ antecedents _/entity7_ and to evaluate each other 's proposals .	NONE entity5 entity6
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ _C_ English _/entity7_ . We show how the limited _entity8_ _P_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity8 entity7
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ _C_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ _P_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity17 entity15
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ _C_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ _P_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity32 entity29
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ _P_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ _C_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity5 entity7
Porting a _entity1_ Natural Language Processing ( NLP ) system _/entity1_ to a _entity2_ _P_ new domain _/entity2_ remains one of the bottlenecks in _entity3_ _C_ syntactic parsing _/entity3_ , because of the amount of effort required to fix gaps in the _entity4_ lexicon _/entity4_ , and to attune the _entity5_ existing grammar _/entity5_ to the idiosyncracies of the _entity6_ new sublanguage _/entity6_ . This paper shows how the process of fitting a _entity7_ lexicalized grammar _/entity7_ to a _entity8_ domain _/entity8_ can be automated to a great extent by using a _entity9_ hybrid system _/entity9_ that combines _entity10_ traditional knowledge-based techniques _/entity10_ with a _entity11_ corpus-based approach _/entity11_ .	NONE entity2 entity3
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ _P_ phone duration models _/entity24_ , and _entity25_ _C_ sex-dependent models _/entity25_ .	NONE entity24 entity25
_entity1_ Unification _/entity1_ is often the appropriate method for expressing _entity2_ relations _/entity2_ between _entity3_ representations _/entity3_ in the form of _entity4_ feature structures _/entity4_ ; however , there are circumstances in which a different approach is desirable . A _entity5_ _C_ declarative formalism _/entity5_ is presented which permits direct _entity6_ mappings _/entity6_ of one _entity7_ _P_ feature structure _/entity7_ into another , and illustrative examples are given of its application to areas of current interest .	NONE entity7 entity5
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ _P_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ _C_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity10 entity12
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ _P_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ _C_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity3 entity5
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ _C_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ _P_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity10 entity7
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ character sets _/entity6_ should be treated differently and the changes between _entity7_ character types _/entity7_ are very important because _entity8_ Japanese script _/entity8_ has both _entity9_ ideograms _/entity9_ like _entity10_ Chinese _/entity10_ ( _entity11_ kanji _/entity11_ ) and _entity12_ _C_ phonograms _/entity12_ like _entity13_ _P_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity13 entity12
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ multiple speakers _/entity5_ without _entity6_ explicit speaker enrollment _/entity6_ . Combining _entity7_ _P_ speech recognition _/entity7_ and _entity8_ _C_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity7 entity8
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ _C_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ _P_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity23 entity20
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ _P_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ _C_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity2 entity5
The applicability of many current _entity1_ _P_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ _C_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ supervised methods _/entity13_ on 50 _entity14_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity1 entity3
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ _C_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ _P_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity11 entity8
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ _P_ LFG _/entity5_ , a _entity6_ _C_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity5 entity6
Porting a _entity1_ Natural Language Processing ( NLP ) system _/entity1_ to a _entity2_ new domain _/entity2_ remains one of the bottlenecks in _entity3_ syntactic parsing _/entity3_ , because of the amount of effort required to fix gaps in the _entity4_ lexicon _/entity4_ , and to attune the _entity5_ existing grammar _/entity5_ to the idiosyncracies of the _entity6_ new sublanguage _/entity6_ . This paper shows how the process of fitting a _entity7_ lexicalized grammar _/entity7_ to a _entity8_ domain _/entity8_ can be automated to a great extent by using a _entity9_ hybrid system _/entity9_ that combines _entity10_ _C_ traditional knowledge-based techniques _/entity10_ with a _entity11_ _P_ corpus-based approach _/entity11_ .	NONE entity11 entity10
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ _P_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ _C_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity9 entity11
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ _C_ phrase translations _/entity9_ in our _entity10_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ _P_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity12 entity9
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ _C_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ _P_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity6 entity3
In order to meet the needs of a publication of papers in English , many systems to run off texts have been developed . In this paper , we report a system _entity1_ FROFF _/entity1_ which can make a fair copy of not only texts but also graphs and tables indispensable to our papers . Its selection of _entity2_ fonts _/entity2_ , specification of _entity3_ _C_ character _/entity3_ size are dynamically changeable , and the _entity4_ typing location _/entity4_ can be also changed in lateral or longitudinal directions . Each _entity5_ _P_ character _/entity5_ has its own width and a line length is counted by the sum of each _entity6_ character _/entity6_ . By using commands or _entity7_ rules _/entity7_ which are defined to facilitate the construction of format expected or some _entity8_ mathematical expressions _/entity8_ , elaborate and pretty documents can be successfully obtained .	NONE entity5 entity3
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ LFG _/entity4_ and _entity5_ UDRS _/entity5_ formalisms . It provides a _entity6_ _P_ model theoretic interpretation _/entity6_ and an _entity7_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ _C_ f-structures _/entity9_ through the _entity10_ translation images _/entity10_ of _entity11_ f-structures _/entity11_ as _entity12_ UDRSs _/entity12_ .	MODEL-FEATURE entity6 entity9
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ _P_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ _C_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity3 entity4
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ _C_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ _P_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity14 entity12
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ _C_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ _P_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity11 entity9
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ _P_ Japanese _/entity9_ back to _entity10_ _C_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity9 entity10
_entity1_ Link detection _/entity1_ has been regarded as a core technology for the _entity2_ Topic Detection and Tracking tasks _/entity2_ of _entity3_ new event detection _/entity3_ . In this paper we formulate _entity4_ story link detection _/entity4_ and _entity5_ new event detection _/entity5_ as _entity6_ information retrieval task _/entity6_ and hypothesize on the impact of _entity7_ precision _/entity7_ and _entity8_ _C_ recall _/entity8_ on both systems . Motivated by these arguments , we introduce a number of new performance enhancing techniques including _entity9_ part of speech tagging _/entity9_ , new _entity10_ similarity measures _/entity10_ and expanded _entity11_ _P_ stop lists _/entity11_ . Experimental results validate our hypothesis .	NONE entity11 entity8
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ _C_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ _P_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity7 entity4
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ _C_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ _P_ constraint-based parser/generator _/entity22_ .	NONE entity22 entity20
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ _C_ duration model _/entity3_ has reduced the _entity4_ error rate _/entity4_ by about 10 % in both _entity5_ triphone and semiphone systems _/entity5_ . A new _entity6_ _P_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity6 entity3
We describe a novel technique and implemented system for constructing a _entity1_ subcategorization dictionary _/entity1_ from _entity2_ textual corpora _/entity2_ . Each _entity3_ dictionary entry _/entity3_ encodes the _entity4_ relative frequency of occurrence _/entity4_ of a comprehensive set of _entity5_ subcategorization classes _/entity5_ for _entity6_ English _/entity6_ . An initial experiment , on a sample of 14 _entity7_ verbs _/entity7_ which exhibit _entity8_ multiple complementation patterns _/entity8_ , demonstrates that the technique achieves _entity9_ _P_ accuracy _/entity9_ comparable to previous approaches , which are all limited to a highly restricted set of _entity10_ subcategorization classes _/entity10_ . We also demonstrate that a _entity11_ subcategorization dictionary _/entity11_ built with the system improves the _entity12_ _C_ accuracy _/entity12_ of a _entity13_ parser _/entity13_ by an appreciable amount	NONE entity9 entity12
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to disambiguate various relations between _entity2_ named entities _/entity2_ by use of various _entity3_ lexical and syntactic features _/entity3_ from the _entity4_ _P_ contexts _/entity4_ . It works by calculating _entity5_ eigenvectors _/entity5_ of an _entity6_ adjacency graph _/entity6_ 's _entity7_ _C_ Laplacian _/entity7_ to recover a _entity8_ submanifold _/entity8_ of data from a _entity9_ high dimensionality space _/entity9_ and then performing _entity10_ cluster number estimation _/entity10_ on the _entity11_ eigenvectors _/entity11_ . Experiment results on _entity12_ ACE corpora _/entity12_ show that this _entity13_ spectral clustering based approach _/entity13_ outperforms the other _entity14_ clustering methods _/entity14_ .	NONE entity4 entity7
In this paper we present _entity1_ ONTOSCORE _/entity1_ , a system for scoring sets of _entity2_ concepts _/entity2_ on the basis of an _entity3_ ontology _/entity3_ . We apply our system to the task of _entity4_ scoring _/entity4_ alternative _entity5_ speech recognition hypotheses ( SRH ) _/entity5_ in terms of their _entity6_ semantic coherence _/entity6_ . We conducted an _entity7_ annotation experiment _/entity7_ and showed that _entity8_ human annotators _/entity8_ can reliably differentiate between semantically coherent and incoherent _entity9_ speech recognition hypotheses _/entity9_ . An evaluation of our system against the _entity10_ annotated data _/entity10_ shows that , it successfully classifies 73.2 % in a _entity11_ German corpus _/entity11_ of 2.284 _entity12_ _C_ SRHs _/entity12_ as either coherent or incoherent ( given a _entity13_ _P_ baseline _/entity13_ of 54.55 % ) .	NONE entity13 entity12
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ _P_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ _C_ baseline _/entity20_ on all three aspects .	NONE entity19 entity20
In this paper , we reported experiments of _entity1_ unsupervised automatic acquisition _/entity1_ of _entity2_ Italian and English verb subcategorization frames ( SCFs ) _/entity2_ from _entity3_ general and domain corpora _/entity3_ . The proposed technique operates on _entity4_ syntactically shallow-parsed corpora _/entity4_ on the basis of a limited number of _entity5_ search heuristics _/entity5_ not relying on any previous _entity6_ lexico-syntactic knowledge _/entity6_ about _entity7_ SCFs _/entity7_ . Although preliminary , reported results are in line with _entity8_ _P_ state-of-the-art lexical acquisition systems _/entity8_ . The issue of whether _entity9_ _C_ verbs _/entity9_ sharing similar _entity10_ SCFs distributions _/entity10_ happen to share _entity11_ similar semantic properties _/entity11_ as well was also explored by clustering _entity12_ verbs _/entity12_ that share _entity13_ frames _/entity13_ with the same _entity14_ distribution _/entity14_ using the _entity15_ Minimum Description Length Principle ( MDL ) _/entity15_ . First experiments in this direction were carried out on _entity16_ Italian verbs _/entity16_ with encouraging results .	NONE entity8 entity9
The _entity1_ _P_ GE NLToolset _/entity1_ is a set of _entity2_ _C_ text interpretation tools _/entity2_ designed to be easily adapted to new _entity3_ domains _/entity3_ . This report summarizes the system and its performance on the _entity4_ MUC-4 task _/entity4_ .	NONE entity1 entity2
The paper proposes and empirically motivates an integration of _entity1_ supervised learning _/entity1_ with _entity2_ unsupervised learning _/entity2_ to deal with human biases in _entity3_ summarization _/entity3_ . In particular , we explore the use of _entity4_ probabilistic decision tree _/entity4_ within the clustering framework to account for the variation as well as regularity in _entity5_ _C_ human created summaries _/entity5_ . The _entity6_ _P_ corpus _/entity6_ of human created extracts is created from a _entity7_ newspaper corpus _/entity7_ and used as a test set . We build _entity8_ probabilistic decision trees _/entity8_ of different flavors and integrate each of them with the clustering framework . Experiments with the _entity9_ corpus _/entity9_ demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone .	NONE entity6 entity5
Towards deep analysis of _entity1_ _C_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ class-oriented framework _/entity2_ for collecting _entity3_ _P_ paraphrase examples _/entity3_ , in which _entity4_ sentential paraphrases _/entity4_ are collected for each _entity5_ paraphrase class _/entity5_ separately by means of _entity6_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ cost-efficiency _/entity9_ , _entity10_ exhaustiveness _/entity10_ , and _entity11_ reliability _/entity11_ .	NONE entity3 entity1
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ _C_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ _P_ Japanese _/entity20_ .	NONE entity20 entity17
Although adequate models of _entity1_ human language _/entity1_ for _entity2_ _C_ syntactic analysis _/entity2_ and _entity3_ semantic interpretation _/entity3_ are of at least _entity4_ _P_ context-free complexity _/entity4_ , for applications such as _entity5_ speech processing _/entity5_ in which speed is important _entity6_ finite-state models _/entity6_ are often preferred . These requirements may be reconciled by using the more complex _entity7_ grammar _/entity7_ to automatically derive a _entity8_ finite-state approximation _/entity8_ which can then be used as a filter to guide _entity9_ speech recognition _/entity9_ or to reject many hypotheses at an early stage of processing . A method is presented here for calculating such _entity10_ finite-state approximations _/entity10_ from _entity11_ context-free grammars _/entity11_ . It is essentially different from the algorithm introduced by Pereira and Wright ( 1991 ; 1996 ) , is faster in some cases , and has the advantage of being open-ended and adaptable .	NONE entity4 entity2
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ phrases _/entity2_ are represented as packed sets of _entity3_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ _C_ statistical ranking _/entity6_ than a previous approach to _entity7_ _P_ statistical generation _/entity7_ . An efficient _entity8_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ lattice-based approach _/entity9_ .	NONE entity7 entity6
Dividing _entity1_ sentences _/entity1_ in _entity2_ _P_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ _C_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity2 entity4
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ _P_ data representation _/entity13_ , our _entity14_ _C_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity13 entity14
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ _C_ speech _/entity24_ from the new ( target ) _entity25_ _P_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity25 entity24
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ _P_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ _C_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity2 entity5
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ _C_ CFG parsers _/entity3_ running in _entity4_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ _P_ grammar G _/entity5_ and a _entity6_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ CFG parsing _/entity11_ : a fast , practical _entity12_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity5 entity3
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ _P_ event structure _/entity13_ of the preceeding _entity14_ _C_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	MODEL-FEATURE entity13 entity14
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ language independent modules _/entity3_ of the _entity4_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ _C_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ _P_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity9 entity6
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ _C_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ _P_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity6 entity4
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ _C_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ _P_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity9 entity7
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ _P_ KL-Conc _/entity13_ , represents the _entity14_ _C_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	MODEL-FEATURE entity13 entity14
We investigate independent and relevant event-based extractive _entity1_ _P_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ events _/entity2_ are defined as _entity3_ _C_ event terms _/entity3_ and _entity4_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ documents _/entity9_ . Experimental results are encouraging .	NONE entity1 entity3
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ _C_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ _P_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity13 entity11
This paper describes three relatively _entity1_ domain-independent capabilities _/entity1_ recently added to the _entity2_ Paramax spoken language understanding system _/entity2_ : _entity3_ non-monotonic reasoning _/entity3_ , _entity4_ implicit reference resolution _/entity4_ , and _entity5_ database query paraphrase _/entity5_ . In addition , we discuss the results of the _entity6_ February 1992 ATIS benchmark tests _/entity6_ . We describe a variation on the _entity7_ _P_ standard evaluation metric _/entity7_ which provides a more tightly controlled measure of progress . Finally , we briefly describe an experiment which we have done in extending the _entity8_ n-best speech/language integration architecture _/entity8_ to improving _entity9_ _C_ OCR _/entity9_ _entity10_ accuracy _/entity10_ .	NONE entity7 entity9
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ _P_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ _C_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity3 entity5
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ _C_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ _P_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity11 entity9
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ _C_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ _P_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity6 entity3
We provide a _entity1_ _C_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ _P_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity4 entity1
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ _P_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ _C_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity12 entity13
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ duration model _/entity3_ has reduced the _entity4_ _P_ error rate _/entity4_ by about 10 % in both _entity5_ _C_ triphone and semiphone systems _/entity5_ . A new _entity6_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity4 entity5
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ _P_ wh-movement _/entity16_ , the _entity17_ _C_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity16 entity17
_entity1_ Topical blog post retrieval _/entity1_ is the task of ranking _entity2_ blog posts _/entity2_ with respect to their _entity3_ relevance _/entity3_ for a given _entity4_ topic _/entity4_ . To improve _entity5_ _C_ topical blog post retrieval _/entity5_ we incorporate _entity6_ textual credibility indicators _/entity6_ in the _entity7_ retrieval process _/entity7_ . We consider two groups of _entity8_ _P_ indicators _/entity8_ : post level ( determined using information about individual _entity9_ blog posts _/entity9_ only ) and blog level ( determined using information from the underlying _entity10_ blogs _/entity10_ ) . We describe how to estimate these _entity11_ indicators _/entity11_ and how to integrate them into a _entity12_ retrieval approach _/entity12_ based on _entity13_ language models _/entity13_ . Experiments on the _entity14_ TREC Blog track test set _/entity14_ show that both groups of _entity15_ credibility indicators _/entity15_ significantly improve _entity16_ retrieval effectiveness _/entity16_ ; the best performance is achieved when combining them .	NONE entity8 entity5
This paper describes a recently collected _entity1_ spoken language corpus _/entity1_ for the _entity2_ _C_ ATIS ( Air Travel Information System ) domain _/entity2_ . This data collection effort has been co-ordinated by _entity3_ _P_ MADCOW ( Multi-site ATIS Data COllection Working group ) _/entity3_ . We summarize the motivation for this effort , the goals , the implementation of a _entity4_ multi-site data collection paradigm _/entity4_ , and the accomplishments of _entity5_ MADCOW _/entity5_ in monitoring the _entity6_ collection _/entity6_ and distribution of 12,000 _entity7_ utterances _/entity7_ of _entity8_ spontaneous speech _/entity8_ from five sites for use in a _entity9_ multi-site common evaluation of speech , natural language and spoken language _/entity9_ .	NONE entity3 entity2
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ _C_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ _P_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity12 entity9
This paper investigates some _entity1_ computational problems _/entity1_ associated with _entity2_ probabilistic translation models _/entity2_ that have recently been adopted in the literature on _entity3_ _C_ machine translation _/entity3_ . These _entity4_ models _/entity4_ can be viewed as pairs of _entity5_ _P_ probabilistic context-free grammars _/entity5_ working in a 'synchronous ' way . Two _entity6_ hardness _/entity6_ results for the class _entity7_ NP _/entity7_ are reported , along with an _entity8_ exponential time lower-bound _/entity8_ for certain classes of algorithms that are currently used in the literature .	NONE entity5 entity3
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ _P_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ _C_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity17 entity19
This study presents a _entity1_ _C_ method to automatically acquire paraphrases _/entity1_ using _entity2_ _P_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	USAGE entity2 entity1
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ human-computer interaction _/entity6_ . We analyzed _entity7_ _P_ eye gaze _/entity7_ , _entity8_ head nods _/entity8_ and _entity9_ _C_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity7 entity9
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ CCRs _/entity3_ are _entity4_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ _P_ categories _/entity5_ in _entity6_ _C_ local trees _/entity6_ which allow the _entity7_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	NONE entity5 entity6
_entity1_ Chart parsing _/entity1_ is _entity2_ directional _/entity2_ in the sense that it works from the starting point ( usually the beginning of the sentence ) extending its activity usually in a rightward manner . We shall introduce the concept of a _entity3_ chart _/entity3_ that works outward from _entity4_ islands _/entity4_ and makes sense of as much of the _entity5_ sentence _/entity5_ as it is actually possible , and after that will lead to predictions of missing _entity6_ fragments _/entity6_ . So , for any place where the easily identifiable _entity7_ fragments _/entity7_ occur in the _entity8_ _P_ sentence _/entity8_ , the process will extend to both the left and the right of the _entity9_ islands _/entity9_ , until possibly completely missing _entity10_ _C_ fragments _/entity10_ are reached . At that point , by virtue of the fact that both a left and a right context were found , _entity11_ heuristics _/entity11_ can be introduced that predict the nature of the missing _entity12_ fragments _/entity12_ .	NONE entity8 entity10
How to obtain _entity1_ hierarchical relations _/entity1_ ( e.g . _entity2_ superordinate -hyponym relation _/entity2_ , _entity3_ synonym relation _/entity3_ ) is one of the most important problems for _entity4_ _C_ thesaurus construction _/entity4_ . A pilot system for extracting these _entity5_ _P_ relations _/entity5_ automatically from an ordinary _entity6_ Japanese language dictionary _/entity6_ ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given . The features of the _entity7_ definition sentences _/entity7_ in the _entity8_ dictionary _/entity8_ , the mechanical extraction of the _entity9_ hierarchical relations _/entity9_ and the estimation of the results are discussed .	NONE entity5 entity4
In the second year of _entity1_ evaluations _/entity1_ of the _entity2_ ARPA HLT Machine Translation ( MT ) Initiative _/entity2_ , methodologies developed and tested in 1992 were applied to the _entity3_ _C_ 1993 MT test runs _/entity3_ . The current methodology optimizes the inherently _entity4_ _P_ subjective judgments _/entity4_ on _entity5_ translation accuracy and quality _/entity5_ by channeling the _entity6_ judgments _/entity6_ of _entity7_ non-translators _/entity7_ into many _entity8_ data points _/entity8_ which reflect both the comparison of the _entity9_ performance _/entity9_ of the _entity10_ research MT systems _/entity10_ with _entity11_ production MT systems _/entity11_ and against the _entity12_ performance _/entity12_ of _entity13_ novice translators _/entity13_ . This paper discusses the three _entity14_ evaluation methods _/entity14_ used in the _entity15_ 1993 evaluation _/entity15_ , the results of the evaluations , and preliminary characterizations of the _entity16_ Winter 1994 evaluation _/entity16_ , now underway . The efforts under discussion focus on measuring the progress of _entity17_ core MT technology _/entity17_ and increasing the sensitivity and _entity18_ portability _/entity18_ of _entity19_ MT evaluation methodology _/entity19_ .	NONE entity4 entity3
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ _C_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ _P_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity5 entity3
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ _C_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ _P_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity17 entity14
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ _C_ words _/entity9_ drawn from a new _entity10_ _P_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ WSD accuracy _/entity15_ .	NONE entity10 entity9
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ _C_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ _P_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity5 entity2
_entity1_ Pipelined Natural Language Generation ( NLG ) systems _/entity1_ have grown increasingly complex as _entity2_ _P_ architectural modules _/entity2_ were added to support _entity3_ _C_ language functionalities _/entity3_ such as _entity4_ referring expressions _/entity4_ , _entity5_ lexical choice _/entity5_ , and _entity6_ revision _/entity6_ . This has given rise to discussions about the relative placement of these new _entity7_ modules _/entity7_ in the overall _entity8_ architecture _/entity8_ . Recent work on another aspect of _entity9_ multi-paragraph text _/entity9_ , _entity10_ discourse markers _/entity10_ , indicates it is time to consider where a _entity11_ discourse marker insertion algorithm _/entity11_ fits in . We present examples which suggest that in a _entity12_ pipelined NLG architecture _/entity12_ , the best approach is to strongly tie it to a _entity13_ revision component _/entity13_ . Finally , we evaluate the approach in a working _entity14_ multi-page system _/entity14_ .	USAGE entity2 entity3
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ performance _/entity5_ of using _entity6_ ASR output _/entity6_ as opposed to _entity7_ human transcription _/entity7_ . Examination of the effect of _entity8_ _P_ features _/entity8_ shows that _entity9_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ _C_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity8 entity10
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ _C_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ _P_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity17 entity15
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ _P_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ _C_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity3 entity4
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ LFG _/entity4_ and _entity5_ UDRS _/entity5_ formalisms . It provides a _entity6_ model theoretic interpretation _/entity6_ and an _entity7_ _C_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ f-structures _/entity9_ through the _entity10_ _P_ translation images _/entity10_ of _entity11_ f-structures _/entity11_ as _entity12_ UDRSs _/entity12_ .	NONE entity10 entity7
This paper describes a _entity1_ characters-based Chinese collocation system _/entity1_ and discusses the advantages of it over a traditional _entity2_ word-based system _/entity2_ . Since _entity3_ wordbreaks _/entity3_ are not conventionally marked in _entity4_ Chinese text corpora _/entity4_ , a _entity5_ character-based collocation system _/entity5_ has the dual advantages of avoiding _entity6_ _P_ pre-processing distortion _/entity6_ and directly accessing _entity7_ _C_ sub-lexical information _/entity7_ . Furthermore , _entity8_ word-based collocational properties _/entity8_ can be obtained through an auxiliary module of _entity9_ automatic segmentation _/entity9_ .	NONE entity6 entity7
A _entity1_ flexible parser _/entity1_ can deal with input that deviates from its _entity2_ grammar _/entity2_ , in addition to input that conforms to it . Ideally , such a _entity3_ parser _/entity3_ will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , _entity4_ correction _/entity4_ will be possible , but only to within a range of ambiguous possibilities . This paper is concerned with such ambiguous situations , and with making it as easy as possible for the _entity5_ ambiguity _/entity5_ to be resolved through consultation with the user of the _entity6_ parser _/entity6_ - we presume interactive use . We show the importance of asking the user for clarification in as focused a way as possible . _entity7_ Focused interaction _/entity7_ of this kind is facilitated by a _entity8_ construction-specific approach _/entity8_ to _entity9_ flexible parsing _/entity9_ , with _entity10_ _C_ specialized parsing techniques _/entity10_ for each type of _entity11_ construction _/entity11_ , and specialized _entity12_ ambiguity representations _/entity12_ for each type of _entity13_ _P_ ambiguity _/entity13_ that a particular _entity14_ construction _/entity14_ can give rise to . A _entity15_ construction-specific approach _/entity15_ also aids in _entity16_ task-specific language development _/entity16_ by allowing a _entity17_ language definition _/entity17_ that is natural in terms of the _entity18_ task domain _/entity18_ to be interpreted directly without compilation into a _entity19_ uniform grammar formalism _/entity19_ , thus greatly speeding the _entity20_ testing _/entity20_ of changes to the _entity21_ language definition _/entity21_ .	NONE entity13 entity10
The reality of _entity1_ analogies between words _/entity1_ is refuted by noone ( e.g. , I walked is to to walk as I laughed is to to laugh , noted I walked : to walk : : I laughed : to laugh ) . But _entity2_ computational linguists _/entity2_ seem to be quite dubious about _entity3_ _C_ analogies between sentences _/entity3_ : they would not be enough numerous to be of any use . We report experiments conducted on a _entity4_ multilingual corpus _/entity4_ to estimate the number of _entity5_ analogies _/entity5_ among the _entity6_ _P_ sentences _/entity6_ that it contains . We give two estimates , a lower one and a higher one . As an _entity7_ analogy _/entity7_ must be valid on the level of _entity8_ form _/entity8_ as well as on the level of _entity9_ meaning _/entity9_ , we relied on the idea that _entity10_ translation _/entity10_ should preserve _entity11_ meaning _/entity11_ to test for similar _entity12_ meanings _/entity12_ .	NONE entity6 entity3
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ _C_ model _/entity2_ that a _entity3_ _P_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity3 entity2
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ _P_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ _C_ discourse structure _/entity12_ . A _entity13_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity9 entity12
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ _C_ source dependency parse _/entity11_ onto the target _entity12_ _P_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity12 entity11
This paper considers the problem of automatic assessment of _entity1_ _P_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ _C_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity1 entity3
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ _C_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ _P_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity6 entity4
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ _C_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ _P_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity16 entity13
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ _C_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ _P_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity36 entity33
Following recent developments in the _entity1_ _P_ automatic evaluation _/entity1_ of _entity2_ _C_ machine translation _/entity2_ and _entity3_ document summarization _/entity3_ , we present a similar approach , implemented in a measure called _entity4_ POURPRE _/entity4_ , for _entity5_ automatically evaluating answers to definition questions _/entity5_ . Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response . The lack of automatic methods for _entity6_ scoring system output _/entity6_ is an impediment to progress in the field , which we address with this work . Experiments with the _entity7_ TREC 2003 and TREC 2004 QA tracks _/entity7_ indicate that _entity8_ rankings _/entity8_ produced by our metric correlate highly with _entity9_ official rankings _/entity9_ , and that _entity10_ POURPRE _/entity10_ outperforms direct application of existing metrics .	USAGE entity1 entity2
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ _C_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ _P_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity8 entity6
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ delimiters _/entity3_ , which poses a challenge to _entity4_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ _P_ MT _/entity5_ , the widely used approach is to apply a _entity6_ _C_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity5 entity6
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ _C_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ _P_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity7 entity4
In this study , we propose a _entity1_ _P_ knowledge-independent method _/entity1_ for aligning _entity2_ _C_ terms _/entity2_ and thus extracting _entity3_ translations _/entity3_ from a _entity4_ small , domain-specific corpus _/entity4_ consisting of _entity5_ parallel English and Chinese court judgments _/entity5_ from Hong Kong . With a _entity6_ sentence-aligned corpus _/entity6_ , _entity7_ translation equivalences _/entity7_ are suggested by analysing the _entity8_ frequency profiles _/entity8_ of _entity9_ parallel concordances _/entity9_ . The method overcomes the limitations of _entity10_ conventional statistical methods _/entity10_ which require _entity11_ large corpora _/entity11_ to be effective , and _entity12_ lexical approaches _/entity12_ which depend on existing _entity13_ bilingual dictionaries _/entity13_ . Pilot testing on a _entity14_ parallel corpus _/entity14_ of about 113K _entity15_ Chinese words _/entity15_ and 120K _entity16_ English words _/entity16_ gives an encouraging 85 % _entity17_ precision _/entity17_ and 45 % _entity18_ recall _/entity18_ . Future work includes fine-tuning the _entity19_ algorithm _/entity19_ upon the analysis of the errors , and acquiring a _entity20_ translation lexicon _/entity20_ for _entity21_ legal terminology _/entity21_ by filtering out _entity22_ general terms _/entity22_ .	NONE entity1 entity2
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ _C_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ _P_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity6 entity3
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ _P_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ _C_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity10 entity12
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ _P_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ _C_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity5 entity7
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ _P_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ _C_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity18 entity20
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ _C_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ _P_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity12 entity11
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ _C_ learning _/entity7_ . We introduced a new _entity8_ _P_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity8 entity7
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ _P_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ _C_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity12 entity15
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ _P_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ _C_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity8 entity11
Recent advances in _entity1_ _P_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ _C_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity1 entity4
This paper describes a _entity1_ computational model _/entity1_ of _entity2_ word segmentation _/entity2_ and presents simulation results on _entity3_ realistic acquisition _/entity3_ . In particular , we explore the capacity and limitations of _entity4_ statistical learning mechanisms _/entity4_ that have recently gained prominence in _entity5_ _P_ cognitive psychology _/entity5_ and _entity6_ _C_ linguistics _/entity6_ .	NONE entity5 entity6
A research program is described in which a particular _entity1_ representational format for meaning _/entity1_ is tested as broadly as possible . In this format , developed by the LNR research group at The University of California at San Diego , _entity2_ verbs _/entity2_ are represented as interconnected sets of _entity3_ subpredicates _/entity3_ . These _entity4_ _C_ subpredicates _/entity4_ may be thought of as the almost inevitable _entity5_ _P_ inferences _/entity5_ that a _entity6_ listener _/entity6_ makes when a _entity7_ verb _/entity7_ is used in a _entity8_ sentence _/entity8_ . They confer a _entity9_ meaning structure _/entity9_ on the _entity10_ sentence _/entity10_ in which the _entity11_ verb _/entity11_ is used .	NONE entity5 entity4
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ _C_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ _P_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity15 entity12
A method for _entity1_ _P_ error correction _/entity1_ of _entity2_ _C_ ill-formed input _/entity2_ is described that acquires _entity3_ dialogue patterns _/entity3_ in typical usage and uses these _entity4_ patterns _/entity4_ to predict new inputs . _entity5_ Error correction _/entity5_ is done by strongly biasing _entity6_ parsing _/entity6_ toward expected _entity7_ meanings _/entity7_ unless clear evidence from the input shows the current _entity8_ sentence _/entity8_ is not expected . A _entity9_ dialogue acquisition and tracking algorithm _/entity9_ is presented along with a description of its _entity10_ implementation _/entity10_ in a _entity11_ voice interactive system _/entity11_ . A series of tests are described that show the power of the _entity12_ error correction methodology _/entity12_ when _entity13_ stereotypic dialogue _/entity13_ occurs .	USAGE entity1 entity2
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ _P_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ _C_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity21 entity23
This paper discusses a _entity1_ decision-tree approach _/entity1_ to the problem of assigning _entity2_ _C_ probabilities _/entity2_ to _entity3_ _P_ words _/entity3_ following a given _entity4_ text _/entity4_ . In contrast with previous _entity5_ decision-tree language model attempts _/entity5_ , an algorithm for selecting _entity6_ nearly optimal questions _/entity6_ is considered . The model is to be tested on a standard task , _entity7_ The Wall Street Journal _/entity7_ , allowing a fair comparison with the well-known _entity8_ tri-gram model _/entity8_ .	NONE entity3 entity2
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ _P_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ _C_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity4 entity6
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ _P_ linguistic structure _/entity12_ consists of segments of the _entity13_ _C_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity12 entity13
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ _C_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ _P_ signal-based document processing functionality _/entity15_ .	NONE entity15 entity12
The project presented here is a part of a long term research program aiming at a full _entity1_ lexicon grammar for Polish ( SyntLex ) _/entity1_ . The main of this project is _entity2_ computer-assisted acquisition and morpho-syntactic description of verb-noun collocations _/entity2_ in _entity3_ Polish _/entity3_ . We present methodology and resources obtained in three main project phases which are : _entity4_ dictionary-based acquisition _/entity4_ of _entity5_ collocation lexicon _/entity5_ , feasibility study for _entity6_ _P_ corpus-based lexicon enlargement _/entity6_ phase , _entity7_ corpus-based lexicon enlargement _/entity7_ and _entity8_ collocation description _/entity8_ . In this paper we focus on the results of the third phase . The presented here _entity9_ _C_ corpus-based approach _/entity9_ permitted us to triple the size the _entity10_ verb-noun collocation dictionary for Polish _/entity10_ . In the paper we describe the _entity11_ SyntLex Dictionary of Collocations _/entity11_ and announce some future research intended to be a separate project continuation .	NONE entity6 entity9
_entity1_ Statistical machine translation ( SMT ) _/entity1_ is currently one of the hot spots in _entity2_ natural language processing _/entity2_ . Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that _entity3_ SMT _/entity3_ gives competitive results to _entity4_ rule-based translation systems _/entity4_ , requiring significantly less development time . This is particularly important when building _entity5_ translation systems _/entity5_ for new _entity6_ language pairs _/entity6_ or new _entity7_ domains _/entity7_ . This workshop is intended to give an introduction to _entity8_ statistical machine translation _/entity8_ with a focus on practical considerations . Participants should be able , after attending this workshop , to set out building an _entity9_ SMT system _/entity9_ themselves and achieving good _entity10_ baseline results _/entity10_ in a short time . The tutorial will cover the basics of _entity11_ SMT _/entity11_ : Theory will be put into practice . _entity12_ STTK _/entity12_ , a _entity13_ statistical machine translation tool kit _/entity13_ , will be introduced and used to build a working _entity14_ translation system _/entity14_ . _entity15_ STTK _/entity15_ has been developed by the presenter and co-workers over a number of years and is currently used as the basis of _entity16_ CMU 's SMT system _/entity16_ . It has also successfully been coupled with _entity17_ _P_ rule-based and example based machine translation modules _/entity17_ to build a _entity18_ _C_ multi engine machine translation system _/entity18_ . The _entity19_ source code _/entity19_ of the _entity20_ tool kit _/entity20_ will be made available .	NONE entity17 entity18
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ _P_ editor _/entity3_ was to provide an adequate tool to help _entity4_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ _C_ linguistic theory _/entity6_ . If we want valuable _entity7_ lexicons _/entity7_ and _entity8_ grammars _/entity8_ to achieve complex _entity9_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity3 entity6
The multiplicative fragment of _entity1_ _P_ linear logic _/entity1_ has found a number of applications in _entity2_ _C_ computational linguistics _/entity2_ : in the _entity3_ `` glue language '' _/entity3_ approach to _entity4_ LFG semantics _/entity4_ , and in the formulation and _entity5_ parsing _/entity5_ of various _entity6_ categorial grammars _/entity6_ . These applications call for efficient deduction methods . Although a number of deduction methods for _entity7_ multiplicative linear logic _/entity7_ are known , none of them are tabular methods , which bring a substantial efficiency gain by avoiding redundant computation ( cf . chart methods in _entity8_ CFG parsing _/entity8_ ) : this paper presents such a method , and discusses its use in relation to the above applications .	USAGE entity1 entity2
_entity1_ _P_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ _C_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity1 entity3
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ _C_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ _P_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity21 entity18
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ _P_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ _C_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	NONE entity2 entity4
For _entity1_ _P_ intelligent interactive systems _/entity1_ to communicate with _entity2_ humans _/entity2_ in a natural manner , they must have knowledge about the _entity3_ _C_ system users _/entity3_ . This paper explores the role of _entity4_ user modeling _/entity4_ in such _entity5_ systems _/entity5_ . It begins with a characterization of what a _entity6_ user model _/entity6_ is and how it can be used . The types of information that a _entity7_ user model _/entity7_ may be required to keep about a _entity8_ user _/entity8_ are then identified and discussed . _entity9_ User models _/entity9_ themselves can vary greatly depending on the requirements of the situation and the implementation , so several dimensions along which they can be classified are presented . Since acquiring the knowledge for a _entity10_ user model _/entity10_ is a fundamental problem in _entity11_ user modeling _/entity11_ , a section is devoted to this topic . Next , the benefits and costs of implementing a _entity12_ user modeling component _/entity12_ for a system are weighed in light of several aspects of the _entity13_ interaction requirements _/entity13_ that may be imposed by the system . Finally , the current state of research in _entity14_ user modeling _/entity14_ is summarized , and future research topics that must be addressed in order to achieve powerful , general _entity15_ user modeling systems _/entity15_ are assessed .	NONE entity1 entity3
This paper proposes that _entity1_ _P_ sentence analysis _/entity1_ should be treated as _entity2_ defeasible reasoning _/entity2_ , and presents such a treatment for _entity3_ _C_ Japanese sentence analyses _/entity3_ using an _entity4_ argumentation system _/entity4_ by Konolige , which is a _entity5_ formalization _/entity5_ of _entity6_ defeasible reasoning _/entity6_ , that includes _entity7_ arguments _/entity7_ and _entity8_ defeat rules _/entity8_ that capture _entity9_ defeasibility _/entity9_ .	NONE entity1 entity3
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ _C_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ _P_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity12 entity9
We give an analysis of _entity1_ ellipsis resolution _/entity1_ in terms of a straightforward _entity2_ _P_ discourse copying algorithm _/entity2_ that correctly predicts a wide range of phenomena . The treatment does not suffer from problems inherent in _entity3_ identity-of-relations analyses _/entity3_ . Furthermore , in contrast to the approach of Dalrymple et al . [ 1991 ] , the treatment directly encodes the intuitive distinction between _entity4_ _C_ full NPs _/entity4_ and the _entity5_ referential elements _/entity5_ that corefer with them through what we term _entity6_ role linking _/entity6_ . The correct _entity7_ predictions _/entity7_ for several problematic examples of _entity8_ ellipsis _/entity8_ naturally result . Finally , the analysis extends directly to other _entity9_ discourse copying phenomena _/entity9_ .	NONE entity2 entity4
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ _C_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ _P_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity10 entity9
We present a framework for _entity1_ word alignment _/entity1_ based on _entity2_ log-linear models _/entity2_ . All _entity3_ knowledge sources _/entity3_ are treated as _entity4_ feature functions _/entity4_ , which depend on the _entity5_ source langauge sentence _/entity5_ , the _entity6_ target language sentence _/entity6_ and possible additional variables . _entity7_ Log-linear models _/entity7_ allow _entity8_ statistical alignment models _/entity8_ to be easily extended by incorporating _entity9_ syntactic information _/entity9_ . In this paper , we use _entity10_ IBM Model 3 alignment probabilities _/entity10_ , _entity11_ _P_ POS correspondence _/entity11_ , and _entity12_ _C_ bilingual dictionary coverage _/entity12_ as _entity13_ features _/entity13_ . Our experiments show that _entity14_ log-linear models _/entity14_ significantly outperform _entity15_ IBM translation models _/entity15_ .	NONE entity11 entity12
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ _P_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ _C_ signal-based document processing functionality _/entity15_ .	NONE entity14 entity15
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ _C_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ _P_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity18 entity15
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ _C_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ semantics _/entity18_ , the _entity19_ _P_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity19 entity17
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ _C_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ _P_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity25 entity22
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ _P_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ _C_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity22 entity24
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ _P_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ _C_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity9 entity11
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ _P_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ _C_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity18 entity19
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ _C_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ _P_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity15 entity12
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ _C_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ _P_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity5 entity2
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ _C_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ _P_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity11 entity9
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ _P_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ _C_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity5 entity8
The paper proposes and empirically motivates an integration of _entity1_ supervised learning _/entity1_ with _entity2_ unsupervised learning _/entity2_ to deal with human biases in _entity3_ summarization _/entity3_ . In particular , we explore the use of _entity4_ probabilistic decision tree _/entity4_ within the clustering framework to account for the variation as well as regularity in _entity5_ _P_ human created summaries _/entity5_ . The _entity6_ corpus _/entity6_ of human created extracts is created from a _entity7_ newspaper corpus _/entity7_ and used as a test set . We build _entity8_ _C_ probabilistic decision trees _/entity8_ of different flavors and integrate each of them with the clustering framework . Experiments with the _entity9_ corpus _/entity9_ demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone .	NONE entity5 entity8
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ _P_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ _C_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity19 entity22
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ _C_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ _P_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity4 entity3
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ _C_ discourse level _/entity11_ of _entity12_ _P_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity12 entity11
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ _P_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ _C_ phrasal constructions _/entity22_ .	NONE entity19 entity22
This article deals with the _entity1_ _P_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ _C_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity1 entity4
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ _P_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ _C_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity8 entity10
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ context-free parsing _/entity2_ is presented . The _entity3_ parser _/entity3_ computes a compact _entity4_ parse forest representation _/entity4_ of the complete set of possible _entity5_ analyses for large treebank grammars _/entity5_ and long _entity6_ input sentences _/entity6_ . The _entity7_ parser _/entity7_ uses _entity8_ bit-vector operations _/entity8_ to parallelise the _entity9_ _C_ basic parsing operations _/entity9_ . The _entity10_ _P_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	NONE entity10 entity9
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ _P_ context _/entity4_ of a _entity5_ _C_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	MODEL-FEATURE entity4 entity5
We have implemented a _entity1_ restricted domain parser _/entity1_ called _entity2_ Plume _/entity2_ . Building on previous work at Carnegie-Mellon University e.g . [ 4 , 5 , 8 ] , _entity3_ Plume 's approach to parsing _/entity3_ is based on _entity4_ semantic caseframe instantiation _/entity4_ . This has the advantages of _entity5_ efficiency _/entity5_ on _entity6_ grammatical input _/entity6_ , and _entity7_ robustness _/entity7_ in the face of _entity8_ ungrammatical input _/entity8_ . While _entity9_ Plume _/entity9_ is well adapted to simple _entity10_ declarative and imperative utterances _/entity10_ , it handles _entity11_ passives _/entity11_ , _entity12_ relative clauses _/entity12_ and _entity13_ interrogatives _/entity13_ in an ad hoc manner leading to patchy _entity14_ syntactic coverage _/entity14_ . This paper outlines _entity15_ _C_ Plume _/entity15_ as it currently exists and describes our detailed design for extending _entity16_ Plume _/entity16_ to handle _entity17_ passives _/entity17_ , _entity18_ _P_ relative clauses _/entity18_ , and _entity19_ interrogatives _/entity19_ in a general manner .	NONE entity18 entity15
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ _P_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ _C_ corpus of sentences _/entity3_ in the domain of the _entity4_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity1 entity3
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ _P_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ _C_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity2 entity5
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ _P_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ _C_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity8 entity10
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ _C_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ _P_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity6 entity3
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ _C_ language independent modules _/entity3_ of the _entity4_ _P_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity4 entity3
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ news commentary texts _/entity7_ : _entity8_ Colon _/entity8_ , _entity9_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ _P_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ _C_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	COMPARE entity17 entity18
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ sentence plans _/entity12_ for a given _entity13_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ plan _/entity16_ . The _entity17_ _C_ SPR _/entity17_ uses _entity18_ _P_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ top human-ranked sentence plan _/entity22_ .	NONE entity18 entity17
The project presented here is a part of a long term research program aiming at a full _entity1_ lexicon grammar for Polish ( SyntLex ) _/entity1_ . The main of this project is _entity2_ computer-assisted acquisition and morpho-syntactic description of verb-noun collocations _/entity2_ in _entity3_ Polish _/entity3_ . We present methodology and resources obtained in three main project phases which are : _entity4_ _C_ dictionary-based acquisition _/entity4_ of _entity5_ collocation lexicon _/entity5_ , feasibility study for _entity6_ corpus-based lexicon enlargement _/entity6_ phase , _entity7_ _P_ corpus-based lexicon enlargement _/entity7_ and _entity8_ collocation description _/entity8_ . In this paper we focus on the results of the third phase . The presented here _entity9_ corpus-based approach _/entity9_ permitted us to triple the size the _entity10_ verb-noun collocation dictionary for Polish _/entity10_ . In the paper we describe the _entity11_ SyntLex Dictionary of Collocations _/entity11_ and announce some future research intended to be a separate project continuation .	NONE entity7 entity4
_entity1_ Link detection _/entity1_ has been regarded as a core technology for the _entity2_ Topic Detection and Tracking tasks _/entity2_ of _entity3_ new event detection _/entity3_ . In this paper we formulate _entity4_ story link detection _/entity4_ and _entity5_ new event detection _/entity5_ as _entity6_ information retrieval task _/entity6_ and hypothesize on the impact of _entity7_ precision _/entity7_ and _entity8_ recall _/entity8_ on both systems . Motivated by these arguments , we introduce a number of new performance enhancing techniques including _entity9_ _P_ part of speech tagging _/entity9_ , new _entity10_ similarity measures _/entity10_ and expanded _entity11_ _C_ stop lists _/entity11_ . Experimental results validate our hypothesis .	NONE entity9 entity11
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ _P_ ccg parsing _/entity16_ . We extend this _entity17_ _C_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity16 entity17
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ _C_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ _P_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity13 entity10
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ _P_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ _C_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity27 entity30
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ _P_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ _C_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity10 entity12
This paper proposes a _entity1_ Hidden Markov Model ( HMM ) _/entity1_ and an _entity2_ HMM-based chunk tagger _/entity2_ , from which a _entity3_ _C_ named entity ( NE ) recognition ( NER ) system _/entity3_ is built to recognize and classify _entity4_ names _/entity4_ , _entity5_ _P_ times and numerical quantities _/entity5_ . Through the _entity6_ HMM _/entity6_ , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the _entity7_ words _/entity7_ , such as _entity8_ capitalization _/entity8_ and digitalization ; 2 ) _entity9_ internal semantic feature _/entity9_ of important triggers ; 3 ) _entity10_ internal gazetteer feature _/entity10_ ; 4 ) _entity11_ external macro context feature _/entity11_ . In this way , the _entity12_ NER problem _/entity12_ can be resolved effectively . Evaluation of our _entity13_ system _/entity13_ on _entity14_ MUC-6 and MUC-7 English NE tasks _/entity14_ achieves _entity15_ F-measures _/entity15_ of 96.6 % and 94.1 % respectively . It shows that the performance is significantly better than reported by any other _entity16_ machine-learning system _/entity16_ . Moreover , the _entity17_ performance _/entity17_ is even consistently better than those based on _entity18_ handcrafted rules _/entity18_ .	NONE entity5 entity3
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ _C_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ _P_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity16 entity14
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ _P_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ _C_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity13 entity16
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ _C_ grammatical and ungrammatical input _/entity9_ . A _entity10_ _P_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity10 entity9
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ _C_ translation paths _/entity17_ in the _entity18_ _P_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity18 entity17
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ class of languages _/entity2_ obtained by adding _entity3_ reduplication _/entity3_ to _entity4_ context-free languages _/entity4_ . The _entity5_ model _/entity5_ is a _entity6_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ model _/entity12_ appears capable of accommodating the sort of _entity13_ reduplications _/entity13_ that have been observed to occur in _entity14_ _C_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ _P_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity15 entity14
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ _P_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ _C_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	USAGE entity9 entity10
Reducing _entity1_ _P_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ LM pruning _/entity3_ . They are probability , _entity4_ _C_ rank _/entity4_ , and _entity5_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ pruning criteria _/entity6_ in a real application of _entity7_ Chinese text input _/entity7_ in terms of _entity8_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ CER _/entity13_ .	NONE entity1 entity4
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ _P_ block unigram model _/entity9_ and a _entity10_ _C_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity9 entity10
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ _P_ verb entailment _/entity11_ using evidence about _entity12_ _C_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity11 entity12
_entity1_ _P_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ _C_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity1 entity2
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ _C_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ _P_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity18 entity17
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ _C_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ _P_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity16 entity13
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ _C_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ _P_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity14 entity12
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ _P_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ _C_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity16 entity18
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ _C_ phrase translations _/entity9_ in our _entity10_ _P_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	NONE entity10 entity9
We provide a unified account of _entity1_ sentence-level and text-level anaphora _/entity1_ within the framework of a _entity2_ _P_ dependency-based grammar model _/entity2_ . Criteria for _entity3_ anaphora resolution _/entity3_ within _entity4_ sentence boundaries _/entity4_ rephrase major concepts from _entity5_ _C_ GB 's binding theory _/entity5_ , while those for _entity6_ text-level anaphora _/entity6_ incorporate an adapted version of a _entity7_ Grosz-Sidner-style focus model _/entity7_ .	NONE entity2 entity5
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ _P_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ _C_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity2 entity4
We introduce a new _entity1_ interactive corpus exploration tool _/entity1_ called _entity2_ InfoMagnets _/entity2_ . _entity3_ InfoMagnets _/entity3_ aims at making _entity4_ exploratory corpus analysis _/entity4_ accessible to researchers who are not experts in _entity5_ _C_ text mining _/entity5_ . As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between _entity6_ language _/entity6_ and _entity7_ behavioral patterns _/entity7_ in two distinct domains : _entity8_ _P_ tutorial dialogue _/entity8_ ( Kumar et al. , submitted ) and _entity9_ on-line communities _/entity9_ ( Arguello et al. , 2006 ) . As an _entity10_ educational tool _/entity10_ , it has been used as part of a unit on _entity11_ protocol analysis _/entity11_ in an _entity12_ Educational Research Methods course _/entity12_ .	NONE entity8 entity5
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ _C_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ _P_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity8 entity6
In this paper , we investigate the problem of automatically predicting _entity1_ segment boundaries _/entity1_ in _entity2_ spoken multiparty dialogue _/entity2_ . We extend prior work in two ways . We first apply approaches that have been proposed for _entity3_ predicting top-level topic shifts _/entity3_ to the problem of _entity4_ identifying subtopic boundaries _/entity4_ . We then explore the impact on _entity5_ performance _/entity5_ of using _entity6_ ASR output _/entity6_ as opposed to _entity7_ _P_ human transcription _/entity7_ . Examination of the effect of _entity8_ _C_ features _/entity8_ shows that _entity9_ predicting top-level and predicting subtopic boundaries _/entity9_ are two distinct tasks : ( 1 ) for predicting _entity10_ subtopic boundaries _/entity10_ , the _entity11_ lexical cohesion-based approach _/entity11_ alone can achieve competitive results , ( 2 ) for _entity12_ predicting top-level boundaries _/entity12_ , the _entity13_ machine learning approach _/entity13_ that combines _entity14_ lexical-cohesion and conversational features _/entity14_ performs best , and ( 3 ) _entity15_ conversational cues _/entity15_ , such as _entity16_ cue phrases _/entity16_ and _entity17_ overlapping speech _/entity17_ , are better indicators for the top-level prediction task . We also find that the _entity18_ transcription errors _/entity18_ inevitable in _entity19_ ASR output _/entity19_ have a negative impact on models that combine _entity20_ lexical-cohesion and conversational features _/entity20_ , but do not change the general preference of approach for the two tasks .	NONE entity7 entity8
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ _C_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ _P_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity14 entity11
We provide a unified account of _entity1_ sentence-level and text-level anaphora _/entity1_ within the framework of a _entity2_ _C_ dependency-based grammar model _/entity2_ . Criteria for _entity3_ anaphora resolution _/entity3_ within _entity4_ _P_ sentence boundaries _/entity4_ rephrase major concepts from _entity5_ GB 's binding theory _/entity5_ , while those for _entity6_ text-level anaphora _/entity6_ incorporate an adapted version of a _entity7_ Grosz-Sidner-style focus model _/entity7_ .	NONE entity4 entity2
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ _P_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ _C_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity10 entity12
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ _P_ duration model _/entity3_ has reduced the _entity4_ _C_ error rate _/entity4_ by about 10 % in both _entity5_ triphone and semiphone systems _/entity5_ . A new _entity6_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	RESULT entity3 entity4
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ _C_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ _P_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity6 entity4
The study addresses the problem of _entity1_ automatic acquisition _/entity1_ of _entity2_ entailment relations _/entity2_ between _entity3_ verbs _/entity3_ . While this task has much in common with _entity4_ paraphrases acquisition _/entity4_ which aims to discover _entity5_ semantic equivalence _/entity5_ between _entity6_ verbs _/entity6_ , the main challenge of _entity7_ entailment acquisition _/entity7_ is to capture _entity8_ asymmetric , or directional , relations _/entity8_ . Motivated by the intuition that it often underlies the _entity9_ local structure _/entity9_ of _entity10_ coherent text _/entity10_ , we develop a method that discovers _entity11_ verb entailment _/entity11_ using evidence about _entity12_ discourse relations _/entity12_ between _entity13_ clauses _/entity13_ available in a _entity14_ parsed corpus _/entity14_ . In comparison with earlier work , the proposed method covers a much wider range of _entity15_ _P_ verb entailment types _/entity15_ and learns the _entity16_ mapping _/entity16_ between _entity17_ _C_ verbs _/entity17_ with highly varied _entity18_ argument structures _/entity18_ .	NONE entity15 entity17
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ _C_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ _P_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity5 entity4
Towards deep analysis of _entity1_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ class-oriented framework _/entity2_ for collecting _entity3_ _C_ paraphrase examples _/entity3_ , in which _entity4_ sentential paraphrases _/entity4_ are collected for each _entity5_ _P_ paraphrase class _/entity5_ separately by means of _entity6_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ cost-efficiency _/entity9_ , _entity10_ exhaustiveness _/entity10_ , and _entity11_ reliability _/entity11_ .	NONE entity5 entity3
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ _C_ tasks _/entity20_ using the _entity21_ _P_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	USAGE entity21 entity20
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ _C_ evaluation techniques _/entity5_ will provide information about both the _entity6_ _P_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity6 entity5
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ _P_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ _C_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	MODEL-FEATURE entity7 entity8
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ _C_ causal and troponymic entailment _/entity9_ ) as _entity10_ _P_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity10 entity9
An extension to the _entity1_ GPSG grammatical formalism _/entity1_ is proposed , allowing _entity2_ non-terminals _/entity2_ to consist of finite sequences of _entity3_ _P_ category labels _/entity3_ , and allowing _entity4_ schematic variables _/entity4_ to range over such sequences . The extension is shown to be sufficient to provide a strongly adequate _entity5_ _C_ grammar _/entity5_ for _entity6_ crossed serial dependencies _/entity6_ , as found in e.g . _entity7_ Dutch subordinate clauses _/entity7_ . The structures induced for such _entity8_ constructions _/entity8_ are argued to be more appropriate to data involving _entity9_ conjunction _/entity9_ than some previous proposals have been . The extension is shown to be parseable by a simple extension to an existing _entity10_ parsing method _/entity10_ for _entity11_ GPSG _/entity11_ .	NONE entity3 entity5
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ natural language interface _/entity2_ to _entity3_ database query applications _/entity3_ . _entity4_ _P_ Multimedia answers _/entity4_ include _entity5_ _C_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ sentences _/entity6_ in _entity7_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity4 entity5
This paper discusses the application of _entity1_ Unification Categorial Grammar ( UCG ) _/entity1_ to the framework of _entity2_ Isomorphic Grammars _/entity2_ for _entity3_ Machine Translation _/entity3_ pioneered by Landsbergen . The _entity4_ Isomorphic Grammars approach to MT _/entity4_ involves developing the _entity5_ grammars _/entity5_ of the _entity6_ Source and Target languages _/entity6_ in parallel , in order to ensure that _entity7_ SL _/entity7_ and _entity8_ TL _/entity8_ expressions which stand in the _entity9_ _P_ translation relation _/entity9_ have _entity10_ isomorphic derivations _/entity10_ . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited , obviating the need for answers to _entity11_ _C_ semantic questions _/entity11_ that we do not yet have . _entity12_ Semantic _/entity12_ and other information may still be incorporated , but as constraints on the _entity13_ translation relation _/entity13_ , not as levels of _entity14_ textual representation _/entity14_ . After introducing this approach to _entity15_ MT system _/entity15_ design , and the basics of _entity16_ monolingual UCG _/entity16_ , we will show how the two can be integrated , and present an example from an implemented _entity17_ bi-directional English-Spanish fragment _/entity17_ . Finally we will present some outstanding problems with the approach .	NONE entity9 entity11
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ _P_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ _C_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity13 entity15
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ _C_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ _P_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity11 entity9
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ _P_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ _C_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity4 entity6
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ editor _/entity3_ was to provide an adequate tool to help _entity4_ _P_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ linguistic theory _/entity6_ . If we want valuable _entity7_ _C_ lexicons _/entity7_ and _entity8_ grammars _/entity8_ to achieve complex _entity9_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity4 entity7
We focus on the problem of building large _entity1_ repositories _/entity1_ of _entity2_ lexical conceptual structure ( LCS ) representations _/entity2_ for _entity3_ verbs _/entity3_ in multiple _entity4_ languages _/entity4_ . One of the main results of this work is the definition of a relation between _entity5_ broad semantic classes _/entity5_ and _entity6_ LCS meaning components _/entity6_ . Our _entity7_ acquisition program - LEXICALL - _/entity7_ takes , as input , the result of previous work on _entity8_ verb classification _/entity8_ and _entity9_ thematic grid tagging _/entity9_ , and outputs _entity10_ LCS representations _/entity10_ for different _entity11_ languages _/entity11_ . These _entity12_ representations _/entity12_ have been ported into _entity13_ _P_ English , Arabic and Spanish lexicons _/entity13_ , each containing approximately 9000 _entity14_ _C_ verbs _/entity14_ . We are currently using these _entity15_ lexicons _/entity15_ in an _entity16_ operational foreign language tutoring _/entity16_ and _entity17_ machine translation _/entity17_ .	NONE entity13 entity14
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ _P_ morpheme sequence _/entity13_ for a given _entity14_ _C_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	MODEL-FEATURE entity13 entity14
This paper proposes a novel method of building _entity1_ polarity-tagged corpus _/entity1_ from _entity2_ HTML documents _/entity2_ . The characteristics of this method is that it is fully automatic and can be applied to arbitrary _entity3_ HTML documents _/entity3_ . The idea behind our method is to utilize certain _entity4_ layout structures _/entity4_ and _entity5_ linguistic pattern _/entity5_ . By using them , we can automatically extract such _entity6_ sentences _/entity6_ that express opinion . In our experiment , the method could construct a _entity7_ _C_ corpus _/entity7_ consisting of 126,610 _entity8_ _P_ sentences _/entity8_ .	PART_WHOLE entity8 entity7
_entity1_ Unification _/entity1_ is often the appropriate method for expressing _entity2_ _P_ relations _/entity2_ between _entity3_ representations _/entity3_ in the form of _entity4_ feature structures _/entity4_ ; however , there are circumstances in which a different approach is desirable . A _entity5_ _C_ declarative formalism _/entity5_ is presented which permits direct _entity6_ mappings _/entity6_ of one _entity7_ feature structure _/entity7_ into another , and illustrative examples are given of its application to areas of current interest .	NONE entity2 entity5
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ _P_ neural network _/entity22_ or a _entity23_ _C_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity22 entity23
In this paper we present a _entity1_ statistical profile _/entity1_ of the _entity2_ Named Entity task _/entity2_ , a specific _entity3_ information extraction task _/entity3_ for which _entity4_ corpora _/entity4_ in several _entity5_ languages _/entity5_ are available . Using the _entity6_ results _/entity6_ of the _entity7_ statistical analysis _/entity7_ , we propose an _entity8_ algorithm _/entity8_ for _entity9_ _P_ lower bound estimation _/entity9_ for _entity10_ Named Entity corpora _/entity10_ and discuss the significance of the _entity11_ _C_ cross-lingual comparisons _/entity11_ provided by the _entity12_ analysis _/entity12_ .	NONE entity9 entity11
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ _P_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ _C_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity11 entity13
In this paper , we present an _entity1_ unlexicalized parser _/entity1_ for _entity2_ German _/entity2_ which employs _entity3_ smoothing _/entity3_ and _entity4_ suffix analysis _/entity4_ to achieve a _entity5_ labelled bracket F-score _/entity5_ of 76.2 , higher than previously reported results on the _entity6_ NEGRA corpus _/entity6_ . In addition to the high _entity7_ _C_ accuracy _/entity7_ of the model , the use of _entity8_ smoothing _/entity8_ in an _entity9_ _P_ unlexicalized parser _/entity9_ allows us to better examine the interplay between _entity10_ smoothing _/entity10_ and _entity11_ parsing _/entity11_ results .	NONE entity9 entity7
A method for _entity1_ error correction _/entity1_ of _entity2_ ill-formed input _/entity2_ is described that acquires _entity3_ _C_ dialogue patterns _/entity3_ in typical usage and uses these _entity4_ patterns _/entity4_ to predict new inputs . _entity5_ _P_ Error correction _/entity5_ is done by strongly biasing _entity6_ parsing _/entity6_ toward expected _entity7_ meanings _/entity7_ unless clear evidence from the input shows the current _entity8_ sentence _/entity8_ is not expected . A _entity9_ dialogue acquisition and tracking algorithm _/entity9_ is presented along with a description of its _entity10_ implementation _/entity10_ in a _entity11_ voice interactive system _/entity11_ . A series of tests are described that show the power of the _entity12_ error correction methodology _/entity12_ when _entity13_ stereotypic dialogue _/entity13_ occurs .	NONE entity5 entity3
This paper presents an analysis of _entity1_ _P_ temporal anaphora _/entity1_ in _entity2_ _C_ sentences _/entity2_ which contain _entity3_ quantification over events _/entity3_ , within the framework of _entity4_ Discourse Representation Theory _/entity4_ . The analysis in ( Partee , 1984 ) of _entity5_ quantified sentences _/entity5_ , introduced by a _entity6_ temporal connective _/entity6_ , gives the wrong _entity7_ truth-conditions _/entity7_ when the _entity8_ temporal connective _/entity8_ in the _entity9_ subordinate clause _/entity9_ is before or after . This _entity10_ problem _/entity10_ has been previously analyzed in ( de Swart , 1991 ) as an instance of the _entity11_ proportion problem _/entity11_ and given a solution from a _entity12_ Generalized Quantifier approach _/entity12_ . By using a careful distinction between the different notions of _entity13_ reference time _/entity13_ based on ( Kamp and Reyle , 1993 ) , we propose a solution to this _entity14_ problem _/entity14_ , within the framework of _entity15_ DRT _/entity15_ . We show some applications of this _entity16_ solution _/entity16_ to additional _entity17_ temporal anaphora phenomena _/entity17_ in _entity18_ quantified sentences _/entity18_ .	NONE entity1 entity2
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ _C_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ _P_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ root _/entity14_ and _entity15_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity7 entity4
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ _P_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ _C_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity34 entity36
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ morphological and syntactic information _/entity3_ . While the _entity4_ model _/entity4_ is more complex than those which have been employed for _entity5_ _C_ unsupervised learning _/entity5_ of _entity6_ POS tags in English _/entity6_ , which use only _entity7_ _P_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ morphology _/entity9_ as well . In many _entity10_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ Bulgarian _/entity15_ , a _entity16_ Slavic language _/entity16_ with relatively _entity17_ free word order _/entity17_ and _entity18_ rich morphology _/entity18_ .	USAGE entity7 entity5
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ _P_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ _C_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity19 entity21
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ _C_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ _P_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity21 entity20
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ _P_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ _C_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity19 entity20
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ LFG _/entity4_ and _entity5_ UDRS _/entity5_ formalisms . It provides a _entity6_ model theoretic interpretation _/entity6_ and an _entity7_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ f-structures _/entity9_ through the _entity10_ translation images _/entity10_ of _entity11_ _P_ f-structures _/entity11_ as _entity12_ _C_ UDRSs _/entity12_ .	NONE entity11 entity12
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ representation tree _/entity8_ as well as the correspondence between _entity9_ _P_ substrings _/entity9_ in the _entity10_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ _C_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity9 entity12
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ _P_ . We train a _/entity3_ maximum entropy classifier _entity4_ _C_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity3 entity4
In this paper we describe a novel _entity1_ data structure _/entity1_ for _entity2_ phrase-based statistical machine translation _/entity2_ which allows for the _entity3_ retrieval _/entity3_ of arbitrarily long _entity4_ phrases _/entity4_ while simultaneously using less _entity5_ memory _/entity5_ than is required by current _entity6_ decoder _/entity6_ implementations . We detail the _entity7_ computational complexity _/entity7_ and _entity8_ average retrieval times _/entity8_ for looking up _entity9_ _P_ phrase translations _/entity9_ in our _entity10_ _C_ suffix array-based data structure _/entity10_ . We show how _entity11_ sampling _/entity11_ can be used to reduce the _entity12_ retrieval time _/entity12_ by orders of magnitude with no loss in _entity13_ translation quality _/entity13_ .	PART_WHOLE entity9 entity10
In this paper , we reported experiments of _entity1_ unsupervised automatic acquisition _/entity1_ of _entity2_ Italian and English verb subcategorization frames ( SCFs ) _/entity2_ from _entity3_ general and domain corpora _/entity3_ . The proposed technique operates on _entity4_ syntactically shallow-parsed corpora _/entity4_ on the basis of a limited number of _entity5_ search heuristics _/entity5_ not relying on any previous _entity6_ lexico-syntactic knowledge _/entity6_ about _entity7_ SCFs _/entity7_ . Although preliminary , reported results are in line with _entity8_ state-of-the-art lexical acquisition systems _/entity8_ . The issue of whether _entity9_ verbs _/entity9_ sharing similar _entity10_ _P_ SCFs distributions _/entity10_ happen to share _entity11_ similar semantic properties _/entity11_ as well was also explored by clustering _entity12_ verbs _/entity12_ that share _entity13_ _C_ frames _/entity13_ with the same _entity14_ distribution _/entity14_ using the _entity15_ Minimum Description Length Principle ( MDL ) _/entity15_ . First experiments in this direction were carried out on _entity16_ Italian verbs _/entity16_ with encouraging results .	NONE entity10 entity13
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ _P_ configuration _/entity15_ , _entity16_ _C_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity15 entity16
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ _C_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ _P_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity14 entity12
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ _C_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ _P_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity16 entity14
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ _P_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ _C_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity4 entity7
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ _C_ grammar _/entity9_ from _entity10_ _P_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity10 entity9
An extension to the _entity1_ GPSG grammatical formalism _/entity1_ is proposed , allowing _entity2_ non-terminals _/entity2_ to consist of finite sequences of _entity3_ category labels _/entity3_ , and allowing _entity4_ schematic variables _/entity4_ to range over such sequences . The extension is shown to be sufficient to provide a strongly adequate _entity5_ grammar _/entity5_ for _entity6_ crossed serial dependencies _/entity6_ , as found in e.g . _entity7_ Dutch subordinate clauses _/entity7_ . The structures induced for such _entity8_ _C_ constructions _/entity8_ are argued to be more appropriate to data involving _entity9_ conjunction _/entity9_ than some previous proposals have been . The extension is shown to be parseable by a simple extension to an existing _entity10_ _P_ parsing method _/entity10_ for _entity11_ GPSG _/entity11_ .	NONE entity10 entity8
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ _P_ WSD evaluation methodology _/entity4_ and _entity5_ _C_ datasets _/entity5_ from the _entity6_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity4 entity5
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ _P_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ _C_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity6 entity8
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ _C_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ _P_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity20 entity18
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ _P_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ _C_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity8 entity10
_entity1_ Automatic summarization _/entity1_ and _entity2_ _C_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ _P_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity4 entity2
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ _P_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ _C_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity2 entity5
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ _P_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ _C_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity11 entity14
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ _P_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ _C_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity11 entity13
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ _P_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ dictionary-based word vectors _/entity10_ better reflect _entity11_ _C_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity9 entity11
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ _P_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ _C_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity20 entity22
Soames 1979 provides some counterexamples to the _entity1_ theory of natural language presuppositions _/entity1_ that is presented in Gazdar 1979 . Soames 1982 provides a theory which explains these counterexamples . Mercer 1987 rejects the solution found in Soames 1982 leaving these counterexamples unexplained . By reappraising these insightful counterexamples , the _entity2_ _C_ inferential theory for natural language presuppositions _/entity2_ described in Mercer 1987 , 1988 gives a simple and straightforward explanation for the _entity3_ _P_ presuppositional nature _/entity3_ of these _entity4_ sentences _/entity4_ .	NONE entity3 entity2
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ _C_ three-way voting _/entity16_ among the _entity17_ _P_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity17 entity16
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ _C_ syntactic structure features _/entity5_ embedded in a _entity6_ parse tree _/entity6_ are very effective for _entity7_ _P_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity7 entity5
In this paper we compare two competing approaches to _entity1_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ _P_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ _C_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity4 entity6
In this paper , we describe a search procedure for _entity1_ statistical machine translation ( MT ) _/entity1_ based on _entity2_ dynamic programming ( DP ) _/entity2_ . Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible _entity3_ word reordering _/entity3_ between _entity4_ _P_ source and target language _/entity4_ in order to achieve an efficient search algorithm . A search restriction especially useful for the translation direction from German to English is presented . The experimental tests are carried out on the _entity5_ Verbmobil task _/entity5_ ( German-English , 8000-word vocabulary ) , which is a _entity6_ _C_ limited-domain spoken-language task _/entity6_ .	NONE entity4 entity6
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ _C_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ _P_ new domains _/entity14_ .	NONE entity14 entity13
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ _C_ grammatical gender _/entity10_ in _entity11_ _P_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity11 entity10
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ _P_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ _C_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity27 entity30
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ _P_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ _C_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity26 entity28
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ _C_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ _P_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity10 entity8
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ _P_ human-computer interaction _/entity6_ . We analyzed _entity7_ eye gaze _/entity7_ , _entity8_ _C_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity6 entity8
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ _P_ sublanguage _/entity3_ , is proposed for identifying _entity4_ unknown words _/entity4_ , especially _entity5_ personal names _/entity5_ , in _entity6_ _C_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ adaptive dynamic word formation _/entity8_ , _entity9_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ WI systems _/entity12_ without the _entity13_ name identification _/entity13_ capability .	NONE entity3 entity6
This paper describes an implemented program that takes a _entity1_ _C_ tagged text corpus _/entity1_ and generates a partial list of the _entity2_ _P_ subcategorization frames _/entity2_ in which each _entity3_ verb _/entity3_ occurs . The completeness of the output list increases monotonically with the total _entity4_ occurrences _/entity4_ of each _entity5_ verb _/entity5_ in the _entity6_ training corpus _/entity6_ . _entity7_ False positive rates _/entity7_ are one to three percent . Five _entity8_ subcategorization frames _/entity8_ are currently detected and we foresee no impediment to detecting many more . Ultimately , we expect to provide a large _entity9_ subcategorization dictionary _/entity9_ to the _entity10_ NLP community _/entity10_ and to train _entity11_ dictionaries _/entity11_ for specific _entity12_ corpora _/entity12_ .	NONE entity2 entity1
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ news commentary texts _/entity7_ : _entity8_ Colon _/entity8_ , _entity9_ _C_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ _P_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity12 entity9
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ _P_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ _C_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity13 entity16
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ _P_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ _C_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity16 entity17
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ _P_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ _C_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity5 entity8
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ corpus of sentences _/entity3_ in the domain of the _entity4_ _P_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ _C_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity4 entity5
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ _C_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ _P_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity17 entity15
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ _C_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ _P_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity13 entity10
In this paper , we show how to construct a _entity1_ _P_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ _C_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity1 entity3
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ _C_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ _P_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity12 entity10
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ _P_ meaning _/entity42_ to other _entity43_ _C_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity42 entity43
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ character sets _/entity6_ should be treated differently and the changes between _entity7_ _P_ character types _/entity7_ are very important because _entity8_ Japanese script _/entity8_ has both _entity9_ _C_ ideograms _/entity9_ like _entity10_ Chinese _/entity10_ ( _entity11_ kanji _/entity11_ ) and _entity12_ phonograms _/entity12_ like _entity13_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity7 entity9
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ _C_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ _P_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity8 entity6
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ _C_ confidence measures _/entity25_ and picking the best _entity26_ _P_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity26 entity25
It is often assumed that when _entity1_ natural language processing _/entity1_ meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned . However , our experience with _entity2_ TACITUS _/entity2_ ; especially in the _entity3_ MUC-3 evaluation _/entity3_ , has shown that principled techniques for _entity4_ syntactic and pragmatic analysis _/entity4_ can be bolstered with methods for achieving robustness . We describe three techniques for making _entity5_ syntactic analysis _/entity5_ more robust -- -an _entity6_ _P_ agenda-based scheduling parser _/entity6_ , a _entity7_ recovery technique for failed parses _/entity7_ , and a new technique called _entity8_ terminal substring parsing _/entity8_ . For _entity9_ _C_ pragmatics processing _/entity9_ , we describe how the method of _entity10_ abductive inference _/entity10_ is inherently robust , in that an interpretation is always possible , so that in the absence of the required _entity11_ world knowledge _/entity11_ , performance degrades gracefully . Each of these techniques have been evaluated and the results of the evaluations are presented .	NONE entity6 entity9
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ _C_ computational methods _/entity15_ of _entity16_ _P_ KDS _/entity16_ are described .	NONE entity16 entity15
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ _P_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ _C_ sentence plans _/entity12_ for a given _entity13_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ plan _/entity16_ . The _entity17_ SPR _/entity17_ uses _entity18_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ top human-ranked sentence plan _/entity22_ .	NONE entity10 entity12
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ _P_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ _C_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity14 entity17
We suggest a new goal and _entity1_ _P_ evaluation criterion _/entity1_ for _entity2_ _C_ word similarity measures _/entity2_ . The new criterion _entity3_ meaning-entailing substitutability _/entity3_ fits the needs of _entity4_ semantic-oriented NLP applications _/entity4_ and can be evaluated directly ( independent of an application ) at a good level of _entity5_ human agreement _/entity5_ . Motivated by this _entity6_ semantic criterion _/entity6_ we analyze the empirical quality of _entity7_ distributional word feature vectors _/entity7_ and its impact on _entity8_ word similarity results _/entity8_ , proposing an objective measure for evaluating _entity9_ feature vector quality _/entity9_ . Finally , a novel _entity10_ feature weighting and selection function _/entity10_ is presented , which yields superior _entity11_ feature vectors _/entity11_ and better _entity12_ word similarity performance _/entity12_ .	NONE entity1 entity2
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ _C_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ _P_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity9 entity6
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ question phrases _/entity5_ in _entity6_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ _C_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ _P_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ MRR _/entity14_ .	NONE entity13 entity11
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ _P_ Nouns _/entity7_ and _entity8_ _C_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity7 entity8
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ _P_ grs _/entity10_ ) in the _entity11_ _C_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity10 entity11
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ _P_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ _C_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity19 entity22
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ _C_ relation extraction _/entity6_ and contributes to most of the _entity7_ _P_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity7 entity6
In our current research into the design of _entity1_ _P_ cognitively well-motivated interfaces _/entity1_ relying primarily on the _entity2_ display of graphical information _/entity2_ , we have observed that _entity3_ _C_ graphical information _/entity3_ alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations . This can occur due to too much _entity4_ information _/entity4_ being requested , too little , _entity5_ information _/entity5_ of the wrong kind , etc . To solve this problem , we are working towards the integration of _entity6_ natural language generation _/entity6_ to augment the _entity7_ interaction _/entity7_	NONE entity1 entity3
We propose a method that automatically generates _entity1_ _P_ paraphrase _/entity1_ sets from _entity2_ _C_ seed sentences _/entity2_ to be used as _entity3_ reference sets _/entity3_ in objective _entity4_ machine translation evaluation measures _/entity4_ like _entity5_ BLEU _/entity5_ and _entity6_ NIST _/entity6_ . We measured the quality of the _entity7_ paraphrases _/entity7_ produced in an experiment , i.e. , ( i ) their _entity8_ grammaticality _/entity8_ : at least 99 % correct _entity9_ sentences _/entity9_ ; ( ii ) their _entity10_ equivalence in meaning _/entity10_ : at least 96 % correct _entity11_ paraphrases _/entity11_ either by _entity12_ meaning equivalence _/entity12_ or _entity13_ entailment _/entity13_ ; and , ( iii ) the amount of internal _entity14_ lexical and syntactical variation _/entity14_ in a set of _entity15_ paraphrases _/entity15_ : slightly superior to that of _entity16_ hand-produced sets _/entity16_ . The _entity17_ paraphrase _/entity17_ sets produced by this method thus seem adequate as _entity18_ reference sets _/entity18_ to be used for _entity19_ MT evaluation _/entity19_ .	PART_WHOLE entity1 entity2
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ _P_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ _C_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity4 entity7
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ _P_ equivalences _/entity21_ , which can be used to simplify _entity22_ _C_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity21 entity22
We present an operable definition of _entity1_ _P_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ _C_ discourse model _/entity3_ and _entity4_ knowledge store _/entity4_ is introduced enabling the _entity5_ decomposition _/entity5_ and _entity6_ formal representation _/entity6_ of its _entity7_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity1 entity3
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ _C_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ _P_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity15 entity12
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ grammar coverage problems _/entity3_ we use a _entity4_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ _C_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ _P_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity12 entity9
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ _P_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ _C_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity9 entity11
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ _C_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ _P_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity23 entity20
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ affix removal _/entity16_ . Our _entity17_ _C_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ _P_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity19 entity17
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ _C_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ _P_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity16 entity13
A _entity1_ model _/entity1_ is presented to characterize the _entity2_ _P_ class of languages _/entity2_ obtained by adding _entity3_ _C_ reduplication _/entity3_ to _entity4_ context-free languages _/entity4_ . The _entity5_ model _/entity5_ is a _entity6_ pushdown automaton _/entity6_ augmented with the ability to check _entity7_ reduplication _/entity7_ by using the _entity8_ stack _/entity8_ in a new way . The _entity9_ class of languages _/entity9_ generated is shown to lie strictly between the _entity10_ context-free languages _/entity10_ and the _entity11_ indexed languages _/entity11_ . The _entity12_ model _/entity12_ appears capable of accommodating the sort of _entity13_ reduplications _/entity13_ that have been observed to occur in _entity14_ natural languages _/entity14_ , but it excludes many of the unnatural _entity15_ constructions _/entity15_ that other _entity16_ formal models _/entity16_ have permitted .	NONE entity2 entity3
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ _P_ electronic discussions _/entity5_ that influence the _entity6_ _C_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity5 entity6
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ _P_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ _C_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity9 entity12
Interpreting _entity1_ metaphors _/entity1_ is an integral and inescapable process in _entity2_ human understanding of natural language _/entity2_ . This paper discusses a _entity3_ method of analyzing metaphors _/entity3_ based on the existence of a small number of _entity4_ generalized metaphor mappings _/entity4_ . Each _entity5_ generalized metaphor _/entity5_ contains a _entity6_ recognition network _/entity6_ , a _entity7_ basic mapping _/entity7_ , additional _entity8_ _C_ transfer mappings _/entity8_ , and an _entity9_ implicit intention component _/entity9_ . It is argued that the method reduces _entity10_ _P_ metaphor interpretation _/entity10_ from a _entity11_ reconstruction _/entity11_ to a _entity12_ recognition task _/entity12_ . Implications towards automating certain aspects of _entity13_ language learning _/entity13_ are also discussed .	NONE entity10 entity8
This paper proposes an _entity1_ alignment adaptation approach _/entity1_ to improve _entity2_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ alignment adaptation _/entity3_ is to use _entity4_ out-of-domain corpus _/entity4_ to improve _entity5_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ _P_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ precision _/entity11_ and _entity12_ _C_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity9 entity12
How to obtain _entity1_ hierarchical relations _/entity1_ ( e.g . _entity2_ superordinate -hyponym relation _/entity2_ , _entity3_ synonym relation _/entity3_ ) is one of the most important problems for _entity4_ thesaurus construction _/entity4_ . A pilot system for extracting these _entity5_ _C_ relations _/entity5_ automatically from an ordinary _entity6_ Japanese language dictionary _/entity6_ ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given . The features of the _entity7_ definition sentences _/entity7_ in the _entity8_ _P_ dictionary _/entity8_ , the mechanical extraction of the _entity9_ hierarchical relations _/entity9_ and the estimation of the results are discussed .	NONE entity8 entity5
Computer programs so far have not fared well in _entity1_ modeling language acquisition _/entity1_ . For one thing , _entity2_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ _P_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ _C_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity16 entity18
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ _C_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ _P_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity30 entity28
This paper introduces a robust _entity1_ interactive method for speech understanding _/entity1_ . The _entity2_ generalized LR parsing _/entity2_ is enhanced in this approach . _entity3_ _P_ Parsing _/entity3_ proceeds from left to right correcting minor errors . When a very noisy _entity4_ portion _/entity4_ is detected , the _entity5_ _C_ parser _/entity5_ skips that _entity6_ portion _/entity6_ using a fake _entity7_ non-terminal symbol _/entity7_ . The unidentified _entity8_ portion _/entity8_ is resolved by _entity9_ re-utterance _/entity9_ of that _entity10_ portion _/entity10_ which is parsed very efficiently by using the _entity11_ parse record _/entity11_ of the first _entity12_ utterance _/entity12_ . The _entity13_ user _/entity13_ does not have to speak the whole _entity14_ sentence _/entity14_ again . This method is also capable of handling _entity15_ unknown words _/entity15_ , which is important in practical systems . Detected _entity16_ unknown words _/entity16_ can be incrementally incorporated into the _entity17_ dictionary _/entity17_ after the interaction with the _entity18_ user _/entity18_ . A _entity19_ pilot system _/entity19_ has shown great effectiveness of this approach .	NONE entity3 entity5
_entity1_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ _P_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ _C_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity5 entity7
Current _entity1_ natural language interfaces _/entity1_ have concentrated largely on determining the literal _entity2_ meaning _/entity2_ of _entity3_ input _/entity3_ from their _entity4_ users _/entity4_ . While such _entity5_ decoding _/entity5_ is an essential underpinning , much recent work suggests that _entity6_ natural language interfaces _/entity6_ will never appear cooperative or graceful unless they also incorporate numerous _entity7_ non-literal aspects of communication _/entity7_ , such as robust _entity8_ communication procedures _/entity8_ . This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these _entity9_ _C_ non-literal aspects of communication _/entity9_ ; that the new technology of powerful _entity10_ personal computers _/entity10_ with integral _entity11_ _P_ graphics displays _/entity11_ offers techniques superior to those of humans for these aspects , while still satisfying _entity12_ human communication needs _/entity12_ . The paper proposes _entity13_ interfaces _/entity13_ based on a judicious mixture of these techniques and the still valuable methods of more traditional _entity14_ natural language interfaces _/entity14_ .	NONE entity11 entity9
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ _P_ concepts _/entity2_ from _entity3_ _C_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity2 entity3
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ _P_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ _C_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity9 entity12
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ ( 100,000 _/entity12_ words _entity13_ _C_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ _P_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity15 entity13
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ _P_ meaning _/entity3_ or _entity4_ _C_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity3 entity4
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ _P_ antecedence recovery _/entity8_ for each of the _entity9_ _C_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	MODEL-FEATURE entity8 entity9
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ _C_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ _P_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity17 entity15
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ _C_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ _P_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity11 entity9
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ _P_ user _/entity6_ . The issue of _entity7_ _C_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity6 entity7
In this paper we introduce a _entity1_ modal language LT _/entity1_ for imposing _entity2_ constraints _/entity2_ on _entity3_ trees _/entity3_ , and an extension _entity4_ _C_ LT ( LF ) _/entity4_ for imposing _entity5_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ _P_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ GPSG _/entity9_ can be captured in _entity10_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	NONE entity7 entity4
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ _C_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ _P_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity13 entity11
This paper presents a _entity1_ formal analysis _/entity1_ for a large class of _entity2_ words _/entity2_ called _entity3_ _C_ alternative markers _/entity3_ , which includes _entity4_ other ( than ) _/entity4_ , _entity5_ such ( as ) _/entity5_ , and _entity6_ _P_ besides _/entity6_ . These _entity7_ words _/entity7_ appear frequently enough in _entity8_ dialog _/entity8_ to warrant serious _entity9_ attention _/entity9_ , yet present _entity10_ natural language search engines _/entity10_ perform poorly on _entity11_ queries _/entity11_ containing them . I show that the _entity12_ performance _/entity12_ of a _entity13_ search engine _/entity13_ can be improved dramatically by incorporating an approximation of the _entity14_ formal analysis _/entity14_ that is compatible with the _entity15_ search engine _/entity15_ 's _entity16_ operational semantics _/entity16_ . The value of this approach is that as the _entity17_ operational semantics _/entity17_ of _entity18_ natural language applications _/entity18_ improve , even larger improvements are possible .	NONE entity6 entity3
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ _P_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ _C_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity6 entity7
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ _P_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ _C_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity5 entity7
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ _C_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ _P_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity12 entity9
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ _C_ system _/entity6_ participated in all the tracks of the _entity7_ _P_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity7 entity6
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ _C_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ spoken language technology _/entity5_ into _entity6_ _P_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity6 entity3
This paper describes to what extent _entity1_ deep processing _/entity1_ may benefit from _entity2_ shallow techniques _/entity2_ and it presents a _entity3_ _C_ NLP system _/entity3_ which integrates a _entity4_ linguistic PoS tagger and chunker _/entity4_ as a preprocessing module of a _entity5_ _P_ broad coverage unification based grammar of Spanish _/entity5_ . Experiments show that the _entity6_ efficiency _/entity6_ of the overall analysis improves significantly and that our system also provides _entity7_ robustness _/entity7_ to the _entity8_ linguistic processing _/entity8_ while maintaining both the _entity9_ accuracy _/entity9_ and the _entity10_ precision _/entity10_ of the _entity11_ grammar _/entity11_ .	NONE entity5 entity3
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ _P_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ _C_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity8 entity11
This paper ties up some loose ends in _entity1_ finite-state Optimality Theory _/entity1_ . First , it discusses how to perform _entity2_ comprehension _/entity2_ under _entity3_ Optimality Theory grammars _/entity3_ consisting of _entity4_ finite-state constraints _/entity4_ . _entity5_ Comprehension _/entity5_ has not been much studied in _entity6_ OT _/entity6_ ; we show that unlike _entity7_ _P_ production _/entity7_ , it does not always yield a regular set , making _entity8_ finite-state methods _/entity8_ inapplicable . However , after giving a suitably flexible presentation of _entity9_ OT _/entity9_ , we show carefully how to treat _entity10_ _C_ comprehension _/entity10_ under recent _entity11_ variants of OT _/entity11_ in which _entity12_ grammars _/entity12_ can be compiled into _entity13_ finite-state transducers _/entity13_ . We then unify these variants , showing that _entity14_ compilation _/entity14_ is possible if all components of the _entity15_ grammar _/entity15_ are _entity16_ regular relations _/entity16_ , including the _entity17_ harmony ordering _/entity17_ on _entity18_ scored candidates _/entity18_ .	NONE entity7 entity10
The multiplicative fragment of _entity1_ linear logic _/entity1_ has found a number of applications in _entity2_ computational linguistics _/entity2_ : in the _entity3_ `` glue language '' _/entity3_ approach to _entity4_ LFG semantics _/entity4_ , and in the formulation and _entity5_ _C_ parsing _/entity5_ of various _entity6_ categorial grammars _/entity6_ . These applications call for efficient deduction methods . Although a number of deduction methods for _entity7_ _P_ multiplicative linear logic _/entity7_ are known , none of them are tabular methods , which bring a substantial efficiency gain by avoiding redundant computation ( cf . chart methods in _entity8_ CFG parsing _/entity8_ ) : this paper presents such a method , and discusses its use in relation to the above applications .	NONE entity7 entity5
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ _C_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ _P_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity5 entity3
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ _C_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ _P_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ lemma-based model _/entity14_ on the _entity15_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	NONE entity9 entity6
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ _C_ semantic composition _/entity14_ . More recently , the _entity15_ _P_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity15 entity14
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ _P_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ _C_ dictionary-based word vectors _/entity10_ better reflect _entity11_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity9 entity10
In this paper we introduce a _entity1_ modal language LT _/entity1_ for imposing _entity2_ constraints _/entity2_ on _entity3_ trees _/entity3_ , and an extension _entity4_ LT ( LF ) _/entity4_ for imposing _entity5_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ _P_ GPSG _/entity9_ can be captured in _entity10_ _C_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	COMPARE entity9 entity10
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ _P_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ _C_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity8 entity10
This paper proposes a practical approach employing _entity1_ n-gram models _/entity1_ and _entity2_ error-correction rules _/entity2_ for _entity3_ Thai key prediction _/entity3_ and _entity4_ Thai-English language identification _/entity4_ . The paper also proposes _entity5_ rule-reduction algorithm _/entity5_ applying _entity6_ mutual information _/entity6_ to reduce the _entity7_ error-correction rules _/entity7_ . Our algorithm reported more than 99 % _entity8_ _P_ accuracy _/entity8_ in both _entity9_ language identification _/entity9_ and _entity10_ _C_ key prediction _/entity10_ .	NONE entity8 entity10
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ _P_ training and test data _/entity5_ with respect to _entity6_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ _C_ training data _/entity7_ labeled with _entity8_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ domain _/entity9_ , _entity10_ topic _/entity10_ and time .	NONE entity5 entity7
In this paper , we present a _entity1_ corpus-based supervised word sense disambiguation ( WSD ) system _/entity1_ for _entity2_ Dutch _/entity2_ which combines _entity3_ statistical classification _/entity3_ ( _entity4_ maximum entropy _/entity4_ ) with _entity5_ linguistic information _/entity5_ . Instead of building individual _entity6_ classifiers _/entity6_ per _entity7_ ambiguous wordform _/entity7_ , we introduce a _entity8_ lemma-based approach _/entity8_ . The advantage of this novel method is that it clusters all _entity9_ inflected forms _/entity9_ of an _entity10_ ambiguous word _/entity10_ in one _entity11_ classifier _/entity11_ , therefore augmenting the _entity12_ training material _/entity12_ available to the _entity13_ algorithm _/entity13_ . Testing the _entity14_ _P_ lemma-based model _/entity14_ on the _entity15_ _C_ Dutch Senseval-2 test data _/entity15_ , we achieve a significant increase in _entity16_ accuracy _/entity16_ over the _entity17_ wordform model _/entity17_ . Also , the _entity18_ WSD system based on lemmas _/entity18_ is smaller and more robust .	USAGE entity14 entity15
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ _P_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ _C_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	PART_WHOLE entity8 entity9
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ feature structure-directed generation _/entity2_ , developed for a _entity3_ dialogue translation system _/entity3_ . The system utilizes _entity4_ typed feature structures _/entity4_ to control the _entity5_ top-down derivation _/entity5_ in a declarative way . This _entity6_ generation system _/entity6_ also uses _entity7_ _C_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ derivation tree _/entity8_ . The _entity9_ grammar _/entity9_ for this _entity10_ _P_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	NONE entity10 entity7
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ English _/entity7_ . We show how the limited _entity8_ _C_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ _P_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity11 entity8
In this paper , we explore correlation of _entity1_ dependency relation paths _/entity1_ to rank candidate answers in _entity2_ answer extraction _/entity2_ . Using the _entity3_ correlation measure _/entity3_ , we compare _entity4_ dependency relations _/entity4_ of a candidate answer and mapped _entity5_ question phrases _/entity5_ in _entity6_ sentence _/entity6_ with the corresponding _entity7_ relations _/entity7_ in question . Different from previous studies , we propose an _entity8_ approximate phrase mapping algorithm _/entity8_ and incorporate the _entity9_ mapping score _/entity9_ into the _entity10_ _C_ correlation measure _/entity10_ . The correlations are further incorporated into a _entity11_ Maximum Entropy-based ranking model _/entity11_ which estimates _entity12_ _P_ path weights _/entity12_ from training . Experimental results show that our method significantly outperforms state-of-the-art _entity13_ syntactic relation-based methods _/entity13_ by up to 20 % in _entity14_ MRR _/entity14_ .	NONE entity12 entity10
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ _C_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ _P_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity17 entity14
_entity1_ _C_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ _P_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity3 entity1
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ _C_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ _P_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity18 entity17
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ _C_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ _P_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity16 entity15
_entity1_ _C_ Machine transliteration/back-transliteration _/entity1_ plays an important role in many _entity2_ _P_ multilingual speech and language applications _/entity2_ . In this paper , a novel framework for _entity3_ machine transliteration/backtransliteration _/entity3_ that allows us to carry out _entity4_ direct orthographical mapping ( DOM ) _/entity4_ between two different _entity5_ languages _/entity5_ is presented . Under this framework , a _entity6_ joint source-channel transliteration model _/entity6_ , also called _entity7_ n-gram transliteration model ( n-gram TM ) _/entity7_ , is further proposed to model the _entity8_ transliteration process _/entity8_ . We evaluate the proposed methods through several _entity9_ transliteration/backtransliteration experiments _/entity9_ for _entity10_ English/Chinese and English/Japanese language pairs _/entity10_ . Our study reveals that the proposed method not only reduces an extensive _entity11_ system development effort _/entity11_ but also improves the _entity12_ transliteration accuracy _/entity12_ significantly .	NONE entity2 entity1
In this paper , we will describe a _entity1_ search tool _/entity1_ for a huge set of _entity2_ ngrams _/entity2_ . The tool supports _entity3_ queries _/entity3_ with an arbitrary number of _entity4_ _P_ wildcards _/entity4_ . It takes a fraction of a second for a search , and can provide the _entity5_ fillers _/entity5_ of the _entity6_ _C_ wildcards _/entity6_ . The system runs on a single Linux PC with reasonable size _entity7_ memory _/entity7_ ( less than 4GB ) and _entity8_ disk space _/entity8_ ( less than 400GB ) . This system can be a very useful tool for _entity9_ linguistic knowledge discovery _/entity9_ and other _entity10_ NLP tasks _/entity10_ .	NONE entity4 entity6
The reality of _entity1_ analogies between words _/entity1_ is refuted by noone ( e.g. , I walked is to to walk as I laughed is to to laugh , noted I walked : to walk : : I laughed : to laugh ) . But _entity2_ computational linguists _/entity2_ seem to be quite dubious about _entity3_ analogies between sentences _/entity3_ : they would not be enough numerous to be of any use . We report experiments conducted on a _entity4_ multilingual corpus _/entity4_ to estimate the number of _entity5_ analogies _/entity5_ among the _entity6_ sentences _/entity6_ that it contains . We give two estimates , a lower one and a higher one . As an _entity7_ analogy _/entity7_ must be valid on the level of _entity8_ _C_ form _/entity8_ as well as on the level of _entity9_ meaning _/entity9_ , we relied on the idea that _entity10_ translation _/entity10_ should preserve _entity11_ _P_ meaning _/entity11_ to test for similar _entity12_ meanings _/entity12_ .	NONE entity11 entity8
In this paper a novel solution to automatic and _entity1_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ _P_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ _C_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ WSI algorithm _/entity14_ .	NONE entity9 entity10
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ _C_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ _P_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity18 entity15
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ _C_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ _P_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity12 entity9
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ _C_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ _P_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity5 entity2
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ content-independent formalism _/entity5_ . Unlike _entity6_ _P_ logic _/entity6_ , the _entity7_ _C_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	COMPARE entity6 entity7
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ _P_ modification probabilities _/entity4_ ; how one _entity5_ _C_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity4 entity5
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ _C_ tree _/entity33_ and the selected _entity34_ _P_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity34 entity33
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ _C_ training and test data _/entity5_ with respect to _entity6_ _P_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ training data _/entity7_ labeled with _entity8_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ domain _/entity9_ , _entity10_ topic _/entity10_ and time .	MODEL-FEATURE entity6 entity5
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ _C_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ _P_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity14 entity11
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ _P_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ _C_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity14 entity16
We describe a method for identifying systematic _entity1_ patterns _/entity1_ in _entity2_ translation data _/entity2_ using _entity3_ _P_ part-of-speech tag sequences _/entity3_ . We incorporate this analysis into a _entity4_ _C_ diagnostic tool _/entity4_ intended for _entity5_ developers _/entity5_ of _entity6_ machine translation systems _/entity6_ , and demonstrate how our application can be used by _entity7_ developers _/entity7_ to explore _entity8_ patterns _/entity8_ in _entity9_ machine translation output _/entity9_ .	NONE entity3 entity4
This report describes _entity1_ _P_ Paul _/entity1_ , a _entity2_ _C_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity1 entity2
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ _P_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ _C_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity3 entity6
Multimodal interfaces require effective _entity1_ _P_ parsing _/entity1_ and understanding of _entity2_ utterances _/entity2_ whose content is distributed across multiple input modes . Johnston 1998 presents an approach in which strategies for _entity3_ multimodal integration _/entity3_ are stated declaratively using a _entity4_ _C_ unification-based grammar _/entity4_ that is used by a _entity5_ multidimensional chart parser _/entity5_ to compose inputs . This approach is highly expressive and supports a broad class of _entity6_ interfaces _/entity6_ , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input . In this paper , we present an alternative approach in which _entity7_ multimodal parsing and understanding _/entity7_ are achieved using a _entity8_ weighted finite-state device _/entity8_ which takes _entity9_ speech and gesture streams _/entity9_ as inputs and outputs their joint interpretation . This approach is significantly more efficient , enables tight-coupling of multimodal understanding with _entity10_ speech recognition _/entity10_ , and provides a general probabilistic framework for _entity11_ multimodal ambiguity resolution _/entity11_ .	NONE entity1 entity4
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ _C_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ _P_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity12 entity10
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ _P_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ _C_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity7 entity10
The _entity1_ interlingual approach to MT _/entity1_ has been repeatedly advocated by researchers originally interested in _entity2_ natural language understanding _/entity2_ who take _entity3_ machine translation _/entity3_ to be one possible application . However , not only the _entity4_ _C_ ambiguity _/entity4_ but also the vagueness which every _entity5_ _P_ natural language _/entity5_ inevitably has leads this approach into essential difficulties . In contrast , our project , the _entity6_ Mu-project _/entity6_ , adopts the _entity7_ transfer approach _/entity7_ as the basic framework of _entity8_ MT _/entity8_ . This paper describes the detailed construction of the _entity9_ transfer phase _/entity9_ of our system from _entity10_ Japanese _/entity10_ to _entity11_ English _/entity11_ , and gives some examples of problems which seem difficult to treat in the _entity12_ interlingual approach _/entity12_ . The basic design principles of the _entity13_ transfer phase _/entity13_ of our system have already been mentioned in ( 1 ) ( 2 ) . Some of the principles which are relevant to the topic of this paper are : ( a ) _entity14_ Multiple Layer of Grammars _/entity14_ ( b ) _entity15_ Multiple Layer Presentation _/entity15_ ( c ) _entity16_ Lexicon Driven Processing _/entity16_ ( d ) _entity17_ Form-Oriented Dictionary Description _/entity17_ . This paper also shows how these principles are realized in the current system .	NONE entity5 entity4
In this paper a novel solution to automatic and _entity1_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ _P_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ _C_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ WSI algorithm _/entity14_ .	NONE entity3 entity6
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ _P_ computational methods _/entity15_ of _entity16_ _C_ KDS _/entity16_ are described .	USAGE entity15 entity16
The _entity1_ interlingual approach to MT _/entity1_ has been repeatedly advocated by researchers originally interested in _entity2_ natural language understanding _/entity2_ who take _entity3_ machine translation _/entity3_ to be one possible application . However , not only the _entity4_ ambiguity _/entity4_ but also the vagueness which every _entity5_ natural language _/entity5_ inevitably has leads this approach into essential difficulties . In contrast , our project , the _entity6_ Mu-project _/entity6_ , adopts the _entity7_ transfer approach _/entity7_ as the basic framework of _entity8_ MT _/entity8_ . This paper describes the detailed construction of the _entity9_ _C_ transfer phase _/entity9_ of our system from _entity10_ Japanese _/entity10_ to _entity11_ English _/entity11_ , and gives some examples of problems which seem difficult to treat in the _entity12_ _P_ interlingual approach _/entity12_ . The basic design principles of the _entity13_ transfer phase _/entity13_ of our system have already been mentioned in ( 1 ) ( 2 ) . Some of the principles which are relevant to the topic of this paper are : ( a ) _entity14_ Multiple Layer of Grammars _/entity14_ ( b ) _entity15_ Multiple Layer Presentation _/entity15_ ( c ) _entity16_ Lexicon Driven Processing _/entity16_ ( d ) _entity17_ Form-Oriented Dictionary Description _/entity17_ . This paper also shows how these principles are realized in the current system .	NONE entity12 entity9
_entity1_ Terminology structuring _/entity1_ has been the subject of much work in the context of _entity2_ terms _/entity2_ extracted from _entity3_ corpora _/entity3_ : given a set of _entity4_ terms _/entity4_ , obtained from an existing resource or extracted from a _entity5_ corpus _/entity5_ , identifying _entity6_ hierarchical ( or other types of ) relations _/entity6_ between these _entity7_ terms _/entity7_ . The present paper focusses on _entity8_ terminology structuring _/entity8_ by _entity9_ lexical methods _/entity9_ , which match _entity10_ terms _/entity10_ on the basis on their _entity11_ _P_ content words _/entity11_ , taking _entity12_ morphological variants _/entity12_ into account . Experiments are done on a 'flat ' list of _entity13_ _C_ terms _/entity13_ obtained from an originally _entity14_ hierarchically-structured terminology _/entity14_ : the French version of the _entity15_ US National Library of Medicine MeSH thesaurus _/entity15_ . We compare the _entity16_ lexically-induced relations _/entity16_ with the original _entity17_ MeSH relations _/entity17_ : after a quantitative evaluation of their congruence through _entity18_ recall and precision metrics _/entity18_ , we perform a qualitative , human analysis ofthe 'new ' _entity19_ relations _/entity19_ not present in the _entity20_ MeSH _/entity20_ . This analysis shows , on the one hand , the limits of the _entity21_ lexical structuring method _/entity21_ . On the other hand , it also reveals some specific structuring choices and _entity22_ naming conventions _/entity22_ made by the _entity23_ MeSH _/entity23_ designers , and emphasizes ontological commitments that can not be left to _entity24_ automatic structuring _/entity24_ .	NONE entity11 entity13
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ _P_ stochastic parsers _/entity8_ trained on the _entity9_ _C_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity8 entity9
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ _C_ compounds _/entity14_ in _entity15_ _P_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity15 entity14
In this paper we discuss a proposed _entity1_ user knowledge modeling architecture _/entity1_ for the _entity2_ ICICLE system _/entity2_ , a _entity3_ language tutoring application _/entity3_ for deaf learners of _entity4_ written English _/entity4_ . The model will represent the _entity5_ language proficiency _/entity5_ of the user and is designed to be referenced during both _entity6_ writing analysis _/entity6_ and _entity7_ feedback production _/entity7_ . We motivate our _entity8_ model design _/entity8_ by citing relevant research on _entity9_ second language and cognitive skill acquisition _/entity9_ , and briefly discuss preliminary empirical evidence supporting the _entity10_ design _/entity10_ . We conclude by showing how our _entity11_ design _/entity11_ can provide a rich and _entity12_ _P_ robust information base _/entity12_ to a language assessment / correction application by modeling _entity13_ _C_ user proficiency _/entity13_ at a high level of granularity and specificity .	NONE entity12 entity13
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ _C_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ _P_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity10 entity8
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ _P_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ _C_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity10 entity12
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ _C_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ _P_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity7 entity6
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ _C_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ _P_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity6 entity4
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ _C_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ _P_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity7 entity5
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ editor _/entity3_ was to provide an adequate tool to help _entity4_ _C_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ linguistic theory _/entity6_ . If we want valuable _entity7_ _P_ lexicons _/entity7_ and _entity8_ grammars _/entity8_ to achieve complex _entity9_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity7 entity4
We propose a method of organizing reading materials for _entity1_ _P_ vocabulary learning _/entity1_ . It enables us to select a concise set of reading _entity2_ texts _/entity2_ ( from a _entity3_ target corpus _/entity3_ ) that contains all the _entity4_ _C_ target vocabulary _/entity4_ to be learned . We used a specialized _entity5_ vocabulary _/entity5_ for an English certification test as the _entity6_ target vocabulary _/entity6_ and used _entity7_ English Wikipedia _/entity7_ , a free-content encyclopedia , as the _entity8_ target corpus _/entity8_ . The organized reading materials would enable learners not only to study the _entity9_ target vocabulary _/entity9_ efficiently but also to gain a variety of knowledge through reading . The reading materials are available on our web site .	NONE entity1 entity4
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ _C_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ _P_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity20 entity18
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ _C_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ _P_ dictionary-based word vectors _/entity10_ better reflect _entity11_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ associative similarity _/entity13_ .	NONE entity10 entity7
The _entity1_ TAP-XL Automated Analyst 's Assistant _/entity1_ is an application designed to help an _entity2_ English _/entity2_ -speaking analyst write a _entity3_ _C_ topical report _/entity3_ , culling information from a large inflow of _entity4_ multilingual , multimedia data _/entity4_ . It gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other _entity5_ languages _/entity5_ by leveraging _entity6_ _P_ human language technology _/entity6_ .	NONE entity6 entity3
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ _C_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ _P_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity12 entity9
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ _C_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ _P_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity19 entity16
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ _C_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ _P_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ signal-based document processing functionality _/entity15_ .	NONE entity12 entity10
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ _P_ terabyte corpus _/entity19_ to answer _entity20_ _C_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity19 entity20
While _entity1_ _C_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ _P_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity4 entity1
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ _P_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ _C_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ phrases _/entity12_ longer than three _entity13_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity4 entity7
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ _C_ discourse _/entity13_ into which the _entity14_ _P_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	PART_WHOLE entity14 entity13
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ _C_ processing paradigm _/entity2_ called _entity3_ _P_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity3 entity2
_entity1_ Large-scale natural language generation _/entity1_ requires the integration of vast amounts of _entity2_ _C_ knowledge _/entity2_ : lexical , grammatical , and conceptual . A _entity3_ robust generator _/entity3_ must be able to operate well even when pieces of _entity4_ knowledge _/entity4_ are missing . It must also be robust against _entity5_ _P_ incomplete or inaccurate inputs _/entity5_ . To attack these problems , we have built a _entity6_ hybrid generator _/entity6_ , in which gaps in _entity7_ symbolic knowledge _/entity7_ are filled by _entity8_ statistical methods _/entity8_ . We describe algorithms and show experimental results . We also discuss how the _entity9_ hybrid generation model _/entity9_ can be used to simplify current _entity10_ generators _/entity10_ and enhance their _entity11_ portability _/entity11_ , even when perfect _entity12_ knowledge _/entity12_ is in principle obtainable .	NONE entity5 entity2
This paper presents a _entity1_ maximum entropy word alignment algorithm _/entity1_ for _entity2_ Arabic-English _/entity2_ based on _entity3_ supervised training data _/entity3_ . We demonstrate that it is feasible to create _entity4_ training material _/entity4_ for problems in _entity5_ machine translation _/entity5_ and that a mixture of _entity6_ supervised and unsupervised methods _/entity6_ yields superior _entity7_ performance _/entity7_ . The _entity8_ probabilistic model _/entity8_ used in the _entity9_ _P_ alignment _/entity9_ directly models the _entity10_ link decisions _/entity10_ . Significant improvement over traditional _entity11_ word alignment techniques _/entity11_ is shown as well as improvement on several _entity12_ _C_ machine translation tests _/entity12_ . Performance of the algorithm is contrasted with _entity13_ human annotation performance _/entity13_ .	NONE entity9 entity12
We present an operable definition of _entity1_ focus _/entity1_ which is argued to be of a cognito-pragmatic nature and explore how it is determined in _entity2_ discourse _/entity2_ in a formalized manner . For this purpose , a file card model of _entity3_ discourse model _/entity3_ and _entity4_ knowledge store _/entity4_ is introduced enabling the _entity5_ _C_ decomposition _/entity5_ and _entity6_ formal representation _/entity6_ of its _entity7_ _P_ determination process _/entity7_ as a programmable algorithm ( _entity8_ FDA _/entity8_ ) . Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of _entity9_ focus _/entity9_ via _entity10_ FDA _/entity10_ as a _entity11_ discourse-level construct _/entity11_ into _entity12_ speech synthesis systems _/entity12_ , in particular , _entity13_ concept-to-speech systems _/entity13_ , is also briefly discussed .	NONE entity7 entity5
A proper treatment of _entity1_ syntax _/entity1_ and _entity2_ semantics _/entity2_ in _entity3_ machine translation _/entity3_ is introduced and discussed from the empirical viewpoint . For _entity4_ English-Japanese machine translation _/entity4_ , the _entity5_ syntax directed approach _/entity5_ is effective where the _entity6_ Heuristic Parsing Model ( HPM ) _/entity6_ and the _entity7_ Syntactic Role System _/entity7_ play important roles . For _entity8_ Japanese-English translation _/entity8_ , the _entity9_ semantics directed approach _/entity9_ is powerful where the _entity10_ Conceptual Dependency Diagram ( CDD ) _/entity10_ and the _entity11_ Augmented Case Marker System _/entity11_ ( which is a kind of _entity12_ _C_ Semantic Role System _/entity12_ ) play essential roles . Some examples of the difference between _entity13_ Japanese sentence structure _/entity13_ and _entity14_ English sentence structure _/entity14_ , which is vital to _entity15_ _P_ machine translation _/entity15_ are also discussed together with various interesting _entity16_ ambiguities _/entity16_ .	NONE entity15 entity12
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ multiple speakers _/entity5_ without _entity6_ _C_ explicit speaker enrollment _/entity6_ . Combining _entity7_ _P_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity7 entity6
The unique properties of _entity1_ tree-adjoining grammars ( TAG ) _/entity1_ present a challenge for the application of _entity2_ TAGs _/entity2_ beyond the limited confines of _entity3_ syntax _/entity3_ , for instance , to the task of _entity4_ semantic interpretation _/entity4_ or _entity5_ automatic translation of natural language _/entity5_ . We present a variant of _entity6_ TAGs _/entity6_ , called _entity7_ synchronous TAGs _/entity7_ , which characterize correspondences between _entity8_ _C_ languages _/entity8_ . The formalism 's intended usage is to relate _entity9_ expressions of natural languages _/entity9_ to their associated _entity10_ _P_ semantics _/entity10_ represented in a _entity11_ logical form language _/entity11_ , or to their _entity12_ translates _/entity12_ in another _entity13_ natural language _/entity13_ ; in summary , we intend it to allow _entity14_ TAGs _/entity14_ to be used beyond their role in _entity15_ syntax proper _/entity15_ . We discuss the application of _entity16_ synchronous TAGs _/entity16_ to concrete examples , mentioning primarily in passing some computational issues that arise in its interpretation .	NONE entity10 entity8
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ word-aligned corpora _/entity5_ is proposed . A _entity6_ _C_ statistical translation model _/entity6_ is also presented that deals such _entity7_ phrases _/entity7_ , as well as a _entity8_ training method _/entity8_ based on the maximization of _entity9_ _P_ translation accuracy _/entity9_ , as measured with the _entity10_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity9 entity6
Current _entity1_ natural language interfaces _/entity1_ have concentrated largely on determining the literal _entity2_ meaning _/entity2_ of _entity3_ input _/entity3_ from their _entity4_ users _/entity4_ . While such _entity5_ decoding _/entity5_ is an essential underpinning , much recent work suggests that _entity6_ natural language interfaces _/entity6_ will never appear cooperative or graceful unless they also incorporate numerous _entity7_ non-literal aspects of communication _/entity7_ , such as robust _entity8_ communication procedures _/entity8_ . This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these _entity9_ non-literal aspects of communication _/entity9_ ; that the new technology of powerful _entity10_ personal computers _/entity10_ with integral _entity11_ graphics displays _/entity11_ offers techniques superior to those of humans for these aspects , while still satisfying _entity12_ _P_ human communication needs _/entity12_ . The paper proposes _entity13_ interfaces _/entity13_ based on a judicious mixture of these techniques and the still valuable methods of more traditional _entity14_ _C_ natural language interfaces _/entity14_ .	NONE entity12 entity14
An attempt has been made to use an _entity1_ Augmented Transition Network _/entity1_ as a procedural _entity2_ dialog model _/entity2_ . The development of such a _entity3_ model _/entity3_ appears to be important in several respects : as a device to represent and to use different _entity4_ dialog schemata _/entity4_ proposed in empirical _entity5_ _C_ conversation analysis _/entity5_ ; as a device to represent and to use _entity6_ _P_ models of verbal interaction _/entity6_ ; as a device combining knowledge about _entity7_ dialog schemata _/entity7_ and about _entity8_ verbal interaction _/entity8_ with knowledge about _entity9_ task-oriented and goal-directed dialogs _/entity9_ . A standard _entity10_ ATN _/entity10_ should be further developed in order to account for the _entity11_ verbal interactions _/entity11_ of _entity12_ task-oriented dialogs _/entity12_ .	NONE entity6 entity5
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ _C_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ _P_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity12 entity9
In this paper a novel solution to automatic and _entity1_ _C_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ _P_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ WSI algorithm _/entity14_ .	NONE entity3 entity1
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ _C_ graph _/entity18_ . We compare two _entity19_ _P_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity19 entity18
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ _C_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ _P_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity20 entity17
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ _C_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ _P_ NLP structures _/entity20_ other than _entity21_ parse trees _/entity21_ .	NONE entity20 entity17
This paper presents necessary and sufficient conditions for the use of _entity1_ demonstrative expressions _/entity1_ in _entity2_ English _/entity2_ and discusses implications for current _entity3_ discourse processing algorithms _/entity3_ . We examine a broad range of _entity4_ _P_ texts _/entity4_ to show how the distribution of _entity5_ demonstrative forms and functions _/entity5_ is _entity6_ _C_ genre dependent _/entity6_ . This research is part of a larger study of _entity7_ anaphoric expressions _/entity7_ , the results of which will be incorporated into a _entity8_ natural language generation system _/entity8_ .	NONE entity4 entity6
In this paper , we reported experiments of _entity1_ unsupervised automatic acquisition _/entity1_ of _entity2_ Italian and English verb subcategorization frames ( SCFs ) _/entity2_ from _entity3_ general and domain corpora _/entity3_ . The proposed technique operates on _entity4_ syntactically shallow-parsed corpora _/entity4_ on the basis of a limited number of _entity5_ search heuristics _/entity5_ not relying on any previous _entity6_ lexico-syntactic knowledge _/entity6_ about _entity7_ SCFs _/entity7_ . Although preliminary , reported results are in line with _entity8_ state-of-the-art lexical acquisition systems _/entity8_ . The issue of whether _entity9_ verbs _/entity9_ sharing similar _entity10_ SCFs distributions _/entity10_ happen to share _entity11_ similar semantic properties _/entity11_ as well was also explored by clustering _entity12_ _C_ verbs _/entity12_ that share _entity13_ _P_ frames _/entity13_ with the same _entity14_ distribution _/entity14_ using the _entity15_ Minimum Description Length Principle ( MDL ) _/entity15_ . First experiments in this direction were carried out on _entity16_ Italian verbs _/entity16_ with encouraging results .	MODEL-FEATURE entity13 entity12
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ _C_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ _P_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity14 entity11
This paper describes an implemented program that takes a _entity1_ tagged text corpus _/entity1_ and generates a partial list of the _entity2_ subcategorization frames _/entity2_ in which each _entity3_ verb _/entity3_ occurs . The completeness of the output list increases monotonically with the total _entity4_ occurrences _/entity4_ of each _entity5_ _P_ verb _/entity5_ in the _entity6_ training corpus _/entity6_ . _entity7_ False positive rates _/entity7_ are one to three percent . Five _entity8_ _C_ subcategorization frames _/entity8_ are currently detected and we foresee no impediment to detecting many more . Ultimately , we expect to provide a large _entity9_ subcategorization dictionary _/entity9_ to the _entity10_ NLP community _/entity10_ and to train _entity11_ dictionaries _/entity11_ for specific _entity12_ corpora _/entity12_ .	NONE entity5 entity8
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ electronic message stream _/entity8_ , such as a news wire . _entity9_ _P_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ _C_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity9 entity12
_entity1_ Large-scale natural language generation _/entity1_ requires the integration of vast amounts of _entity2_ knowledge _/entity2_ : lexical , grammatical , and conceptual . A _entity3_ robust generator _/entity3_ must be able to operate well even when pieces of _entity4_ knowledge _/entity4_ are missing . It must also be robust against _entity5_ incomplete or inaccurate inputs _/entity5_ . To attack these problems , we have built a _entity6_ hybrid generator _/entity6_ , in which gaps in _entity7_ symbolic knowledge _/entity7_ are filled by _entity8_ statistical methods _/entity8_ . We describe algorithms and show experimental results . We also discuss how the _entity9_ hybrid generation model _/entity9_ can be used to simplify current _entity10_ generators _/entity10_ and enhance their _entity11_ _C_ portability _/entity11_ , even when perfect _entity12_ _P_ knowledge _/entity12_ is in principle obtainable .	NONE entity12 entity11
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ _P_ parsing _/entity13_ with a _entity14_ _C_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity13 entity14
Informally , a _entity1_ disposition _/entity1_ is a _entity2_ proposition _/entity2_ which is preponderantly , but not necessarily always , true . For example , birds can fly is a _entity3_ disposition _/entity3_ , as are the _entity4_ propositions _/entity4_ Swedes are blond and Spaniards are dark . An idea which underlies the theory described in this paper is that a _entity5_ disposition _/entity5_ may be viewed as a _entity6_ proposition _/entity6_ with implicit _entity7_ fuzzy quantifiers _/entity7_ which are approximations to all and always , e.g. , almost all , almost always , most , frequently , etc . For example , birds can fly may be interpreted as the result of suppressing the _entity8_ fuzzy quantifier _/entity8_ most in the _entity9_ proposition _/entity9_ most birds can fly . Similarly , young men like young women may be read as most young men like mostly young women . The process of transforming a _entity10_ disposition _/entity10_ into a _entity11_ proposition _/entity11_ is referred to as _entity12_ explicitation _/entity12_ or _entity13_ restoration _/entity13_ . _entity14_ Explicitation _/entity14_ sets the stage for representing the _entity15_ meaning _/entity15_ of a _entity16_ proposition _/entity16_ through the use of _entity17_ _P_ test-score semantics _/entity17_ ( Zadeh , 1978 , 1982 ) . In this approach to _entity18_ _C_ semantics _/entity18_ , the _entity19_ meaning _/entity19_ of a _entity20_ proposition _/entity20_ , p , is represented as a procedure which tests , scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to _entity21_ reasoning with dispositions _/entity21_ which is based on the concept of a _entity22_ fuzzy syllogism _/entity22_ . _entity23_ Syllogistic reasoning with dispositions _/entity23_ has an important bearing on _entity24_ commonsense reasoning _/entity24_ as well as on the _entity25_ management of uncertainty _/entity25_ in _entity26_ expert systems _/entity26_ . As a simple application of the techniques described in this paper , we formulate a definition of _entity27_ typicality _/entity27_ -- a concept which plays an important role in _entity28_ human cognition _/entity28_ and is of relevance to _entity29_ default reasoning _/entity29_ .	NONE entity17 entity18
In this paper , we describe a search procedure for _entity1_ statistical machine translation ( MT ) _/entity1_ based on _entity2_ dynamic programming ( DP ) _/entity2_ . Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible _entity3_ word reordering _/entity3_ between _entity4_ source and target language _/entity4_ in order to achieve an efficient search algorithm . A search restriction especially useful for the translation direction from German to English is presented . The experimental tests are carried out on the _entity5_ _C_ Verbmobil task _/entity5_ ( German-English , 8000-word vocabulary ) , which is a _entity6_ _P_ limited-domain spoken-language task _/entity6_ .	MODEL-FEATURE entity6 entity5
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ information retrieval techniques _/entity5_ use a _entity6_ _P_ histogram _/entity6_ of _entity7_ _C_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity6 entity7
This paper presents a _entity1_ word segmentation system _/entity1_ in France Telecom R & D Beijing , which uses a unified approach to _entity2_ word breaking _/entity2_ and _entity3_ OOV identification _/entity3_ . The _entity4_ output _/entity4_ can be customized to meet different _entity5_ segmentation standards _/entity5_ through the application of an ordered list of transformation . The _entity6_ system _/entity6_ participated in all the tracks of the _entity7_ segmentation bakeoff _/entity7_ -- _entity8_ PK-open _/entity8_ , _entity9_ PK-closed _/entity9_ , _entity10_ AS-open _/entity10_ , _entity11_ AS-closed _/entity11_ , _entity12_ HK-open _/entity12_ , _entity13_ HK-closed _/entity13_ , _entity14_ MSR-open _/entity14_ and _entity15_ MSR- closed _/entity15_ -- and achieved the _entity16_ state-of-the-art performance _/entity16_ in _entity17_ _P_ MSR-open _/entity17_ , _entity18_ MSR-close _/entity18_ and _entity19_ _C_ PK-open _/entity19_ tracks . Analysis of the results shows that each component of the system contributed to the _entity20_ scores _/entity20_ .	NONE entity17 entity19
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ _P_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ _C_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity15 entity18
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ _P_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ _C_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	USAGE entity12 entity13
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ _P_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ _C_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity20 entity23
This paper presents a _entity1_ maximum entropy word alignment algorithm _/entity1_ for _entity2_ Arabic-English _/entity2_ based on _entity3_ supervised training data _/entity3_ . We demonstrate that it is feasible to create _entity4_ training material _/entity4_ for problems in _entity5_ machine translation _/entity5_ and that a mixture of _entity6_ supervised and unsupervised methods _/entity6_ yields superior _entity7_ performance _/entity7_ . The _entity8_ _P_ probabilistic model _/entity8_ used in the _entity9_ alignment _/entity9_ directly models the _entity10_ link decisions _/entity10_ . Significant improvement over traditional _entity11_ _C_ word alignment techniques _/entity11_ is shown as well as improvement on several _entity12_ machine translation tests _/entity12_ . Performance of the algorithm is contrasted with _entity13_ human annotation performance _/entity13_ .	NONE entity8 entity11
The _entity1_ LOGON MT demonstrator _/entity1_ assembles independently valuable _entity2_ general-purpose NLP components _/entity2_ into a _entity3_ machine translation pipeline _/entity3_ that capitalizes on _entity4_ _C_ output quality _/entity4_ . The demonstrator embodies an interesting combination of _entity5_ hand-built , symbolic resources _/entity5_ and _entity6_ _P_ stochastic processes _/entity6_ .	NONE entity6 entity4
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ classification-based representation techniques _/entity7_ can be applied to _entity8_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ semantic interpretation _/entity13_ was first explored in the _entity14_ _P_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ _C_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	NONE entity14 entity15
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ _C_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ _P_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity5 entity3
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ _C_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ restricted subset _/entity6_ of _entity7_ _P_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	NONE entity7 entity5
One of the major problems one is faced with when decomposing _entity1_ _P_ words _/entity1_ into their _entity2_ _C_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity1 entity2
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ _P_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ _C_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity6 entity8
_entity1_ _C_ Statistical machine translation ( SMT ) _/entity1_ is currently one of the hot spots in _entity2_ natural language processing _/entity2_ . Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that _entity3_ _P_ SMT _/entity3_ gives competitive results to _entity4_ rule-based translation systems _/entity4_ , requiring significantly less development time . This is particularly important when building _entity5_ translation systems _/entity5_ for new _entity6_ language pairs _/entity6_ or new _entity7_ domains _/entity7_ . This workshop is intended to give an introduction to _entity8_ statistical machine translation _/entity8_ with a focus on practical considerations . Participants should be able , after attending this workshop , to set out building an _entity9_ SMT system _/entity9_ themselves and achieving good _entity10_ baseline results _/entity10_ in a short time . The tutorial will cover the basics of _entity11_ SMT _/entity11_ : Theory will be put into practice . _entity12_ STTK _/entity12_ , a _entity13_ statistical machine translation tool kit _/entity13_ , will be introduced and used to build a working _entity14_ translation system _/entity14_ . _entity15_ STTK _/entity15_ has been developed by the presenter and co-workers over a number of years and is currently used as the basis of _entity16_ CMU 's SMT system _/entity16_ . It has also successfully been coupled with _entity17_ rule-based and example based machine translation modules _/entity17_ to build a _entity18_ multi engine machine translation system _/entity18_ . The _entity19_ source code _/entity19_ of the _entity20_ tool kit _/entity20_ will be made available .	NONE entity3 entity1
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ _C_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ _P_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity11 entity8
This paper discusses a _entity1_ decision-tree approach _/entity1_ to the problem of assigning _entity2_ probabilities _/entity2_ to _entity3_ _C_ words _/entity3_ following a given _entity4_ text _/entity4_ . In contrast with previous _entity5_ decision-tree language model attempts _/entity5_ , an algorithm for selecting _entity6_ _P_ nearly optimal questions _/entity6_ is considered . The model is to be tested on a standard task , _entity7_ The Wall Street Journal _/entity7_ , allowing a fair comparison with the well-known _entity8_ tri-gram model _/entity8_ .	NONE entity6 entity3
This paper describes an implemented program that takes a _entity1_ tagged text corpus _/entity1_ and generates a partial list of the _entity2_ _C_ subcategorization frames _/entity2_ in which each _entity3_ _P_ verb _/entity3_ occurs . The completeness of the output list increases monotonically with the total _entity4_ occurrences _/entity4_ of each _entity5_ verb _/entity5_ in the _entity6_ training corpus _/entity6_ . _entity7_ False positive rates _/entity7_ are one to three percent . Five _entity8_ subcategorization frames _/entity8_ are currently detected and we foresee no impediment to detecting many more . Ultimately , we expect to provide a large _entity9_ subcategorization dictionary _/entity9_ to the _entity10_ NLP community _/entity10_ and to train _entity11_ dictionaries _/entity11_ for specific _entity12_ corpora _/entity12_ .	NONE entity3 entity2
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ _C_ knowledge _/entity6_ or typical kinds of _entity7_ _P_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity7 entity6
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ _P_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ _C_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity8 entity9
_entity1_ _P_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ _C_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity1 entity4
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ _P_ categorization task _/entity9_ , _entity10_ _C_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ user-directed summaries _/entity18_ . The result shows that the _entity19_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity9 entity10
Currently several _entity1_ _P_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ _C_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity1 entity4
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ _C_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ _P_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity8 entity5
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ _C_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ _P_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity14 entity11
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ _P_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ _C_ priming _/entity3_ into an _entity4_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ rules _/entity6_ between _entity7_ sentences _/entity7_ , within _entity8_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity2 entity3
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ _P_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ _C_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	NONE entity15 entity17
In the past the evaluation of _entity1_ machine translation systems _/entity1_ has focused on single system evaluations because there were only few systems available . But now there are several commercial systems for the same _entity2_ _C_ language pair _/entity2_ . This requires new methods of comparative evaluation . In the paper we propose a _entity3_ black-box method _/entity3_ for comparing the _entity4_ lexical coverage _/entity4_ of _entity5_ _P_ MT systems _/entity5_ . The method is based on lists of _entity6_ words _/entity6_ from different _entity7_ frequency classes _/entity7_ . It is shown how these _entity8_ word lists _/entity8_ can be compiled and used for testing . We also present the results of using our method on 6 _entity9_ MT systems _/entity9_ that translate between _entity10_ English _/entity10_ and _entity11_ German _/entity11_ .	NONE entity5 entity2
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ _C_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ _P_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity10 entity8
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ interactive problem solving _/entity3_ . The system will accept _entity4_ continuous speech _/entity4_ , and will handle _entity5_ _C_ multiple speakers _/entity5_ without _entity6_ _P_ explicit speaker enrollment _/entity6_ . Combining _entity7_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity6 entity5
We present the first known _entity1_ empirical test _/entity1_ of an increasingly common speculative claim , by evaluating a representative _entity2_ Chinese-to-English SMT model _/entity2_ directly on _entity3_ word sense disambiguation performance _/entity3_ , using standard _entity4_ WSD evaluation methodology _/entity4_ and _entity5_ datasets _/entity5_ from the _entity6_ _C_ Senseval-3 Chinese lexical sample task _/entity6_ . Much effort has been put in designing and evaluating dedicated _entity7_ word sense disambiguation ( WSD ) models _/entity7_ , in particular with the _entity8_ Senseval _/entity8_ series of workshops . At the same time , the recent improvements in the _entity9_ _P_ BLEU scores _/entity9_ of _entity10_ statistical machine translation ( SMT ) _/entity10_ suggests that _entity11_ SMT models _/entity11_ are good at predicting the right _entity12_ translation _/entity12_ of the _entity13_ words _/entity13_ in _entity14_ source language sentences _/entity14_ . Surprisingly however , the _entity15_ WSD _/entity15_ _entity16_ accuracy _/entity16_ of _entity17_ SMT models _/entity17_ has never been evaluated and compared with that of the dedicated _entity18_ WSD models _/entity18_ . We present controlled experiments showing the _entity19_ WSD _/entity19_ _entity20_ accuracy _/entity20_ of current typical _entity21_ SMT models _/entity21_ to be significantly lower than that of all the dedicated _entity22_ WSD models _/entity22_ considered . This tends to support the view that despite recent speculative claims to the contrary , current _entity23_ SMT models _/entity23_ do have limitations in comparison with dedicated _entity24_ WSD models _/entity24_ , and that _entity25_ SMT _/entity25_ should benefit from the better predictions made by the _entity26_ WSD models _/entity26_ .	NONE entity9 entity6
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ character sets _/entity6_ should be treated differently and the changes between _entity7_ character types _/entity7_ are very important because _entity8_ Japanese script _/entity8_ has both _entity9_ _P_ ideograms _/entity9_ like _entity10_ _C_ Chinese _/entity10_ ( _entity11_ kanji _/entity11_ ) and _entity12_ phonograms _/entity12_ like _entity13_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity9 entity10
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ _C_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ _P_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity20 entity17
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ _P_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ _C_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity10 entity13
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ _C_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ _P_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ structured data _/entity11_ ( based on a _entity12_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity7 entity6
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ human interaction with data sources _/entity2_ . We integrate a _entity3_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ _C_ users _/entity5_ and _entity6_ _P_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity6 entity5
We present an efficient algorithm for the _entity1_ redundancy elimination problem _/entity1_ : Given an _entity2_ underspecified semantic representation ( USR ) _/entity2_ of a _entity3_ scope ambiguity _/entity3_ , compute an _entity4_ USR _/entity4_ with fewer mutually _entity5_ equivalent readings _/entity5_ . The algorithm operates on _entity6_ underspecified chart representations _/entity6_ which are derived from _entity7_ dominance graphs _/entity7_ ; it can be applied to the _entity8_ _P_ USRs _/entity8_ computed by _entity9_ large-scale grammars _/entity9_ . We evaluate the algorithm on a _entity10_ _C_ corpus _/entity10_ , and show that it reduces the degree of _entity11_ ambiguity _/entity11_ significantly while taking negligible runtime .	NONE entity8 entity10
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ _P_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ _C_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity24 entity26
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ _P_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ _C_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity2 entity3
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ _P_ paraphrases _/entity11_ under a given _entity12_ _C_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity11 entity12
To support engaging human users in robust , _entity1_ mixed-initiative speech dialogue interactions _/entity1_ which reach beyond current capabilities in _entity2_ dialogue systems _/entity2_ , the _entity3_ DARPA Communicator program _/entity3_ [ 1 ] is funding the development of a _entity4_ _C_ distributed message-passing infrastructure _/entity4_ for _entity5_ dialogue systems _/entity5_ which all _entity6_ Communicator _/entity6_ participants are using . In this presentation , we describe the features of and _entity7_ _P_ requirements _/entity7_ for a genuinely useful _entity8_ software infrastructure _/entity8_ for this purpose .	NONE entity7 entity4
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ _C_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ _P_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity11 entity8
We present _entity1_ _P_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ _C_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity1 entity4
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ feature structure-directed generation _/entity2_ , developed for a _entity3_ dialogue translation system _/entity3_ . The system utilizes _entity4_ typed feature structures _/entity4_ to control the _entity5_ top-down derivation _/entity5_ in a declarative way . This _entity6_ _C_ generation system _/entity6_ also uses _entity7_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ _P_ derivation tree _/entity8_ . The _entity9_ grammar _/entity9_ for this _entity10_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	NONE entity8 entity6
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ _C_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ _P_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity27 entity24
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ _C_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ _P_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity17 entity14
In this paper , we present a novel _entity1_ training method _/entity1_ for a _entity2_ _P_ localized phrase-based prediction model _/entity2_ for _entity3_ _C_ statistical machine translation ( SMT ) _/entity3_ . The _entity4_ model _/entity4_ predicts _entity5_ blocks _/entity5_ with orientation to handle _entity6_ local phrase re-ordering _/entity6_ . We use a _entity7_ maximum likelihood criterion _/entity7_ to train a _entity8_ log-linear block bigram model _/entity8_ which uses _entity9_ real-valued features _/entity9_ ( e.g . a _entity10_ language model score _/entity10_ ) as well as _entity11_ binary features _/entity11_ based on the _entity12_ block _/entity12_ identities themselves , e.g . block bigram features . Our _entity13_ training algorithm _/entity13_ can easily handle millions of _entity14_ features _/entity14_ . The best system obtains a 18.6 % improvement over the _entity15_ baseline _/entity15_ on a standard _entity16_ Arabic-English translation task _/entity16_ .	USAGE entity2 entity3
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ _C_ context _/entity19_ , constrain the possible _entity20_ _P_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity20 entity19
_entity1_ Systemic grammar _/entity1_ has been used for _entity2_ AI text generation _/entity2_ work in the past , but the _entity3_ implementations _/entity3_ have tended be ad hoc or inefficient . This paper presents an approach to systemic _entity4_ text generation _/entity4_ where _entity5_ _P_ AI problem solving techniques _/entity5_ are applied directly to an unadulterated _entity6_ _C_ systemic grammar _/entity6_ . This _entity7_ approach _/entity7_ is made possible by a special relationship between _entity8_ systemic grammar _/entity8_ and _entity9_ problem solving _/entity9_ : both are organized primarily as choosing from alternatives . The result is simple , efficient _entity10_ text generation _/entity10_ firmly based in a _entity11_ linguistic theory _/entity11_ .	USAGE entity5 entity6
In the past the evaluation of _entity1_ machine translation systems _/entity1_ has focused on single system evaluations because there were only few systems available . But now there are several commercial systems for the same _entity2_ language pair _/entity2_ . This requires new methods of comparative evaluation . In the paper we propose a _entity3_ _C_ black-box method _/entity3_ for comparing the _entity4_ lexical coverage _/entity4_ of _entity5_ _P_ MT systems _/entity5_ . The method is based on lists of _entity6_ words _/entity6_ from different _entity7_ frequency classes _/entity7_ . It is shown how these _entity8_ word lists _/entity8_ can be compiled and used for testing . We also present the results of using our method on 6 _entity9_ MT systems _/entity9_ that translate between _entity10_ English _/entity10_ and _entity11_ German _/entity11_ .	NONE entity5 entity3
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ _C_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ _P_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity11 entity10
This paper investigates some _entity1_ computational problems _/entity1_ associated with _entity2_ probabilistic translation models _/entity2_ that have recently been adopted in the literature on _entity3_ machine translation _/entity3_ . These _entity4_ models _/entity4_ can be viewed as pairs of _entity5_ probabilistic context-free grammars _/entity5_ working in a 'synchronous ' way . Two _entity6_ _P_ hardness _/entity6_ results for the class _entity7_ _C_ NP _/entity7_ are reported , along with an _entity8_ exponential time lower-bound _/entity8_ for certain classes of algorithms that are currently used in the literature .	NONE entity6 entity7
_entity1_ Word Identification _/entity1_ has been an important and active issue in _entity2_ Chinese Natural Language Processing _/entity2_ . In this paper , a new mechanism , based on the concept of _entity3_ _C_ sublanguage _/entity3_ , is proposed for identifying _entity4_ unknown words _/entity4_ , especially _entity5_ personal names _/entity5_ , in _entity6_ _P_ Chinese newspapers _/entity6_ . The proposed mechanism includes _entity7_ title-driven name recognition _/entity7_ , _entity8_ adaptive dynamic word formation _/entity8_ , _entity9_ identification of 2-character and 3-character Chinese names without title _/entity9_ . We will show the experimental results for two _entity10_ corpora _/entity10_ and compare them with the results by the _entity11_ NTHU 's statistic-based system _/entity11_ , the only system that we know has attacked the same problem . The experimental results have shown significant improvements over the _entity12_ WI systems _/entity12_ without the _entity13_ name identification _/entity13_ capability .	NONE entity6 entity3
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ _P_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ _C_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity15 entity16
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ _P_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ _C_ human judgment _/entity19_ .	NONE entity16 entity19
In this paper we present a novel , customizable : _entity1_ IE paradigm _/entity1_ that takes advantage of _entity2_ predicate-argument structures _/entity2_ . We also introduce a new way of automatically identifying _entity3_ predicate argument structures _/entity3_ , which is central to our _entity4_ IE paradigm _/entity4_ . It is based on : ( 1 ) an extended set of _entity5_ features _/entity5_ ; and ( 2 ) _entity6_ _P_ inductive decision tree learning _/entity6_ . The experimental results prove our claim that accurate _entity7_ _C_ predicate-argument structures _/entity7_ enable high quality _entity8_ IE _/entity8_ results .	NONE entity6 entity7
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ _C_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ _P_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity10 entity7
In this paper , a discrimination and robustness oriented _entity1_ adaptive learning procedure _/entity1_ is proposed to deal with the task of _entity2_ syntactic ambiguity resolution _/entity2_ . Owing to the problem of _entity3_ _P_ insufficient training data _/entity3_ and _entity4_ _C_ approximation error _/entity4_ introduced by the _entity5_ language model _/entity5_ , traditional _entity6_ statistical approaches _/entity6_ , which resolve _entity7_ ambiguities _/entity7_ by indirectly and implicitly using _entity8_ maximum likelihood method _/entity8_ , fail to achieve high _entity9_ performance _/entity9_ in real applications . The proposed method remedies these problems by adjusting the parameters to maximize the _entity10_ accuracy rate _/entity10_ directly . To make the proposed algorithm robust , the possible variations between the _entity11_ training corpus _/entity11_ and the real tasks are also taken into consideration by enlarging the _entity12_ separation margin _/entity12_ between the correct candidate and its competing members . Significant improvement has been observed in the test . The _entity13_ accuracy rate _/entity13_ of _entity14_ syntactic disambiguation _/entity14_ is raised from 46.0 % to 60.62 % by using this novel approach .	NONE entity3 entity4
This paper discusses two problems that arise in the _entity1_ Generation _/entity1_ of _entity2_ Referring Expressions _/entity2_ : ( a ) _entity3_ _C_ numeric-valued attributes _/entity3_ , such as size or location ; ( b ) _entity4_ perspective-taking _/entity4_ in _entity5_ _P_ reference _/entity5_ . Both problems , it is argued , can be resolved if some structure is imposed on the available knowledge prior to _entity6_ content determination _/entity6_ . We describe a _entity7_ clustering algorithm _/entity7_ which is sufficiently general to be applied to these diverse problems , discuss its application , and evaluate its performance .	NONE entity5 entity3
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ _C_ utterances _/entity14_ naturally aggregate . The _entity15_ _P_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity15 entity14
In this paper a novel solution to automatic and _entity1_ unsupervised word sense induction ( WSI ) _/entity1_ is introduced . It represents an instantiation of the _entity2_ one sense per collocation observation _/entity2_ ( Gale et al. , 1992 ) . Like most existing approaches it utilizes _entity3_ clustering of word co-occurrences _/entity3_ . This approach differs from other approaches to _entity4_ WSI _/entity4_ in that it enhances the effect of the _entity5_ one sense per collocation observation _/entity5_ by using triplets of _entity6_ words _/entity6_ instead of pairs . The combination with a _entity7_ two-step clustering process _/entity7_ using _entity8_ sentence co-occurrences _/entity8_ as _entity9_ _P_ features _/entity9_ allows for accurate results . Additionally , a novel and likewise automatic and _entity10_ unsupervised evaluation method _/entity10_ inspired by Schutze 's ( 1992 ) idea of evaluation of _entity11_ word sense disambiguation algorithms _/entity11_ is employed . Offering advantages like reproducability and independency of a given biased _entity12_ _C_ gold standard _/entity12_ it also enables _entity13_ automatic parameter optimization _/entity13_ of the _entity14_ WSI algorithm _/entity14_ .	NONE entity9 entity12
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ _C_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ _P_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity9 entity7
In this paper we show how two standard outputs from _entity1_ information extraction ( IE ) systems _/entity1_ - _entity2_ named entity annotations _/entity2_ and _entity3_ scenario templates _/entity3_ - can be used to enhance access to _entity4_ text collections _/entity4_ via a standard _entity5_ text browser _/entity5_ . We describe how this information is used in a _entity6_ prototype system _/entity6_ designed to support _entity7_ information workers _/entity7_ ' access to a _entity8_ _P_ pharmaceutical news archive _/entity8_ as part of their _entity9_ _C_ industry watch _/entity9_ function . We also report results of a preliminary , _entity10_ qualitative user evaluation _/entity10_ of the system , which while broadly positive indicates further work needs to be done on the _entity11_ interface _/entity11_ to make _entity12_ users _/entity12_ aware of the increased potential of _entity13_ IE-enhanced text browsers _/entity13_ .	NONE entity8 entity9
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ training and test data _/entity5_ with respect to _entity6_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ _C_ training data _/entity7_ labeled with _entity8_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ _P_ domain _/entity9_ , _entity10_ topic _/entity10_ and time .	NONE entity9 entity7
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ _P_ candidate parses _/entity3_ for each input _entity4_ _C_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	MODEL-FEATURE entity3 entity4
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ _P_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ _C_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity2 entity3
We examine the relationship between the two _entity1_ grammatical formalisms _/entity1_ : _entity2_ _C_ Tree Adjoining Grammars _/entity2_ and _entity3_ Head Grammars _/entity3_ . We briefly investigate the weak _entity4_ equivalence _/entity4_ of the two _entity5_ _P_ formalisms _/entity5_ . We then turn to a discussion comparing the _entity6_ linguistic expressiveness _/entity6_ of the two _entity7_ formalisms _/entity7_ .	NONE entity5 entity2
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ _P_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ _C_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity2 entity5
We investigate the utility of an _entity1_ algorithm for translation lexicon acquisition ( SABLE ) _/entity1_ , used previously on a very large _entity2_ corpus _/entity2_ to acquire general _entity3_ _P_ translation lexicons _/entity3_ , when that _entity4_ algorithm _/entity4_ is applied to a much smaller _entity5_ corpus _/entity5_ to produce candidates for _entity6_ _C_ domain-specific translation lexicons _/entity6_ .	NONE entity3 entity6
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ sentence plans _/entity12_ for a given _entity13_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ plan _/entity16_ . The _entity17_ SPR _/entity17_ uses _entity18_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ _C_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ _P_ top human-ranked sentence plan _/entity22_ .	NONE entity22 entity20
The _entity1_ _C_ interlingual approach to MT _/entity1_ has been repeatedly advocated by researchers originally interested in _entity2_ natural language understanding _/entity2_ who take _entity3_ _P_ machine translation _/entity3_ to be one possible application . However , not only the _entity4_ ambiguity _/entity4_ but also the vagueness which every _entity5_ natural language _/entity5_ inevitably has leads this approach into essential difficulties . In contrast , our project , the _entity6_ Mu-project _/entity6_ , adopts the _entity7_ transfer approach _/entity7_ as the basic framework of _entity8_ MT _/entity8_ . This paper describes the detailed construction of the _entity9_ transfer phase _/entity9_ of our system from _entity10_ Japanese _/entity10_ to _entity11_ English _/entity11_ , and gives some examples of problems which seem difficult to treat in the _entity12_ interlingual approach _/entity12_ . The basic design principles of the _entity13_ transfer phase _/entity13_ of our system have already been mentioned in ( 1 ) ( 2 ) . Some of the principles which are relevant to the topic of this paper are : ( a ) _entity14_ Multiple Layer of Grammars _/entity14_ ( b ) _entity15_ Multiple Layer Presentation _/entity15_ ( c ) _entity16_ Lexicon Driven Processing _/entity16_ ( d ) _entity17_ Form-Oriented Dictionary Description _/entity17_ . This paper also shows how these principles are realized in the current system .	NONE entity3 entity1
In this paper , we will describe a _entity1_ _C_ search tool _/entity1_ for a huge set of _entity2_ ngrams _/entity2_ . The tool supports _entity3_ queries _/entity3_ with an arbitrary number of _entity4_ _P_ wildcards _/entity4_ . It takes a fraction of a second for a search , and can provide the _entity5_ fillers _/entity5_ of the _entity6_ wildcards _/entity6_ . The system runs on a single Linux PC with reasonable size _entity7_ memory _/entity7_ ( less than 4GB ) and _entity8_ disk space _/entity8_ ( less than 400GB ) . This system can be a very useful tool for _entity9_ linguistic knowledge discovery _/entity9_ and other _entity10_ NLP tasks _/entity10_ .	NONE entity4 entity1
_entity1_ Sentence planning _/entity1_ is a set of inter-related but distinct tasks , one of which is _entity2_ sentence scoping _/entity2_ , i.e . the choice of _entity3_ syntactic structure _/entity3_ for elementary _entity4_ speech acts _/entity4_ and the decision of how to combine them into one or more _entity5_ sentences _/entity5_ . In this paper , we present _entity6_ SPoT _/entity6_ , a _entity7_ _P_ sentence planner _/entity7_ , and a new methodology for automatically training _entity8_ _C_ SPoT _/entity8_ on the basis of _entity9_ feedback _/entity9_ provided by _entity10_ human judges _/entity10_ . We reconceptualize the task into two distinct phases . First , a very simple , _entity11_ randomized sentence-plan-generator ( SPG ) _/entity11_ generates a potentially large list of possible _entity12_ sentence plans _/entity12_ for a given _entity13_ text-plan input _/entity13_ . Second , the _entity14_ sentence-plan-ranker ( SPR ) _/entity14_ ranks the list of output _entity15_ sentence plans _/entity15_ , and then selects the top-ranked _entity16_ plan _/entity16_ . The _entity17_ SPR _/entity17_ uses _entity18_ ranking rules _/entity18_ automatically learned from _entity19_ training data _/entity19_ . We show that the trained _entity20_ SPR _/entity20_ learns to select a _entity21_ sentence plan _/entity21_ whose rating on average is only 5 % worse than the _entity22_ top human-ranked sentence plan _/entity22_ .	NONE entity7 entity8
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ _C_ participants _/entity20_ as the _entity21_ _P_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity21 entity20
The _entity1_ JAVELIN system _/entity1_ integrates a flexible , _entity2_ planning-based architecture _/entity2_ with a variety of _entity3_ language processing modules _/entity3_ to provide an _entity4_ open-domain question answering capability _/entity4_ on _entity5_ free text _/entity5_ . The demonstration will focus on how _entity6_ JAVELIN _/entity6_ processes _entity7_ _P_ questions _/entity7_ and retrieves the most likely _entity8_ _C_ answer candidates _/entity8_ from the given _entity9_ text corpus _/entity9_ . The operation of the system will be explained in depth through browsing the _entity10_ repository _/entity10_ of _entity11_ data objects _/entity11_ created by the system during each _entity12_ question answering session _/entity12_ .	NONE entity7 entity8
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ _C_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ _P_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity15 entity13
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ _P_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ _C_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity2 entity5
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ output _/entity11_ of a robust _entity12_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ _C_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ _P_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity22 entity19
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ _C_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ _P_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity10 entity7
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ _C_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ _P_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity20 entity17
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ _C_ representation scheme _/entity13_ for _entity14_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ _P_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity16 entity13
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ _C_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ _P_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity39 entity37
_entity1_ Automatic estimation _/entity1_ of _entity2_ _C_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ _P_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity4 entity2
Sources of _entity1_ _P_ training data _/entity1_ suitable for _entity2_ language modeling _/entity2_ of _entity3_ _C_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	NONE entity1 entity3
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ _C_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ _P_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity18 entity15
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ _C_ canonical answer _/entity10_ , returning either true or false . The _entity11_ _P_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity11 entity10
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ _P_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ _C_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity2 entity4
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ _P_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ _C_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity4 entity7
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ _P_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ _C_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity4 entity7
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ _C_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ _P_ topics _/entity12_ of the _entity13_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity12 entity10
In this study , we propose a _entity1_ knowledge-independent method _/entity1_ for aligning _entity2_ terms _/entity2_ and thus extracting _entity3_ _P_ translations _/entity3_ from a _entity4_ small , domain-specific corpus _/entity4_ consisting of _entity5_ _C_ parallel English and Chinese court judgments _/entity5_ from Hong Kong . With a _entity6_ sentence-aligned corpus _/entity6_ , _entity7_ translation equivalences _/entity7_ are suggested by analysing the _entity8_ frequency profiles _/entity8_ of _entity9_ parallel concordances _/entity9_ . The method overcomes the limitations of _entity10_ conventional statistical methods _/entity10_ which require _entity11_ large corpora _/entity11_ to be effective , and _entity12_ lexical approaches _/entity12_ which depend on existing _entity13_ bilingual dictionaries _/entity13_ . Pilot testing on a _entity14_ parallel corpus _/entity14_ of about 113K _entity15_ Chinese words _/entity15_ and 120K _entity16_ English words _/entity16_ gives an encouraging 85 % _entity17_ precision _/entity17_ and 45 % _entity18_ recall _/entity18_ . Future work includes fine-tuning the _entity19_ algorithm _/entity19_ upon the analysis of the errors , and acquiring a _entity20_ translation lexicon _/entity20_ for _entity21_ legal terminology _/entity21_ by filtering out _entity22_ general terms _/entity22_ .	NONE entity3 entity5
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ _C_ generalized translation knowledge _/entity15_ for _entity16_ _P_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity16 entity15
This paper proposes a generic _entity1_ mathematical formalism _/entity1_ for the combination of various _entity2_ structures _/entity2_ : _entity3_ strings _/entity3_ , _entity4_ trees _/entity4_ , _entity5_ dags _/entity5_ , _entity6_ graphs _/entity6_ , and products of them . The _entity7_ polarization _/entity7_ of the objects of the _entity8_ elementary structures _/entity8_ controls the _entity9_ saturation _/entity9_ of the final _entity10_ structure _/entity10_ . This formalism is both elementary and powerful enough to strongly simulate many _entity11_ grammar formalisms _/entity11_ , such as _entity12_ rewriting systems _/entity12_ , _entity13_ _P_ dependency grammars _/entity13_ , _entity14_ TAG _/entity14_ , _entity15_ HPSG _/entity15_ and _entity16_ _C_ LFG _/entity16_ .	NONE entity13 entity16
This paper describes a new , _entity1_ large scale discourse-level annotation _/entity1_ project - the _entity2_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ discourse structure _/entity3_ that is based on identifying _entity4_ discourse connectives _/entity4_ and their _entity5_ arguments _/entity5_ . The _entity6_ PDTB _/entity6_ is being built directly on top of the _entity7_ _P_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ _C_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ inter-annotator agreement _/entity11_ - both the _entity12_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity7 entity9
We consider the problem of _entity1_ question-focused sentence retrieval _/entity1_ from complex _entity2_ news articles _/entity2_ describing _entity3_ multi-event stories published over time _/entity3_ . _entity4_ _C_ Annotators _/entity4_ generated a list of _entity5_ questions _/entity5_ central to understanding each _entity6_ _P_ story _/entity6_ in our _entity7_ corpus _/entity7_ . Because of the dynamic nature of the _entity8_ stories _/entity8_ , many _entity9_ questions _/entity9_ are time-sensitive ( e.g . `` How many victims have been found ? '' ) . _entity10_ Judges _/entity10_ found _entity11_ sentences _/entity11_ providing an _entity12_ answer _/entity12_ to each _entity13_ question _/entity13_ . To address the _entity14_ sentence retrieval problem _/entity14_ , we apply a _entity15_ stochastic , graph-based method _/entity15_ for comparing the relative importance of the _entity16_ textual units _/entity16_ , which was previously used successfully for _entity17_ generic summarization _/entity17_ . Currently , we present a topic-sensitive version of our _entity18_ method _/entity18_ and hypothesize that it can outperform a competitive _entity19_ baseline _/entity19_ , which compares the _entity20_ similarity _/entity20_ of each _entity21_ sentence _/entity21_ to the input _entity22_ question _/entity22_ via _entity23_ IDF-weighted word overlap _/entity23_ . In our experiments , the _entity24_ method _/entity24_ achieves a _entity25_ TRDR score _/entity25_ that is significantly higher than that of the _entity26_ baseline _/entity26_ .	NONE entity6 entity4
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ _C_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ _P_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity18 entity15
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ _C_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ _P_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity5 entity3
Towards deep analysis of _entity1_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ class-oriented framework _/entity2_ for collecting _entity3_ paraphrase examples _/entity3_ , in which _entity4_ sentential paraphrases _/entity4_ are collected for each _entity5_ paraphrase class _/entity5_ separately by means of _entity6_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ _P_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ _C_ cost-efficiency _/entity9_ , _entity10_ exhaustiveness _/entity10_ , and _entity11_ reliability _/entity11_ .	NONE entity8 entity9
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ temporal expressions _/entity2_ in _entity3_ _C_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ temporal expressions _/entity4_ in a novel _entity5_ _P_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ expressions _/entity6_ fits well with our _entity7_ constraint-based representation _/entity7_ of time , _entity8_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ baseline _/entity10_ , and compares favorably with some of the closely related work .	NONE entity5 entity3
This abstract describes a _entity1_ _P_ natural language system _/entity1_ which deals usefully with _entity2_ _C_ ungrammatical input _/entity2_ and describes some actual and potential applications of it in _entity3_ computer aided second language learning _/entity3_ . However , this is not the only area in which the principles of the system might be used , and the aim in building it was simply to demonstrate the workability of the general mechanism , and provide a framework for assessing developments of it .	USAGE entity1 entity2
In this paper , we present a _entity1_ fully automated extraction system _/entity1_ , named _entity2_ IntEx _/entity2_ , to identify _entity3_ gene and protein interactions _/entity3_ in _entity4_ biomedical text _/entity4_ . Our approach is based on first splitting _entity5_ complex sentences _/entity5_ into _entity6_ simple clausal structures _/entity6_ made up of _entity7_ syntactic roles _/entity7_ . Then , tagging _entity8_ biological entities _/entity8_ with the help of _entity9_ biomedical and linguistic ontologies _/entity9_ . Finally , extracting _entity10_ _C_ complete interactions _/entity10_ by analyzing the matching contents of _entity11_ syntactic roles _/entity11_ and their linguistically significant combinations . Our _entity12_ _P_ extraction system _/entity12_ handles _entity13_ complex sentences _/entity13_ and extracts _entity14_ multiple and nested interactions _/entity14_ specified in a _entity15_ sentence _/entity15_ . Experimental evaluations with two other state of the art _entity16_ extraction systems _/entity16_ indicate that the _entity17_ IntEx system _/entity17_ achieves better _entity18_ performance _/entity18_ without the labor intensive _entity19_ pattern engineering requirement _/entity19_ .	NONE entity12 entity10
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ _C_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ _P_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity8 entity6
Towards deep analysis of _entity1_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ class-oriented framework _/entity2_ for collecting _entity3_ paraphrase examples _/entity3_ , in which _entity4_ sentential paraphrases _/entity4_ are collected for each _entity5_ paraphrase class _/entity5_ separately by means of _entity6_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ cost-efficiency _/entity9_ , _entity10_ _P_ exhaustiveness _/entity10_ , and _entity11_ _C_ reliability _/entity11_ .	NONE entity10 entity11
We describe a simple _entity1_ unsupervised technique _/entity1_ for learning _entity2_ morphology _/entity2_ by identifying _entity3_ hubs _/entity3_ in an _entity4_ automaton _/entity4_ . For our purposes , a _entity5_ hub _/entity5_ is a _entity6_ node _/entity6_ in a _entity7_ graph _/entity7_ with _entity8_ in-degree _/entity8_ greater than one and _entity9_ out-degree _/entity9_ greater than one . We create a _entity10_ word-trie _/entity10_ , transform it into a _entity11_ minimal DFA _/entity11_ , then identify _entity12_ hubs _/entity12_ . Those _entity13_ hubs _/entity13_ mark the boundary between _entity14_ _C_ root _/entity14_ and _entity15_ _P_ suffix _/entity15_ , achieving similar _entity16_ performance _/entity16_ to more complex mixtures of techniques .	NONE entity15 entity14
We propose a solution to the challenge of the _entity1_ CoNLL 2008 shared task _/entity1_ that uses a _entity2_ generative history-based latent variable model _/entity2_ to predict the most likely _entity3_ derivation _/entity3_ of a _entity4_ synchronous dependency parser _/entity4_ for both _entity5_ syntactic and semantic dependencies _/entity5_ . The submitted _entity6_ model _/entity6_ yields 79.1 % _entity7_ macro-average F1 performance _/entity7_ , for the joint task , 86.9 % _entity8_ syntactic dependencies LAS _/entity8_ and 71.0 % _entity9_ _C_ semantic dependencies F1 _/entity9_ . A larger _entity10_ _P_ model _/entity10_ trained after the deadline achieves 80.5 % _entity11_ macro-average F1 _/entity11_ , 87.6 % _entity12_ syntactic dependencies LAS _/entity12_ , and 73.1 % _entity13_ semantic dependencies F1 _/entity13_ .	NONE entity10 entity9
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ _C_ classification-based representation techniques _/entity7_ can be applied to _entity8_ _P_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ semantic interpretation _/entity13_ was first explored in the _entity14_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	NONE entity8 entity7
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ _C_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ _P_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity19 entity17
In this paper , we improve an _entity1_ unsupervised learning method _/entity1_ using the _entity2_ _C_ Expectation-Maximization ( EM ) algorithm _/entity2_ proposed by Nigam et al . for _entity3_ text classification problems _/entity3_ in order to apply it to _entity4_ word sense disambiguation ( WSD ) problems _/entity4_ . The improved method stops the _entity5_ _P_ EM algorithm _/entity5_ at the _entity6_ optimum iteration number _/entity6_ . To estimate that number , we propose two methods . In experiments , we solved 50 _entity7_ noun WSD problems _/entity7_ in the _entity8_ Japanese Dictionary Task in SENSEVAL2 _/entity8_ . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for _entity9_ verb WSD problems _/entity9_ .	NONE entity5 entity2
This paper proposes an _entity1_ alignment adaptation approach _/entity1_ to improve _entity2_ _C_ domain-specific ( in-domain ) word alignment _/entity2_ . The basic idea of _entity3_ _P_ alignment adaptation _/entity3_ is to use _entity4_ out-of-domain corpus _/entity4_ to improve _entity5_ in-domain word alignment _/entity5_ results . In this paper , we first train two _entity6_ statistical word alignment models _/entity6_ with the large-scale _entity7_ out-of-domain corpus _/entity7_ and the small-scale _entity8_ in-domain corpus _/entity8_ respectively , and then interpolate these two models to improve the _entity9_ domain-specific word alignment _/entity9_ . Experimental results show that our approach improves _entity10_ domain-specific word alignment _/entity10_ in terms of both _entity11_ precision _/entity11_ and _entity12_ recall _/entity12_ , achieving a _entity13_ relative error rate reduction _/entity13_ of 6.56 % as compared with the state-of-the-art technologies .	NONE entity3 entity2
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ _C_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ _P_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity16 entity14
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ _P_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ _C_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity16 entity18
In order to meet the needs of a publication of papers in English , many systems to run off texts have been developed . In this paper , we report a system _entity1_ _P_ FROFF _/entity1_ which can make a fair copy of not only texts but also graphs and tables indispensable to our papers . Its selection of _entity2_ fonts _/entity2_ , specification of _entity3_ character _/entity3_ size are dynamically changeable , and the _entity4_ _C_ typing location _/entity4_ can be also changed in lateral or longitudinal directions . Each _entity5_ character _/entity5_ has its own width and a line length is counted by the sum of each _entity6_ character _/entity6_ . By using commands or _entity7_ rules _/entity7_ which are defined to facilitate the construction of format expected or some _entity8_ mathematical expressions _/entity8_ , elaborate and pretty documents can be successfully obtained .	NONE entity1 entity4
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ _P_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ _C_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity32 entity35
This poster paper describes a _entity1_ full scale two-level morphological description _/entity1_ ( Karttunen , 1983 ; Koskenniemi , 1983 ) of _entity2_ Turkish word structures _/entity2_ . The description has been implemented using the _entity3_ PC-KIMMO environment _/entity3_ ( Antworth , 1990 ) and is based on a _entity4_ root word lexicon _/entity4_ of about 23,000 _entity5_ roots words _/entity5_ . Almost all the special cases of and exceptions to _entity6_ phonological and morphological rules _/entity6_ have been implemented . _entity7_ Turkish _/entity7_ is an _entity8_ agglutinative language _/entity8_ with _entity9_ word structures _/entity9_ formed by _entity10_ productive affixations of derivational and inflectional suffixes _/entity10_ to _entity11_ root words _/entity11_ . _entity12_ _P_ Turkish _/entity12_ has _entity13_ finite-state _/entity13_ but nevertheless rather complex morphotactics . _entity14_ _C_ Morphemes _/entity14_ added to a _entity15_ root word _/entity15_ or a _entity16_ stem _/entity16_ can convert the _entity17_ word _/entity17_ from a _entity18_ nominal _/entity18_ to a _entity19_ verbal structure _/entity19_ or vice-versa , or can create _entity20_ adverbial constructs _/entity20_ . The _entity21_ surface realizations _/entity21_ of _entity22_ morphological constructions _/entity22_ are constrained and modified by a number of _entity23_ phonetic rules _/entity23_ such as _entity24_ vowel harmony _/entity24_ .	NONE entity12 entity14
In this paper , we present an _entity1_ unlexicalized parser _/entity1_ for _entity2_ German _/entity2_ which employs _entity3_ smoothing _/entity3_ and _entity4_ suffix analysis _/entity4_ to achieve a _entity5_ labelled bracket F-score _/entity5_ of 76.2 , higher than previously reported results on the _entity6_ _C_ NEGRA corpus _/entity6_ . In addition to the high _entity7_ _P_ accuracy _/entity7_ of the model , the use of _entity8_ smoothing _/entity8_ in an _entity9_ unlexicalized parser _/entity9_ allows us to better examine the interplay between _entity10_ smoothing _/entity10_ and _entity11_ parsing _/entity11_ results .	NONE entity7 entity6
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ _P_ paraphrasing method _/entity19_ effectively extracts _entity20_ _C_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity19 entity20
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ _C_ source of data _/entity13_ for _entity14_ _P_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity14 entity13
In this paper we present a novel , customizable : _entity1_ IE paradigm _/entity1_ that takes advantage of _entity2_ predicate-argument structures _/entity2_ . We also introduce a new way of automatically identifying _entity3_ predicate argument structures _/entity3_ , which is central to our _entity4_ IE paradigm _/entity4_ . It is based on : ( 1 ) an extended set of _entity5_ features _/entity5_ ; and ( 2 ) _entity6_ inductive decision tree learning _/entity6_ . The experimental results prove our claim that accurate _entity7_ _C_ predicate-argument structures _/entity7_ enable high quality _entity8_ _P_ IE _/entity8_ results .	NONE entity8 entity7
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ _C_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ _P_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ real texts _/entity15_ , provided that complementary _entity16_ semantic information _/entity16_ are retrieved .	NONE entity9 entity6
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ _C_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ _P_ unknown words _/entity15_ .	NONE entity15 entity12
We provide a unified account of _entity1_ sentence-level and text-level anaphora _/entity1_ within the framework of a _entity2_ dependency-based grammar model _/entity2_ . Criteria for _entity3_ anaphora resolution _/entity3_ within _entity4_ sentence boundaries _/entity4_ rephrase major concepts from _entity5_ GB 's binding theory _/entity5_ , while those for _entity6_ _C_ text-level anaphora _/entity6_ incorporate an adapted version of a _entity7_ _P_ Grosz-Sidner-style focus model _/entity7_ .	MODEL-FEATURE entity7 entity6
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ _C_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ _P_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity8 entity5
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ _P_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ _C_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity16 entity19
This paper proposes an approach to _entity1_ full parsing _/entity1_ suitable for _entity2_ Information Extraction _/entity2_ from _entity3_ texts _/entity3_ . Sequences of cascades of _entity4_ rules _/entity4_ deterministically analyze the _entity5_ text _/entity5_ , building _entity6_ unambiguous structures _/entity6_ . Initially basic _entity7_ chunks _/entity7_ are analyzed ; then _entity8_ argumental relations _/entity8_ are recognized ; finally _entity9_ modifier attachment _/entity9_ is performed and the _entity10_ global parse tree _/entity10_ is built . The approach was proven to work for three _entity11_ _P_ languages _/entity11_ and different _entity12_ domains _/entity12_ . It was implemented in the _entity13_ _C_ IE module _/entity13_ of _entity14_ FACILE , a EU project for multilingual text classification and IE _/entity14_ .	NONE entity11 entity13
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ _P_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ _C_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity16 entity18
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ model parameters _/entity3_ than similar _entity4_ _C_ phrase-based models _/entity4_ . The _entity5_ _P_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity5 entity4
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ _C_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ _P_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity11 entity9
We describe the ongoing construction of a large , _entity1_ semantically annotated corpus _/entity1_ resource as reliable basis for the large-scale _entity2_ acquisition of word-semantic information _/entity2_ , e.g . the construction of _entity3_ domain-independent lexica _/entity3_ . The backbone of the _entity4_ annotation _/entity4_ are _entity5_ semantic roles _/entity5_ in the _entity6_ frame semantics paradigm _/entity6_ . We report experiences and evaluate the _entity7_ _C_ annotated data _/entity7_ from the first project stage . On this basis , we discuss the problems of _entity8_ vagueness _/entity8_ and _entity9_ ambiguity _/entity9_ in _entity10_ _P_ semantic annotation _/entity10_ .	NONE entity10 entity7
This paper describes the framework of a _entity1_ Korean phonological knowledge base system _/entity1_ using the _entity2_ unification-based grammar formalism _/entity2_ : _entity3_ Korean Phonology Structure Grammar ( KPSG ) _/entity3_ . The approach of _entity4_ KPSG _/entity4_ provides an explicit development model for constructing a computational _entity5_ _P_ phonological system _/entity5_ : _entity6_ speech recognition _/entity6_ and _entity7_ _C_ synthesis system _/entity7_ . We show that the proposed approach is more describable than other approaches such as those employing a traditional _entity8_ generative phonological approach _/entity8_ .	NONE entity5 entity7
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ _C_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ _P_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity17 entity14
We propose a method of organizing reading materials for _entity1_ vocabulary learning _/entity1_ . It enables us to select a concise set of reading _entity2_ texts _/entity2_ ( from a _entity3_ _C_ target corpus _/entity3_ ) that contains all the _entity4_ _P_ target vocabulary _/entity4_ to be learned . We used a specialized _entity5_ vocabulary _/entity5_ for an English certification test as the _entity6_ target vocabulary _/entity6_ and used _entity7_ English Wikipedia _/entity7_ , a free-content encyclopedia , as the _entity8_ target corpus _/entity8_ . The organized reading materials would enable learners not only to study the _entity9_ target vocabulary _/entity9_ efficiently but also to gain a variety of knowledge through reading . The reading materials are available on our web site .	NONE entity4 entity3
By generalizing the notion of _entity1_ location of a constituent _/entity1_ to allow _entity2_ discontinuous locations _/entity2_ , one can describe the _entity3_ discontinuous constituents _/entity3_ of _entity4_ non-configurational languages _/entity4_ . These _entity5_ _C_ discontinuous constituents _/entity5_ can be described by a variant of _entity6_ definite clause grammars _/entity6_ , and these _entity7_ _P_ grammars _/entity7_ can be used in conjunction with a _entity8_ proof procedure _/entity8_ to create a _entity9_ parser for non-configurational languages _/entity9_ .	NONE entity7 entity5
This paper proposes _entity1_ document oriented preference sets ( DoPS ) _/entity1_ for the disambiguation of the _entity2_ _P_ dependency structure _/entity2_ of _entity3_ sentences _/entity3_ . The _entity4_ _C_ DoPS system _/entity4_ extracts preference knowledge from a _entity5_ target document _/entity5_ or other _entity6_ documents _/entity6_ automatically . _entity7_ Sentence ambiguities _/entity7_ can be resolved by using domain targeted preference knowledge without using complicated large _entity8_ knowledgebases _/entity8_ . _entity9_ Implementation _/entity9_ and _entity10_ empirical results _/entity10_ are described for the the analysis of _entity11_ dependency structures _/entity11_ of _entity12_ Japanese patent claim sentences _/entity12_ .	NONE entity2 entity4
In this paper , we will describe a _entity1_ search tool _/entity1_ for a huge set of _entity2_ ngrams _/entity2_ . The tool supports _entity3_ _P_ queries _/entity3_ with an arbitrary number of _entity4_ wildcards _/entity4_ . It takes a fraction of a second for a search , and can provide the _entity5_ fillers _/entity5_ of the _entity6_ _C_ wildcards _/entity6_ . The system runs on a single Linux PC with reasonable size _entity7_ memory _/entity7_ ( less than 4GB ) and _entity8_ disk space _/entity8_ ( less than 400GB ) . This system can be a very useful tool for _entity9_ linguistic knowledge discovery _/entity9_ and other _entity10_ NLP tasks _/entity10_ .	NONE entity3 entity6
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ _C_ sense-tagged training data _/entity4_ from _entity5_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ nouns _/entity6_ in the _entity7_ _P_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ WSD programs _/entity14_ .	NONE entity7 entity4
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ _C_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ _P_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity25 entity22
The _entity1_ _P_ LOGON MT demonstrator _/entity1_ assembles independently valuable _entity2_ general-purpose NLP components _/entity2_ into a _entity3_ _C_ machine translation pipeline _/entity3_ that capitalizes on _entity4_ output quality _/entity4_ . The demonstrator embodies an interesting combination of _entity5_ hand-built , symbolic resources _/entity5_ and _entity6_ stochastic processes _/entity6_ .	NONE entity1 entity3
_entity1_ _P_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ _C_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity1 entity2
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ broadcast news speech _/entity9_ ) on both _entity10_ _C_ human transcriptions _/entity10_ and _entity11_ _P_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity11 entity10
We present a new _entity1_ part-of-speech tagger _/entity1_ that demonstrates the following ideas : ( i ) explicit use of both preceding and following _entity2_ tag contexts _/entity2_ via a _entity3_ dependency network representation _/entity3_ , ( ii ) broad use of _entity4_ lexical features _/entity4_ , including _entity5_ jointly conditioning on multiple consecutive words _/entity5_ , ( iii ) effective use of _entity6_ priors _/entity6_ in _entity7_ conditional loglinear models _/entity7_ , and ( iv ) fine-grained modeling of _entity8_ unknown word features _/entity8_ . Using these ideas together , the resulting _entity9_ _C_ tagger _/entity9_ gives a 97.24 % _entity10_ _P_ accuracy _/entity10_ on the _entity11_ Penn Treebank WSJ _/entity11_ , an _entity12_ error reduction _/entity12_ of 4.4 % on the best previous single automatically learned _entity13_ tagging _/entity13_ result .	NONE entity10 entity9
This paper considers the problem of automatic assessment of _entity1_ _C_ local coherence _/entity1_ . We present a novel _entity2_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ _P_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity4 entity1
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ _P_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ _C_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ strength of potential antecedence _/entity10_ of each element in the _entity11_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity3 entity5
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ _P_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ _C_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity23 entity26
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ _P_ SLS _/entity9_ accords with a _entity10_ _C_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ CAS _/entity19_ are particular to individual _entity20_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity9 entity10
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ _P_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ _C_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ complexity _/entity15_ .	NONE entity3 entity6
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ word-aligned corpora _/entity5_ is proposed . A _entity6_ statistical translation model _/entity6_ is also presented that deals such _entity7_ _P_ phrases _/entity7_ , as well as a _entity8_ training method _/entity8_ based on the maximization of _entity9_ translation accuracy _/entity9_ , as measured with the _entity10_ _C_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity7 entity10
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ _P_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ _C_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity6 entity7
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ _P_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ _C_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity10 entity12
Computer programs so far have not fared well in _entity1_ _C_ modeling language acquisition _/entity1_ . For one thing , _entity2_ _P_ learning methodology _/entity2_ applicable in _entity3_ general domains _/entity3_ does not readily lend itself in the _entity4_ linguistic domain _/entity4_ . For another , _entity5_ linguistic representation _/entity5_ used by _entity6_ language processing systems _/entity6_ is not geared to _entity7_ learning _/entity7_ . We introduced a new _entity8_ linguistic representation _/entity8_ , the _entity9_ Dynamic Hierarchical Phrasal Lexicon ( DHPL ) _/entity9_ [ Zernik88 ] , to facilitate _entity10_ language acquisition _/entity10_ . From this , a _entity11_ language learning model _/entity11_ was implemented in the program _entity12_ RINA _/entity12_ , which enhances its own _entity13_ lexical hierarchy _/entity13_ by processing examples in context . We identified two tasks : First , how _entity14_ linguistic concepts _/entity14_ are acquired from _entity15_ training examples _/entity15_ and organized in a _entity16_ hierarchy _/entity16_ ; this task was discussed in previous papers [ Zernik87 ] . Second , we show in this paper how a _entity17_ lexical hierarchy _/entity17_ is used in predicting new _entity18_ linguistic concepts _/entity18_ . Thus , a _entity19_ program _/entity19_ does not stall even in the presence of a _entity20_ lexical unknown _/entity20_ , and a _entity21_ hypothesis _/entity21_ can be produced for covering that _entity22_ lexical gap _/entity22_ .	NONE entity2 entity1
Interpreting _entity1_ metaphors _/entity1_ is an integral and inescapable process in _entity2_ human understanding of natural language _/entity2_ . This paper discusses a _entity3_ method of analyzing metaphors _/entity3_ based on the existence of a small number of _entity4_ generalized metaphor mappings _/entity4_ . Each _entity5_ generalized metaphor _/entity5_ contains a _entity6_ recognition network _/entity6_ , a _entity7_ basic mapping _/entity7_ , additional _entity8_ transfer mappings _/entity8_ , and an _entity9_ _P_ implicit intention component _/entity9_ . It is argued that the method reduces _entity10_ metaphor interpretation _/entity10_ from a _entity11_ reconstruction _/entity11_ to a _entity12_ _C_ recognition task _/entity12_ . Implications towards automating certain aspects of _entity13_ language learning _/entity13_ are also discussed .	NONE entity9 entity12
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ _C_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ _P_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity11 entity8
This paper describes a method for _entity1_ utterance classification _/entity1_ that does not require _entity2_ manual transcription _/entity2_ of _entity3_ training data _/entity3_ . The method combines _entity4_ domain independent acoustic models _/entity4_ with off-the-shelf _entity5_ classifiers _/entity5_ to give _entity6_ utterance classification performance _/entity6_ that is surprisingly close to what can be achieved using conventional _entity7_ word-trigram recognition _/entity7_ requiring _entity8_ _C_ manual transcription _/entity8_ . In our method , _entity9_ unsupervised training _/entity9_ is first used to train a _entity10_ _P_ phone n-gram model _/entity10_ for a particular _entity11_ domain _/entity11_ ; the _entity12_ output _/entity12_ of _entity13_ recognition _/entity13_ with this _entity14_ model _/entity14_ is then passed to a _entity15_ phone-string classifier _/entity15_ . The _entity16_ classification accuracy _/entity16_ of the method is evaluated on three different _entity17_ spoken language system domains _/entity17_ .	NONE entity10 entity8
We present a _entity1_ statistical model _/entity1_ of _entity2_ Japanese unknown words _/entity2_ consisting of a set of _entity3_ length and spelling models _/entity3_ classified by the _entity4_ character types _/entity4_ that constitute a _entity5_ word _/entity5_ . The point is quite simple : different _entity6_ character sets _/entity6_ should be treated differently and the changes between _entity7_ character types _/entity7_ are very important because _entity8_ Japanese script _/entity8_ has both _entity9_ ideograms _/entity9_ like _entity10_ _C_ Chinese _/entity10_ ( _entity11_ _P_ kanji _/entity11_ ) and _entity12_ phonograms _/entity12_ like _entity13_ English _/entity13_ ( _entity14_ katakana _/entity14_ ) . Both _entity15_ word segmentation accuracy _/entity15_ and _entity16_ part of speech tagging accuracy _/entity16_ are improved by the proposed model . The model can achieve 96.6 % _entity17_ tagging accuracy _/entity17_ if _entity18_ unknown words _/entity18_ are correctly segmented .	NONE entity11 entity10
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ _C_ statistical techniques _/entity10_ . We present our _entity11_ _P_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity11 entity10
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ _C_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ _P_ classification-based representation techniques _/entity7_ can be applied to _entity8_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ semantic interpretation _/entity13_ was first explored in the _entity14_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	NONE entity7 entity5
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ _C_ summarization _/entity12_ quality of _entity13_ sentence condensation systems _/entity13_ . An _entity14_ _P_ experimental evaluation _/entity14_ of _entity15_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity14 entity12
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ _P_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ _C_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ frequency of occurrences _/entity23_ .	NONE entity3 entity4
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ _C_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ _P_ song lyrics _/entity5_ actually contribute little to _entity6_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity5 entity2
In this paper we sketch an approach for _entity1_ Natural Language parsing _/entity1_ . Our approach is an _entity2_ example-based approach _/entity2_ , which relies mainly on examples that already parsed to their _entity3_ representation structure _/entity3_ , and on the knowledge that we can get from these examples the required information to parse a new _entity4_ input sentence _/entity4_ . In our approach , examples are annotated with the _entity5_ Structured String Tree Correspondence ( SSTC ) annotation schema _/entity5_ where each _entity6_ SSTC _/entity6_ describes a _entity7_ sentence _/entity7_ , a _entity8_ _P_ representation tree _/entity8_ as well as the correspondence between _entity9_ substrings _/entity9_ in the _entity10_ _C_ sentence _/entity10_ and _entity11_ subtrees _/entity11_ in the _entity12_ representation tree _/entity12_ . In the process of _entity13_ parsing _/entity13_ , we first try to build _entity14_ subtrees _/entity14_ for _entity15_ phrases _/entity15_ in the _entity16_ input sentence _/entity16_ which have been successfully found in the _entity17_ example-base _/entity17_ - a bottom up approach . These _entity18_ subtrees _/entity18_ will then be combined together to form a _entity19_ single rooted representation tree _/entity19_ based on an example with similar _entity20_ representation structure _/entity20_ - a top down approach.Keywords :	NONE entity8 entity10
_entity1_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ language-independent system _/entity2_ for _entity3_ automatic discovery of text _/entity3_ in _entity4_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ _P_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ _C_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ words _/entity9_ per _entity10_ language _/entity10_ .	NONE entity6 entity7
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ Gaussian mixture _/entity10_ for _entity11_ _C_ acoustic modeling _/entity11_ and _entity12_ _P_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity12 entity11
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ _P_ word string _/entity13_ has been obtained by using a different _entity14_ _C_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity13 entity14
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ _C_ non-deterministic parsing choices _/entity14_ of the _entity15_ _P_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity15 entity14
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ _C_ ccg parsing _/entity16_ . We extend this _entity17_ _P_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity17 entity16
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ _C_ units _/entity9_ given the _entity10_ _P_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity10 entity9
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ _P_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ _C_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity2 entity4
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ _C_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ _P_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity7 entity5
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ _C_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ _P_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity5 entity3
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ _C_ language processing pipeline _/entity22_ results in more accurate _entity23_ _P_ ccg supertagging _/entity23_ .	NONE entity23 entity22
The _entity1_ psycholinguistic literature _/entity1_ provides evidence for _entity2_ syntactic priming _/entity2_ , i.e. , the tendency to repeat structures . This paper describes a method for incorporating _entity3_ priming _/entity3_ into an _entity4_ _P_ incremental probabilistic parser _/entity4_ . Three models are compared , which involve _entity5_ priming _/entity5_ of _entity6_ _C_ rules _/entity6_ between _entity7_ sentences _/entity7_ , within _entity8_ sentences _/entity8_ , and within _entity9_ coordinate structures _/entity9_ . These models simulate the reading time advantage for _entity10_ parallel structures _/entity10_ found in _entity11_ human data _/entity11_ , and also yield a small increase in overall _entity12_ parsing accuracy _/entity12_ .	NONE entity4 entity6
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ _C_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ _P_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity16 entity13
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ topics _/entity12_ of the _entity13_ _C_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ _P_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity16 entity13
_entity1_ Sentence boundary detection _/entity1_ in _entity2_ speech _/entity2_ is important for enriching _entity3_ speech recognition _/entity3_ output , making it easier for humans to read and downstream modules to process . In previous work , we have developed _entity4_ hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers _/entity4_ that integrate textual and prosodic _entity5_ knowledge sources _/entity5_ for detecting _entity6_ sentence boundaries _/entity6_ . In this paper , we evaluate the use of a _entity7_ conditional random field ( CRF ) _/entity7_ for this task and relate results with this model to our prior work . We evaluate across two corpora ( conversational _entity8_ telephone speech _/entity8_ and _entity9_ _C_ broadcast news speech _/entity9_ ) on both _entity10_ _P_ human transcriptions _/entity10_ and _entity11_ speech recognition _/entity11_ output . In general , our _entity12_ CRF _/entity12_ model yields a lower error rate than the _entity13_ HMM and Max-ent models _/entity13_ on the _entity14_ NIST sentence boundary detection task _/entity14_ in _entity15_ speech _/entity15_ , although it is interesting to note that the best results are achieved by _entity16_ three-way voting _/entity16_ among the _entity17_ classifiers _/entity17_ . This probably occurs because each _entity18_ model _/entity18_ has different strengths and weaknesses for modeling the _entity19_ knowledge sources _/entity19_ .	NONE entity10 entity9
In this paper we present a _entity1_ statistical profile _/entity1_ of the _entity2_ Named Entity task _/entity2_ , a specific _entity3_ information extraction task _/entity3_ for which _entity4_ corpora _/entity4_ in several _entity5_ languages _/entity5_ are available . Using the _entity6_ results _/entity6_ of the _entity7_ statistical analysis _/entity7_ , we propose an _entity8_ algorithm _/entity8_ for _entity9_ _P_ lower bound estimation _/entity9_ for _entity10_ Named Entity corpora _/entity10_ and discuss the significance of the _entity11_ cross-lingual comparisons _/entity11_ provided by the _entity12_ _C_ analysis _/entity12_ .	NONE entity9 entity12
_entity1_ Oral communication _/entity1_ is ubiquitous and carries important information yet it is also time consuming to document . Given the development of _entity2_ storage media and networks _/entity2_ one could just record and store a _entity3_ _P_ conversation _/entity3_ for documentation . The question is , however , how an interesting information piece would be found in a _entity4_ large database _/entity4_ . Traditional _entity5_ _C_ information retrieval techniques _/entity5_ use a _entity6_ histogram _/entity6_ of _entity7_ keywords _/entity7_ as the _entity8_ document representation _/entity8_ but _entity9_ oral communication _/entity9_ may offer additional _entity10_ indices _/entity10_ such as the time and place of the rejoinder and the attendance . An alternative _entity11_ index _/entity11_ could be the activity such as discussing , planning , informing , story-telling , etc . This paper addresses the problem of the _entity12_ automatic detection _/entity12_ of those activities in meeting situation and everyday rejoinders . Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger _entity13_ database _/entity13_ and detect those automatically which is shown on a large _entity14_ database _/entity14_ of _entity15_ TV shows _/entity15_ . _entity16_ Emotions _/entity16_ and other _entity17_ indices _/entity17_ such as the _entity18_ dominance distribution of speakers _/entity18_ might be available on the _entity19_ surface _/entity19_ and could be used directly . Despite the small size of the _entity20_ databases _/entity20_ used some results about the effectiveness of these _entity21_ indices _/entity21_ can be obtained .	NONE entity3 entity5
The paper proposes and empirically motivates an integration of _entity1_ supervised learning _/entity1_ with _entity2_ unsupervised learning _/entity2_ to deal with human biases in _entity3_ _P_ summarization _/entity3_ . In particular , we explore the use of _entity4_ probabilistic decision tree _/entity4_ within the clustering framework to account for the variation as well as regularity in _entity5_ _C_ human created summaries _/entity5_ . The _entity6_ corpus _/entity6_ of human created extracts is created from a _entity7_ newspaper corpus _/entity7_ and used as a test set . We build _entity8_ probabilistic decision trees _/entity8_ of different flavors and integrate each of them with the clustering framework . Experiments with the _entity9_ corpus _/entity9_ demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone .	NONE entity3 entity5
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ _P_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ _C_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity17 entity19
_entity1_ Lyric-based song sentiment classification _/entity1_ seeks to assign songs appropriate _entity2_ sentiment labels _/entity2_ such as light-hearted heavy-hearted . Four problems render _entity3_ _C_ vector space model ( VSM ) -based text classification approach _/entity3_ ineffective : 1 ) Many _entity4_ words _/entity4_ within _entity5_ song lyrics _/entity5_ actually contribute little to _entity6_ _P_ sentiment _/entity6_ ; 2 ) _entity7_ Nouns _/entity7_ and _entity8_ verbs _/entity8_ used to express _entity9_ sentiment _/entity9_ are ambiguous ; 3 ) _entity10_ Negations _/entity10_ and _entity11_ modifiers _/entity11_ around the _entity12_ sentiment keywords _/entity12_ make particular contributions to _entity13_ sentiment _/entity13_ ; 4 ) _entity14_ Song lyric _/entity14_ is usually very short . To address these problems , the _entity15_ sentiment vector space model ( s-VSM ) _/entity15_ is proposed to represent _entity16_ song lyric document _/entity16_ . The preliminary experiments prove that the _entity17_ s-VSM model _/entity17_ outperforms the _entity18_ VSM model _/entity18_ in the _entity19_ lyric-based song sentiment classification task _/entity19_ .	NONE entity6 entity3
To verify _entity1_ hardware designs _/entity1_ by _entity2_ model checking _/entity2_ , _entity3_ circuit specifications _/entity3_ are commonly expressed in the _entity4_ temporal logic CTL _/entity4_ . _entity5_ Automatic conversion of English to CTL _/entity5_ requires the definition of an appropriately _entity6_ _P_ restricted subset _/entity6_ of _entity7_ _C_ English _/entity7_ . We show how the limited _entity8_ semantic expressibility _/entity8_ of _entity9_ CTL _/entity9_ can be exploited to derive a hierarchy of _entity10_ subsets _/entity10_ . Our strategy avoids potential difficulties with approaches that take existing _entity11_ computational semantic analyses _/entity11_ of _entity12_ English _/entity12_ as their starting point -- such as the need to ensure that all _entity13_ sentences _/entity13_ in the _entity14_ subset _/entity14_ possess a _entity15_ CTL translation _/entity15_ .	PART_WHOLE entity6 entity7
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ _P_ segment order-sensitive methods _/entity17_ in terms of _entity18_ _C_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity17 entity18
One of the claimed benefits of _entity1_ _P_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ feature structure unification _/entity3_ during _entity4_ _C_ parsing _/entity4_ . We compare two wide-coverage _entity5_ lexicalized grammars of English _/entity5_ , _entity6_ LEXSYS _/entity6_ and _entity7_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity1 entity4
In this paper , we present a _entity1_ fully automated extraction system _/entity1_ , named _entity2_ IntEx _/entity2_ , to identify _entity3_ gene and protein interactions _/entity3_ in _entity4_ biomedical text _/entity4_ . Our approach is based on first splitting _entity5_ complex sentences _/entity5_ into _entity6_ simple clausal structures _/entity6_ made up of _entity7_ syntactic roles _/entity7_ . Then , tagging _entity8_ biological entities _/entity8_ with the help of _entity9_ biomedical and linguistic ontologies _/entity9_ . Finally , extracting _entity10_ complete interactions _/entity10_ by analyzing the matching contents of _entity11_ syntactic roles _/entity11_ and their linguistically significant combinations . Our _entity12_ extraction system _/entity12_ handles _entity13_ complex sentences _/entity13_ and extracts _entity14_ _P_ multiple and nested interactions _/entity14_ specified in a _entity15_ _C_ sentence _/entity15_ . Experimental evaluations with two other state of the art _entity16_ extraction systems _/entity16_ indicate that the _entity17_ IntEx system _/entity17_ achieves better _entity18_ performance _/entity18_ without the labor intensive _entity19_ pattern engineering requirement _/entity19_ .	PART_WHOLE entity14 entity15
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ _C_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ _P_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity7 entity4
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ _C_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ _P_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity20 entity18
In this paper we present a _entity1_ formalization _/entity1_ of the _entity2_ centering approach _/entity2_ to modeling _entity3_ attentional structure in discourse _/entity3_ and use it as the basis for an _entity4_ algorithm _/entity4_ to track _entity5_ discourse context _/entity5_ and bind _entity6_ _P_ pronouns _/entity6_ . As described in [ GJW86 ] , the process of _entity7_ _C_ centering attention on entities in the discourse _/entity7_ gives rise to the _entity8_ intersentential transitional states of continuing , retaining and shifting _/entity8_ . We propose an extension to these _entity9_ states _/entity9_ which handles some additional cases of multiple _entity10_ ambiguous pronouns _/entity10_ . The _entity11_ algorithm _/entity11_ has been implemented in an _entity12_ HPSG natural language system _/entity12_ which serves as the interface to a _entity13_ database query application _/entity13_ .	NONE entity6 entity7
This paper explores the issue of using different _entity1_ co-occurrence similarities _/entity1_ between _entity2_ terms _/entity2_ for separating _entity3_ query terms _/entity3_ that are useful for _entity4_ retrieval _/entity4_ from those that are harmful . The hypothesis under examination is that _entity5_ _P_ useful terms _/entity5_ tend to be more similar to each other than to other _entity6_ query terms _/entity6_ . Preliminary experiments with similarities computed using _entity7_ _C_ first-order and second-order co-occurrence _/entity7_ seem to confirm the hypothesis . _entity8_ Term similarities _/entity8_ could then be used for determining which _entity9_ query terms _/entity9_ are useful and best reflect the user 's information need . A possible application would be to use this source of evidence for tuning the _entity10_ weights _/entity10_ of the _entity11_ query terms _/entity11_ .	NONE entity5 entity7
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ _P_ machine-readable resources _/entity2_ for the construction of _entity3_ _C_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity2 entity3
This paper describes a method of _entity1_ _P_ interactively visualizing and directing the process _/entity1_ of _entity2_ translating a sentence _/entity2_ . The method allows a _entity3_ _C_ user _/entity3_ to explore a _entity4_ model _/entity4_ of _entity5_ syntax-based statistical machine translation ( MT ) _/entity5_ , to understand the _entity6_ model _/entity6_ 's strengths and weaknesses , and to compare it to other _entity7_ MT systems _/entity7_ . Using this _entity8_ visualization method _/entity8_ , we can find and address conceptual and practical problems in an _entity9_ MT system _/entity9_ . In our demonstration at _entity10_ ACL _/entity10_ , new _entity11_ users _/entity11_ of our tool will drive a _entity12_ syntax-based decoder _/entity12_ for themselves .	NONE entity1 entity3
An attempt has been made to use an _entity1_ Augmented Transition Network _/entity1_ as a procedural _entity2_ dialog model _/entity2_ . The development of such a _entity3_ model _/entity3_ appears to be important in several respects : as a device to represent and to use different _entity4_ dialog schemata _/entity4_ proposed in empirical _entity5_ conversation analysis _/entity5_ ; as a device to represent and to use _entity6_ models of verbal interaction _/entity6_ ; as a device combining knowledge about _entity7_ dialog schemata _/entity7_ and about _entity8_ verbal interaction _/entity8_ with knowledge about _entity9_ _P_ task-oriented and goal-directed dialogs _/entity9_ . A standard _entity10_ ATN _/entity10_ should be further developed in order to account for the _entity11_ verbal interactions _/entity11_ of _entity12_ _C_ task-oriented dialogs _/entity12_ .	NONE entity9 entity12
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ morphological and syntactic information _/entity3_ . While the _entity4_ model _/entity4_ is more complex than those which have been employed for _entity5_ unsupervised learning _/entity5_ of _entity6_ POS tags in English _/entity6_ , which use only _entity7_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ morphology _/entity9_ as well . In many _entity10_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ _C_ Bulgarian _/entity15_ , a _entity16_ Slavic language _/entity16_ with relatively _entity17_ _P_ free word order _/entity17_ and _entity18_ rich morphology _/entity18_ .	NONE entity17 entity15
This paper proposes a novel method of building _entity1_ polarity-tagged corpus _/entity1_ from _entity2_ HTML documents _/entity2_ . The characteristics of this method is that it is fully automatic and can be applied to arbitrary _entity3_ HTML documents _/entity3_ . The idea behind our method is to utilize certain _entity4_ layout structures _/entity4_ and _entity5_ linguistic pattern _/entity5_ . By using them , we can automatically extract such _entity6_ _P_ sentences _/entity6_ that express opinion . In our experiment , the method could construct a _entity7_ corpus _/entity7_ consisting of 126,610 _entity8_ _C_ sentences _/entity8_ .	NONE entity6 entity8
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ _P_ word sense _/entity3_ , a _entity4_ topic signature _/entity4_ is a set of _entity5_ _C_ words _/entity5_ that tend to co-occur with it . _entity6_ Topic signatures _/entity6_ can be useful in a number of _entity7_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ topic signatures _/entity15_ on a _entity16_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ standard WSD datasets _/entity18_ , with promising results .	NONE entity3 entity5
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ _P_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ _C_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity28 entity30
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ _P_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ _C_ word-aligned corpora _/entity5_ is proposed . A _entity6_ statistical translation model _/entity6_ is also presented that deals such _entity7_ phrases _/entity7_ , as well as a _entity8_ training method _/entity8_ based on the maximization of _entity9_ translation accuracy _/entity9_ , as measured with the _entity10_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity3 entity5
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ _P_ latent variable _/entity7_ of the _entity8_ _C_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ models _/entity16_ . The results also revealed an upper bound of _entity17_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	PART_WHOLE entity7 entity8
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ _P_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ _C_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity13 entity15
This paper proposes _entity1_ document oriented preference sets ( DoPS ) _/entity1_ for the disambiguation of the _entity2_ dependency structure _/entity2_ of _entity3_ sentences _/entity3_ . The _entity4_ DoPS system _/entity4_ extracts preference knowledge from a _entity5_ target document _/entity5_ or other _entity6_ documents _/entity6_ automatically . _entity7_ Sentence ambiguities _/entity7_ can be resolved by using domain targeted preference knowledge without using complicated large _entity8_ knowledgebases _/entity8_ . _entity9_ _P_ Implementation _/entity9_ and _entity10_ empirical results _/entity10_ are described for the the analysis of _entity11_ dependency structures _/entity11_ of _entity12_ _C_ Japanese patent claim sentences _/entity12_ .	NONE entity9 entity12
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ CFG parsers _/entity3_ running in _entity4_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ grammar G _/entity5_ and a _entity6_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ _C_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ CFG parsing _/entity11_ : a fast , practical _entity12_ _P_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity12 entity9
We present a _entity1_ _C_ syntax-based constraint _/entity1_ for _entity2_ word alignment _/entity2_ , known as the _entity3_ _P_ cohesion constraint _/entity3_ . It requires disjoint _entity4_ English phrases _/entity4_ to be mapped to non-overlapping intervals in the _entity5_ French sentence _/entity5_ . We evaluate the utility of this _entity6_ constraint _/entity6_ in two different algorithms . The results show that it can provide a significant improvement in _entity7_ alignment quality _/entity7_ .	NONE entity3 entity1
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ _C_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ _P_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity12 entity9
When people use _entity1_ natural language _/entity1_ in natural settings , they often use it ungrammatically , missing out or repeating words , breaking-off and restarting , speaking in fragments , etc.. Their _entity2_ human listeners _/entity2_ are usually able to cope with these deviations with little difficulty . If a _entity3_ _C_ computer system _/entity3_ wishes to accept _entity4_ _P_ natural language input _/entity4_ from its _entity5_ users _/entity5_ on a routine basis , it must display a similar indifference . In this paper , we outline a set of _entity6_ parsing flexibilities _/entity6_ that such a system should provide . We go , on to describe _entity7_ FlexP _/entity7_ , a _entity8_ bottom-up pattern-matching parser _/entity8_ that we have designed and implemented to provide these flexibilities for _entity9_ restricted natural language _/entity9_ input to a limited-domain computer system .	NONE entity4 entity3
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ _P_ Chine language _/entity19_ , showing improvement of _entity20_ _C_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity19 entity20
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ _P_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ _C_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ approach _/entity21_ with the combination of _entity22_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity11 entity13
We describe a method for interpreting _entity1_ abstract flat syntactic representations , LFG f-structures _/entity1_ , as _entity2_ underspecified semantic representations , here Underspecified Discourse Representation Structures ( UDRSs ) _/entity2_ . The method establishes a _entity3_ one-to-one correspondence _/entity3_ between subsets of the _entity4_ _P_ LFG _/entity4_ and _entity5_ _C_ UDRS _/entity5_ formalisms . It provides a _entity6_ model theoretic interpretation _/entity6_ and an _entity7_ inferential component _/entity7_ which operates directly on _entity8_ underspecified representations _/entity8_ for _entity9_ f-structures _/entity9_ through the _entity10_ translation images _/entity10_ of _entity11_ f-structures _/entity11_ as _entity12_ UDRSs _/entity12_ .	NONE entity4 entity5
We present a framework for the fast _entity1_ _C_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ _P_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity3 entity1
We provide a _entity1_ logical definition _/entity1_ of _entity2_ Minimalist grammars _/entity2_ , that are _entity3_ Stabler 's formalization _/entity3_ of _entity4_ Chomsky 's minimalist program _/entity4_ . Our _entity5_ logical definition _/entity5_ leads to a neat relation to _entity6_ categorial grammar _/entity6_ , ( yielding a treatment of _entity7_ Montague semantics _/entity7_ ) , a _entity8_ parsing-as-deduction _/entity8_ in a _entity9_ resource sensitive logic _/entity9_ , and a _entity10_ learning algorithm _/entity10_ from _entity11_ _C_ structured data _/entity11_ ( based on a _entity12_ _P_ typing-algorithm _/entity12_ and _entity13_ type-unification _/entity13_ ) . Here we emphasize the connection to _entity14_ Montague semantics _/entity14_ which can be viewed as a _entity15_ formal computation _/entity15_ of the _entity16_ logical form _/entity16_ .	NONE entity12 entity11
_entity1_ Statistical machine translation ( SMT ) _/entity1_ is currently one of the hot spots in _entity2_ natural language processing _/entity2_ . Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that _entity3_ SMT _/entity3_ gives competitive results to _entity4_ rule-based translation systems _/entity4_ , requiring significantly less development time . This is particularly important when building _entity5_ translation systems _/entity5_ for new _entity6_ language pairs _/entity6_ or new _entity7_ domains _/entity7_ . This workshop is intended to give an introduction to _entity8_ statistical machine translation _/entity8_ with a focus on practical considerations . Participants should be able , after attending this workshop , to set out building an _entity9_ SMT system _/entity9_ themselves and achieving good _entity10_ baseline results _/entity10_ in a short time . The tutorial will cover the basics of _entity11_ SMT _/entity11_ : Theory will be put into practice . _entity12_ _C_ STTK _/entity12_ , a _entity13_ statistical machine translation tool kit _/entity13_ , will be introduced and used to build a working _entity14_ translation system _/entity14_ . _entity15_ _P_ STTK _/entity15_ has been developed by the presenter and co-workers over a number of years and is currently used as the basis of _entity16_ CMU 's SMT system _/entity16_ . It has also successfully been coupled with _entity17_ rule-based and example based machine translation modules _/entity17_ to build a _entity18_ multi engine machine translation system _/entity18_ . The _entity19_ source code _/entity19_ of the _entity20_ tool kit _/entity20_ will be made available .	NONE entity15 entity12
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ _C_ grammar coverage problems _/entity3_ we use a _entity4_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ _P_ speech-search algorithm _/entity5_ is implemented on a _entity6_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity5 entity3
Previous work has used _entity1_ monolingual parallel corpora _/entity1_ to extract and generate _entity2_ paraphrases _/entity2_ . We show that this task can be done using _entity3_ bilingual parallel corpora _/entity3_ , a much more commonly available _entity4_ resource _/entity4_ . Using _entity5_ alignment techniques _/entity5_ from _entity6_ phrase-based statistical machine translation _/entity6_ , we show how _entity7_ paraphrases _/entity7_ in one _entity8_ language _/entity8_ can be identified using a _entity9_ phrase _/entity9_ in another language as a pivot . We define a _entity10_ paraphrase probability _/entity10_ that allows _entity11_ _C_ paraphrases _/entity11_ extracted from a _entity12_ bilingual parallel corpus _/entity12_ to be ranked using _entity13_ translation probabilities _/entity13_ , and show how it can be refined to take _entity14_ _P_ contextual information _/entity14_ into account . We evaluate our _entity15_ paraphrase extraction and ranking methods _/entity15_ using a set of _entity16_ manual word alignments _/entity16_ , and contrast the _entity17_ quality _/entity17_ with _entity18_ paraphrases _/entity18_ extracted from _entity19_ automatic alignments _/entity19_ .	NONE entity14 entity11
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ _P_ natural language environment _/entity2_ . Because a _entity3_ _C_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity2 entity3
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ _P_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ _C_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity16 entity18
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ parsing _/entity3_ , _entity4_ _C_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ data representation _/entity6_ for _entity7_ _P_ chunking _/entity7_ by converting it to a _entity8_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity7 entity4
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ _P_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ _C_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity7 entity9
This paper describes the status of the _entity1_ MIT ATIS system _/entity1_ as of February 1992 , focusing especially on the changes made to the _entity2_ SUMMIT recognizer _/entity2_ . These include _entity3_ context-dependent phonetic modelling _/entity3_ , the use of a _entity4_ _C_ bigram language model _/entity4_ in conjunction with a _entity5_ probabilistic LR parser _/entity5_ , and refinements made to the _entity6_ lexicon _/entity6_ . Together with the use of a larger _entity7_ _P_ training set _/entity7_ , these modifications combined to reduce the _entity8_ speech recognition word and sentence error rates _/entity8_ by a factor of 2.5 and 1.6 , respectively , on the _entity9_ October '91 test set _/entity9_ . The weighted error for the entire _entity10_ spoken language system _/entity10_ on the same _entity11_ test set _/entity11_ is 49.3 % . Similar results were also obtained on the _entity12_ February '92 benchmark evaluation _/entity12_ .	NONE entity7 entity4
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ _C_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ _P_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity20 entity17
Instances of a _entity1_ word _/entity1_ drawn from different _entity2_ domains _/entity2_ may have different _entity3_ sense priors _/entity3_ ( the proportions of the different _entity4_ senses _/entity4_ of a _entity5_ word _/entity5_ ) . This in turn affects the accuracy of _entity6_ word sense disambiguation ( WSD ) systems _/entity6_ trained and applied on different _entity7_ domains _/entity7_ . This paper presents a method to estimate the _entity8_ sense priors _/entity8_ of _entity9_ words _/entity9_ drawn from a new _entity10_ domain _/entity10_ , and highlights the importance of using _entity11_ well calibrated probabilities _/entity11_ when performing these _entity12_ estimations _/entity12_ . By using _entity13_ well calibrated probabilities _/entity13_ , we are able to estimate the _entity14_ _P_ sense priors _/entity14_ effectively to achieve significant improvements in _entity15_ _C_ WSD accuracy _/entity15_ .	NONE entity14 entity15
This paper presents an _entity1_ evaluation method _/entity1_ employing a _entity2_ latent variable model _/entity2_ for _entity3_ paraphrases _/entity3_ with their _entity4_ contexts _/entity4_ . We assume that the _entity5_ context _/entity5_ of a _entity6_ sentence _/entity6_ is indicated by a _entity7_ latent variable _/entity7_ of the _entity8_ model _/entity8_ as a _entity9_ topic _/entity9_ and that the _entity10_ likelihood _/entity10_ of each _entity11_ variable _/entity11_ can be inferred . A _entity12_ paraphrase _/entity12_ is evaluated for whether its _entity13_ sentences _/entity13_ are used in the same _entity14_ context _/entity14_ . Experimental results showed that the proposed method achieves almost 60 % _entity15_ accuracy _/entity15_ and that there is not a large performance difference between the two _entity16_ _P_ models _/entity16_ . The results also revealed an upper bound of _entity17_ _C_ accuracy _/entity17_ of 77 % with the _entity18_ method _/entity18_ when using only _entity19_ topic information _/entity19_ .	NONE entity16 entity17
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ _C_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ _P_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity18 entity15
A novel _entity1_ bootstrapping approach _/entity1_ to _entity2_ Named Entity ( NE ) tagging _/entity2_ using _entity3_ concept-based seeds _/entity3_ and _entity4_ successive learners _/entity4_ is presented . This approach only requires a few _entity5_ common noun _/entity5_ or _entity6_ pronoun _/entity6_ _entity7_ seeds _/entity7_ that correspond to the _entity8_ concept _/entity8_ for the targeted _entity9_ NE _/entity9_ , e.g . he/she/man/woman for _entity10_ PERSON NE _/entity10_ . The _entity11_ bootstrapping procedure _/entity11_ is implemented as training two _entity12_ successive learners _/entity12_ . First , _entity13_ _C_ decision list _/entity13_ is used to learn the _entity14_ parsing-based NE rules _/entity14_ . Then , a _entity15_ Hidden Markov Model _/entity15_ is trained on a _entity16_ _P_ corpus _/entity16_ automatically tagged by the first _entity17_ learner _/entity17_ . The resulting _entity18_ NE system _/entity18_ approaches _entity19_ supervised NE _/entity19_ performance for some _entity20_ NE types _/entity20_ .	NONE entity16 entity13
This paper describes a _entity1_ computational model _/entity1_ of _entity2_ _P_ word segmentation _/entity2_ and presents simulation results on _entity3_ realistic acquisition _/entity3_ . In particular , we explore the capacity and limitations of _entity4_ statistical learning mechanisms _/entity4_ that have recently gained prominence in _entity5_ _C_ cognitive psychology _/entity5_ and _entity6_ linguistics _/entity6_ .	NONE entity2 entity5
We directly investigate a subject of much recent debate : do _entity1_ word sense disambigation models _/entity1_ help _entity2_ statistical machine translation _/entity2_ _entity3_ quality _/entity3_ ? We present empirical results casting doubt on this common , but unproved , assumption . Using a state-of-the-art _entity4_ Chinese word sense disambiguation model _/entity4_ to choose _entity5_ _C_ translation candidates _/entity5_ for a typical _entity6_ IBM statistical MT system _/entity6_ , we find that _entity7_ word sense disambiguation _/entity7_ does not yield significantly better _entity8_ _P_ translation quality _/entity8_ than the _entity9_ statistical machine translation system _/entity9_ alone . _entity10_ Error analysis _/entity10_ suggests several key factors behind this surprising finding , including inherent limitations of current _entity11_ statistical MT architectures _/entity11_ .	NONE entity8 entity5
Towards deep analysis of _entity1_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ class-oriented framework _/entity2_ for collecting _entity3_ paraphrase examples _/entity3_ , in which _entity4_ sentential paraphrases _/entity4_ are collected for each _entity5_ paraphrase class _/entity5_ separately by means of _entity6_ _C_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ _P_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ cost-efficiency _/entity9_ , _entity10_ exhaustiveness _/entity10_ , and _entity11_ reliability _/entity11_ .	NONE entity8 entity6
This paper describes a _entity1_ characters-based Chinese collocation system _/entity1_ and discusses the advantages of it over a traditional _entity2_ _P_ word-based system _/entity2_ . Since _entity3_ _C_ wordbreaks _/entity3_ are not conventionally marked in _entity4_ Chinese text corpora _/entity4_ , a _entity5_ character-based collocation system _/entity5_ has the dual advantages of avoiding _entity6_ pre-processing distortion _/entity6_ and directly accessing _entity7_ sub-lexical information _/entity7_ . Furthermore , _entity8_ word-based collocational properties _/entity8_ can be obtained through an auxiliary module of _entity9_ automatic segmentation _/entity9_ .	NONE entity2 entity3
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ _C_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ _P_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity12 entity10
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ _P_ decoder _/entity15_ and show that using these _entity16_ _C_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity15 entity16
In this paper _entity1_ discourse segments _/entity1_ are defined and a method for _entity2_ discourse segmentation _/entity2_ primarily based on _entity3_ abduction _/entity3_ of _entity4_ _P_ temporal relations _/entity4_ between _entity5_ segments _/entity5_ is proposed . This method is precise and _entity6_ _C_ computationally feasible _/entity6_ and is supported by previous work in the area of _entity7_ temporal anaphora resolution _/entity7_ .	NONE entity4 entity6
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ _P_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ _C_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity14 entity17
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ _P_ grammar coverage problems _/entity3_ we use a _entity4_ _C_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity3 entity4
This paper describes an _entity1_ unsupervised learning method _/entity1_ for _entity2_ associative relationships between verb phrases _/entity2_ , which is important in developing reliable _entity3_ Q & A systems _/entity3_ . Consider the situation that a user gives a _entity4_ query _/entity4_ `` How much petrol was imported to Japan from Saudi Arabia ? '' to a _entity5_ Q & A system _/entity5_ , but the _entity6_ text _/entity6_ given to the system includes only the _entity7_ description _/entity7_ `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' . We think that the _entity8_ description _/entity8_ is a good clue to find the answer for our _entity9_ query _/entity9_ , `` X tonnes '' . But there is no _entity10_ _C_ large-scale database _/entity10_ that provides the _entity11_ associative relationship _/entity11_ between `` imported '' and `` conveyed '' . Our aim is to develop an _entity12_ _P_ unsupervised learning method _/entity12_ that can obtain such an _entity13_ associative relationship _/entity13_ , which we call _entity14_ scenario consistency _/entity14_ . The method we are currently working on uses an _entity15_ expectation-maximization ( EM ) based word-clustering algorithm _/entity15_ , and we have evaluated the effectiveness of this method using _entity16_ Japanese verb phrases _/entity16_ .	NONE entity12 entity10
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ _P_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ _C_ features _/entity4_ are not enough to distinguish _entity5_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity2 entity4
In this study , we propose a _entity1_ knowledge-independent method _/entity1_ for aligning _entity2_ terms _/entity2_ and thus extracting _entity3_ translations _/entity3_ from a _entity4_ small , domain-specific corpus _/entity4_ consisting of _entity5_ parallel English and Chinese court judgments _/entity5_ from Hong Kong . With a _entity6_ sentence-aligned corpus _/entity6_ , _entity7_ _C_ translation equivalences _/entity7_ are suggested by analysing the _entity8_ frequency profiles _/entity8_ of _entity9_ parallel concordances _/entity9_ . The method overcomes the limitations of _entity10_ _P_ conventional statistical methods _/entity10_ which require _entity11_ large corpora _/entity11_ to be effective , and _entity12_ lexical approaches _/entity12_ which depend on existing _entity13_ bilingual dictionaries _/entity13_ . Pilot testing on a _entity14_ parallel corpus _/entity14_ of about 113K _entity15_ Chinese words _/entity15_ and 120K _entity16_ English words _/entity16_ gives an encouraging 85 % _entity17_ precision _/entity17_ and 45 % _entity18_ recall _/entity18_ . Future work includes fine-tuning the _entity19_ algorithm _/entity19_ upon the analysis of the errors , and acquiring a _entity20_ translation lexicon _/entity20_ for _entity21_ legal terminology _/entity21_ by filtering out _entity22_ general terms _/entity22_ .	NONE entity10 entity7
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ _P_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ _C_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity7 entity8
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ _C_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ _P_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity7 entity4
In this paper , we show how to construct a _entity1_ transfer dictionary _/entity1_ automatically . _entity2_ Dictionary construction _/entity2_ , one of the most difficult tasks in developing a _entity3_ machine translation system _/entity3_ , is expensive . To avoid this problem , we investigate how we build a _entity4_ dictionary _/entity4_ using existing _entity5_ linguistic resources _/entity5_ . Our _entity6_ algorithm _/entity6_ can be applied to any _entity7_ language pairs _/entity7_ , but for the present we focus on building a _entity8_ Korean-to-Japanese dictionary _/entity8_ using _entity9_ English _/entity9_ as a _entity10_ pivot _/entity10_ . We attempt three ways of _entity11_ automatic construction _/entity11_ to corroborate the effect of the _entity12_ directionality _/entity12_ of _entity13_ dictionaries _/entity13_ . First , we introduce _entity14_ `` one-time look up '' method _/entity14_ using a _entity15_ _P_ Korean-to-English and a Japanese-to-English dictionary _/entity15_ . Second , we show a method using _entity16_ `` overlapping constraint '' _/entity16_ with a _entity17_ Korean-to-English dictionary _/entity17_ and an _entity18_ _C_ English-to-Japanese dictionary _/entity18_ . Third , we consider another alternative method rarely used for building a _entity19_ dictionary _/entity19_ : an _entity20_ English-to-Korean dictionary _/entity20_ and _entity21_ English-to-Japanese dictionary _/entity21_ . We found that the first method is the most effective and the best result can be obtained from combining the three methods .	NONE entity15 entity18
_entity1_ Words _/entity1_ in _entity2_ Chinese text _/entity2_ are not naturally separated by _entity3_ delimiters _/entity3_ , which poses a challenge to _entity4_ standard machine translation ( MT ) systems _/entity4_ . In _entity5_ MT _/entity5_ , the widely used approach is to apply a _entity6_ Chinese word segmenter _/entity6_ trained from _entity7_ manually annotated data _/entity7_ , using a fixed _entity8_ lexicon _/entity8_ . Such _entity9_ word segmentation _/entity9_ is not necessarily optimal for _entity10_ _P_ translation _/entity10_ . We propose a _entity11_ Bayesian semi-supervised Chinese word segmentation model _/entity11_ which uses both _entity12_ _C_ monolingual and bilingual information _/entity12_ to derive a _entity13_ segmentation _/entity13_ suitable for _entity14_ MT _/entity14_ . Experiments show that our method improves a _entity15_ state-of-the-art MT system _/entity15_ in a small and a _entity16_ large data environment _/entity16_ .	NONE entity10 entity12
This paper proposes a _entity1_ Hidden Markov Model ( HMM ) _/entity1_ and an _entity2_ HMM-based chunk tagger _/entity2_ , from which a _entity3_ named entity ( NE ) recognition ( NER ) system _/entity3_ is built to recognize and classify _entity4_ names _/entity4_ , _entity5_ times and numerical quantities _/entity5_ . Through the _entity6_ HMM _/entity6_ , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the _entity7_ words _/entity7_ , such as _entity8_ _C_ capitalization _/entity8_ and digitalization ; 2 ) _entity9_ internal semantic feature _/entity9_ of important triggers ; 3 ) _entity10_ internal gazetteer feature _/entity10_ ; 4 ) _entity11_ _P_ external macro context feature _/entity11_ . In this way , the _entity12_ NER problem _/entity12_ can be resolved effectively . Evaluation of our _entity13_ system _/entity13_ on _entity14_ MUC-6 and MUC-7 English NE tasks _/entity14_ achieves _entity15_ F-measures _/entity15_ of 96.6 % and 94.1 % respectively . It shows that the performance is significantly better than reported by any other _entity16_ machine-learning system _/entity16_ . Moreover , the _entity17_ performance _/entity17_ is even consistently better than those based on _entity18_ handcrafted rules _/entity18_ .	NONE entity11 entity8
Theoretical research in the area of _entity1_ machine translation _/entity1_ usually involves the search for and creation of an appropriate _entity2_ formalism _/entity2_ . An important issue in this respect is the way in which the _entity3_ compositionality _/entity3_ of _entity4_ translation _/entity4_ is to be defined . In this paper , we will introduce the _entity5_ anaphoric component _/entity5_ of the _entity6_ _P_ Mimo formalism _/entity6_ . It makes the definition and _entity7_ translation _/entity7_ of _entity8_ anaphoric relations _/entity8_ possible , _entity9_ _C_ relations _/entity9_ which are usually problematic for systems that adhere to _entity10_ strict compositionality _/entity10_ . In _entity11_ Mimo _/entity11_ , the _entity12_ translation _/entity12_ of _entity13_ anaphoric relations _/entity13_ is compositional . The _entity14_ anaphoric component _/entity14_ is used to define _entity15_ linguistic phenomena _/entity15_ such as _entity16_ wh-movement _/entity16_ , the _entity17_ passive _/entity17_ and the _entity18_ binding of reflexives and pronouns _/entity18_ mono-lingually . The actual working of the component will be shown in this paper by means of a detailed discussion of _entity19_ wh-movement _/entity19_ .	NONE entity6 entity9
_entity1_ Determiners _/entity1_ play an important role in conveying the _entity2_ meaning _/entity2_ of an _entity3_ utterance _/entity3_ , but they have often been disregarded , perhaps because it seemed more important to devise methods to grasp the _entity4_ global meaning _/entity4_ of a _entity5_ sentence _/entity5_ , even if not in a precise way . Another problem with _entity6_ determiners _/entity6_ is their inherent _entity7_ ambiguity _/entity7_ . In this paper we propose a _entity8_ _C_ logical formalism _/entity8_ , which , among other things , is suitable for representing _entity9_ determiners _/entity9_ without forcing a particular _entity10_ interpretation _/entity10_ when their _entity11_ _P_ meaning _/entity11_ is still not clear .	NONE entity11 entity8
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ _P_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ _C_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	NONE entity13 entity16
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ _P_ system _/entity2_ is based on a _entity3_ _C_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity2 entity3
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ GLOSSER _/entity4_ : _entity5_ _C_ English-Bulgarian _/entity5_ , _entity6_ English-Estonian _/entity6_ , _entity7_ _P_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity7 entity5
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ _P_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ _C_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity20 entity23
_entity1_ Automatic summarization _/entity1_ and _entity2_ information extraction _/entity2_ are two important Internet services . _entity3_ MUC _/entity3_ and _entity4_ SUMMAC _/entity4_ play their appropriate roles in the next generation Internet . This paper focuses on the _entity5_ automatic summarization _/entity5_ and proposes two different models to extract _entity6_ sentences _/entity6_ for _entity7_ summary generation _/entity7_ under two tasks initiated by _entity8_ SUMMAC-1 _/entity8_ . For _entity9_ categorization task _/entity9_ , _entity10_ positive feature vectors _/entity10_ and _entity11_ negative feature vectors _/entity11_ are used cooperatively to construct generic , indicative _entity12_ summaries _/entity12_ . For adhoc task , a _entity13_ text model _/entity13_ based on relationship between _entity14_ nouns _/entity14_ and _entity15_ verbs _/entity15_ is used to filter out irrelevant _entity16_ discourse segment _/entity16_ , to rank relevant _entity17_ sentences _/entity17_ , and to generate the _entity18_ _P_ user-directed summaries _/entity18_ . The result shows that the _entity19_ _C_ NormF _/entity19_ of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 . 447 . The _entity20_ NormF _/entity20_ of the best summary and that of the fixed summary for _entity21_ categorization task _/entity21_ are 0.4090 and 0.4023 . Our system outperforms the average system in _entity22_ categorization task _/entity22_ but does a common job in adhoc task .	NONE entity18 entity19
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ _P_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ _C_ formal intersections of FSAs _/entity21_ .	NONE entity19 entity21
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ _C_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ _P_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity13 entity11
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ _P_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ _C_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity10 entity11
_entity1_ Extracting semantic relationships between entities _/entity1_ is challenging . This paper investigates the incorporation of diverse _entity2_ lexical , syntactic and semantic knowledge _/entity2_ in _entity3_ feature-based relation extraction _/entity3_ using _entity4_ SVM _/entity4_ . Our study illustrates that the base _entity5_ phrase chunking _/entity5_ information is very effective for _entity6_ relation extraction _/entity6_ and contributes to most of the _entity7_ performance improvement _/entity7_ from _entity8_ syntactic _/entity8_ aspect while additional information from _entity9_ full parsing _/entity9_ gives limited further enhancement . This suggests that most of useful information in _entity10_ full parse trees _/entity10_ for _entity11_ relation extraction _/entity11_ is shallow and can be captured by _entity12_ chunking _/entity12_ . We also demonstrate how _entity13_ semantic information _/entity13_ such as _entity14_ WordNet _/entity14_ and _entity15_ Name List _/entity15_ , can be used in _entity16_ feature-based relation extraction _/entity16_ to further improve the _entity17_ performance _/entity17_ . _entity18_ _C_ Evaluation _/entity18_ on the _entity19_ ACE corpus _/entity19_ shows that effective incorporation of diverse _entity20_ features _/entity20_ enables our _entity21_ _P_ system _/entity21_ outperform previously best-reported _entity22_ systems _/entity22_ on the _entity23_ 24 ACE relation subtypes _/entity23_ and significantly outperforms _entity24_ tree kernel-based systems _/entity24_ by over 20 in _entity25_ F-measure _/entity25_ on the _entity26_ 5 ACE relation types _/entity26_ .	NONE entity21 entity18
In this paper we show how two standard outputs from _entity1_ information extraction ( IE ) systems _/entity1_ - _entity2_ named entity annotations _/entity2_ and _entity3_ scenario templates _/entity3_ - can be used to enhance access to _entity4_ _C_ text collections _/entity4_ via a standard _entity5_ _P_ text browser _/entity5_ . We describe how this information is used in a _entity6_ prototype system _/entity6_ designed to support _entity7_ information workers _/entity7_ ' access to a _entity8_ pharmaceutical news archive _/entity8_ as part of their _entity9_ industry watch _/entity9_ function . We also report results of a preliminary , _entity10_ qualitative user evaluation _/entity10_ of the system , which while broadly positive indicates further work needs to be done on the _entity11_ interface _/entity11_ to make _entity12_ users _/entity12_ aware of the increased potential of _entity13_ IE-enhanced text browsers _/entity13_ .	NONE entity5 entity4
While _entity1_ paraphrasing _/entity1_ is critical both for _entity2_ interpretation and generation of natural language _/entity2_ , current systems use manual or semi-automatic methods to collect _entity3_ paraphrases _/entity3_ . We present an _entity4_ _P_ unsupervised learning algorithm _/entity4_ for _entity5_ identification of paraphrases _/entity5_ from a _entity6_ _C_ corpus of multiple English translations _/entity6_ of the same _entity7_ source text _/entity7_ . Our approach yields _entity8_ phrasal and single word lexical paraphrases _/entity8_ as well as _entity9_ syntactic paraphrases _/entity9_ .	NONE entity4 entity6
In this paper , we describe a _entity1_ phrase-based unigram model _/entity1_ for _entity2_ _C_ statistical machine translation _/entity2_ that uses a much simpler set of _entity3_ _P_ model parameters _/entity3_ than similar _entity4_ phrase-based models _/entity4_ . The _entity5_ units of translation _/entity5_ are _entity6_ blocks _/entity6_ - pairs of _entity7_ phrases _/entity7_ . During _entity8_ decoding _/entity8_ , we use a _entity9_ block unigram model _/entity9_ and a _entity10_ word-based trigram language model _/entity10_ . During _entity11_ training _/entity11_ , the _entity12_ blocks _/entity12_ are learned from _entity13_ source interval projections _/entity13_ using an underlying _entity14_ word alignment _/entity14_ . We show experimental results on _entity15_ block selection criteria _/entity15_ based on _entity16_ unigram _/entity16_ counts and _entity17_ phrase _/entity17_ length .	NONE entity3 entity2
The paper presents a method for _entity1_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ word alignment _/entity3_ and _entity4_ _C_ word clustering _/entity4_ based on _entity5_ _P_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	USAGE entity5 entity4
This paper presents a specialized _entity1_ editor _/entity1_ for a highly structured _entity2_ dictionary _/entity2_ . The basic goal in building that _entity3_ editor _/entity3_ was to provide an adequate tool to help _entity4_ lexicologists _/entity4_ produce a valid and coherent _entity5_ dictionary _/entity5_ on the basis of a _entity6_ linguistic theory _/entity6_ . If we want valuable _entity7_ lexicons _/entity7_ and _entity8_ _C_ grammars _/entity8_ to achieve complex _entity9_ natural language processing _/entity9_ , we must provide very powerful tools to help create and ensure the validity of such complex _entity10_ _P_ linguistic databases _/entity10_ . Our most important task in building the _entity11_ editor _/entity11_ was to define a set of _entity12_ coherence rules _/entity12_ that could be computationally applied to ensure the validity of _entity13_ lexical entries _/entity13_ . A customized _entity14_ interface _/entity14_ for browsing and editing was also designed and implemented .	NONE entity10 entity8
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ _P_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ _C_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity16 entity18
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ _P_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ _C_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity8 entity10
We propose a method of organizing reading materials for _entity1_ vocabulary learning _/entity1_ . It enables us to select a concise set of reading _entity2_ texts _/entity2_ ( from a _entity3_ target corpus _/entity3_ ) that contains all the _entity4_ _C_ target vocabulary _/entity4_ to be learned . We used a specialized _entity5_ vocabulary _/entity5_ for an English certification test as the _entity6_ _P_ target vocabulary _/entity6_ and used _entity7_ English Wikipedia _/entity7_ , a free-content encyclopedia , as the _entity8_ target corpus _/entity8_ . The organized reading materials would enable learners not only to study the _entity9_ target vocabulary _/entity9_ efficiently but also to gain a variety of knowledge through reading . The reading materials are available on our web site .	NONE entity6 entity4
This paper examines the properties of _entity1_ _P_ feature-based partial descriptions _/entity1_ built on top of _entity2_ Halliday 's systemic networks _/entity2_ . We show that the crucial operation of _entity3_ _C_ consistency checking _/entity3_ for such descriptions is NP-complete , and therefore probably intractable , but proceed to develop _entity4_ algorithms _/entity4_ which can sometimes alleviate the unpleasant consequences of this _entity5_ intractability _/entity5_ .	NONE entity1 entity3
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ user _/entity3_ in _entity4_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ _P_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ _C_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity22 entity24
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ _C_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ _P_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity10 entity7
_entity1_ Taiwan Child Language Corpus _/entity1_ contains _entity2_ scripts _/entity2_ transcribed from about 330 hours of _entity3_ recordings _/entity3_ of fourteen young children from _entity4_ Southern Min Chinese _/entity4_ speaking families in Taiwan . The format of the _entity5_ corpus _/entity5_ adopts the _entity6_ Child Language Data Exchange System ( CHILDES ) _/entity6_ . The size of the _entity7_ corpus _/entity7_ is about 1.6 million _entity8_ words _/entity8_ . In this paper , we describe _entity9_ data collection _/entity9_ , _entity10_ _C_ transcription _/entity10_ , _entity11_ _P_ word segmentation _/entity11_ , and _entity12_ part-of-speech annotation _/entity12_ of this _entity13_ corpus _/entity13_ . Applications of the _entity14_ corpus _/entity14_ are also discussed .	NONE entity11 entity10
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ _C_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ _P_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity19 entity17
The _entity1_ _P_ transfer phase _/entity1_ in _entity2_ machine translation ( MT ) systems _/entity2_ has been considered to be more complicated than _entity3_ analysis _/entity3_ and _entity4_ _C_ generation _/entity4_ , since it is inherently a conglomeration of individual _entity5_ lexical rules _/entity5_ . Currently some attempts are being made to use _entity6_ case-based reasoning _/entity6_ in _entity7_ machine translation _/entity7_ , that is , to make decisions on the basis of _entity8_ translation examples _/entity8_ at appropriate pints in _entity9_ MT _/entity9_ . This paper proposes a new type of _entity10_ transfer system _/entity10_ , called a _entity11_ Similarity-driven Transfer System ( SimTran ) _/entity11_ , for use in such _entity12_ case-based MT ( CBMT ) _/entity12_ .	NONE entity1 entity4
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ _P_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ _C_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity10 entity11
Following recent developments in the _entity1_ automatic evaluation _/entity1_ of _entity2_ machine translation _/entity2_ and _entity3_ document summarization _/entity3_ , we present a similar approach , implemented in a measure called _entity4_ POURPRE _/entity4_ , for _entity5_ automatically evaluating answers to definition questions _/entity5_ . Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response . The lack of automatic methods for _entity6_ scoring system output _/entity6_ is an impediment to progress in the field , which we address with this work . Experiments with the _entity7_ TREC 2003 and TREC 2004 QA tracks _/entity7_ indicate that _entity8_ _C_ rankings _/entity8_ produced by our metric correlate highly with _entity9_ _P_ official rankings _/entity9_ , and that _entity10_ POURPRE _/entity10_ outperforms direct application of existing metrics .	NONE entity9 entity8
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ _P_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ grammatical number _/entity8_ in _entity9_ _C_ English _/entity9_ , like _entity10_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity7 entity9
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ _C_ training speakers _/entity14_ for _entity15_ _P_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity15 entity14
We address appropriate _entity1_ user modeling _/entity1_ in order to generate _entity2_ cooperative responses _/entity2_ to each _entity3_ _P_ user _/entity3_ in _entity4_ _C_ spoken dialogue systems _/entity4_ . Unlike previous studies that focus on _entity5_ user _/entity5_ 's _entity6_ knowledge _/entity6_ or typical kinds of _entity7_ users _/entity7_ , the _entity8_ user model _/entity8_ we propose is more comprehensive . Specifically , we set up three dimensions of _entity9_ user models _/entity9_ : _entity10_ skill level _/entity10_ to the system , _entity11_ knowledge level _/entity11_ on the _entity12_ target domain _/entity12_ and the degree of _entity13_ hastiness _/entity13_ . Moreover , the _entity14_ models _/entity14_ are automatically derived by _entity15_ decision tree learning _/entity15_ using real _entity16_ dialogue data _/entity16_ collected by the system . We obtained reasonable _entity17_ classification accuracy _/entity17_ for all dimensions . _entity18_ Dialogue strategies _/entity18_ based on the _entity19_ user modeling _/entity19_ are implemented in _entity20_ Kyoto city bus information system _/entity20_ that has been developed at our laboratory . Experimental evaluation shows that the _entity21_ cooperative responses _/entity21_ adaptive to _entity22_ individual users _/entity22_ serve as good guidance for _entity23_ novice users _/entity23_ without increasing the _entity24_ dialogue duration _/entity24_ for _entity25_ skilled users _/entity25_ .	NONE entity3 entity4
We present a framework for the fast _entity1_ computation _/entity1_ of _entity2_ lexical affinity models _/entity2_ . The framework is composed of a novel algorithm to efficiently compute the _entity3_ _P_ co-occurrence distribution _/entity3_ between pairs of _entity4_ terms _/entity4_ , an _entity5_ _C_ independence model _/entity5_ , and a _entity6_ parametric affinity model _/entity6_ . In comparison with previous _entity7_ models _/entity7_ , which either use arbitrary windows to compute _entity8_ similarity _/entity8_ between _entity9_ words _/entity9_ or use _entity10_ lexical affinity _/entity10_ to create _entity11_ sequential models _/entity11_ , in this paper we focus on _entity12_ models _/entity12_ intended to capture the _entity13_ co-occurrence patterns _/entity13_ of any pair of _entity14_ words _/entity14_ or _entity15_ phrases _/entity15_ at any distance in the _entity16_ corpus _/entity16_ . The framework is flexible , allowing fast _entity17_ adaptation _/entity17_ to _entity18_ applications _/entity18_ and it is scalable . We apply it in combination with a _entity19_ terabyte corpus _/entity19_ to answer _entity20_ natural language tests _/entity20_ , achieving encouraging results .	NONE entity3 entity5
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ _C_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ _P_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity23 entity20
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ corpus of sentences _/entity3_ in the domain of the _entity4_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ _C_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ _P_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity10 entity8
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ _C_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ _P_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity20 entity19
We propose a method that automatically generates _entity1_ paraphrase _/entity1_ sets from _entity2_ seed sentences _/entity2_ to be used as _entity3_ reference sets _/entity3_ in objective _entity4_ machine translation evaluation measures _/entity4_ like _entity5_ BLEU _/entity5_ and _entity6_ _P_ NIST _/entity6_ . We measured the quality of the _entity7_ paraphrases _/entity7_ produced in an experiment , i.e. , ( i ) their _entity8_ _C_ grammaticality _/entity8_ : at least 99 % correct _entity9_ sentences _/entity9_ ; ( ii ) their _entity10_ equivalence in meaning _/entity10_ : at least 96 % correct _entity11_ paraphrases _/entity11_ either by _entity12_ meaning equivalence _/entity12_ or _entity13_ entailment _/entity13_ ; and , ( iii ) the amount of internal _entity14_ lexical and syntactical variation _/entity14_ in a set of _entity15_ paraphrases _/entity15_ : slightly superior to that of _entity16_ hand-produced sets _/entity16_ . The _entity17_ paraphrase _/entity17_ sets produced by this method thus seem adequate as _entity18_ reference sets _/entity18_ to be used for _entity19_ MT evaluation _/entity19_ .	NONE entity6 entity8
To support engaging human users in robust , _entity1_ mixed-initiative speech dialogue interactions _/entity1_ which reach beyond current capabilities in _entity2_ dialogue systems _/entity2_ , the _entity3_ DARPA Communicator program _/entity3_ [ 1 ] is funding the development of a _entity4_ _P_ distributed message-passing infrastructure _/entity4_ for _entity5_ dialogue systems _/entity5_ which all _entity6_ _C_ Communicator _/entity6_ participants are using . In this presentation , we describe the features of and _entity7_ requirements _/entity7_ for a genuinely useful _entity8_ software infrastructure _/entity8_ for this purpose .	NONE entity4 entity6
A _entity1_ domain independent model _/entity1_ is proposed for the _entity2_ automated interpretation _/entity2_ of _entity3_ nominal compounds _/entity3_ in _entity4_ English _/entity4_ . This _entity5_ model _/entity5_ is meant to account for _entity6_ productive rules of interpretation _/entity6_ which are inferred from the _entity7_ morpho-syntactic and semantic characteristics _/entity7_ of the _entity8_ nominal constituents _/entity8_ . In particular , we make extensive use of Pustejovsky 's principles concerning the _entity9_ predicative information _/entity9_ associated with _entity10_ nominals _/entity10_ . We argue that it is necessary to draw a line between _entity11_ generalizable semantic principles _/entity11_ and _entity12_ domain-specific semantic information _/entity12_ . We explain this distinction and we show how this model may be applied to the _entity13_ interpretation _/entity13_ of _entity14_ compounds _/entity14_ in _entity15_ _P_ real texts _/entity15_ , provided that complementary _entity16_ _C_ semantic information _/entity16_ are retrieved .	NONE entity15 entity16
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ _P_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ _C_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity8 entity10
This paper presents a _entity1_ phrase-based statistical machine translation method _/entity1_ , based on _entity2_ non-contiguous phrases _/entity2_ , i.e . _entity3_ phrases _/entity3_ with gaps . A method for producing such _entity4_ phrases _/entity4_ from a _entity5_ word-aligned corpora _/entity5_ is proposed . A _entity6_ statistical translation model _/entity6_ is also presented that deals such _entity7_ phrases _/entity7_ , as well as a _entity8_ _C_ training method _/entity8_ based on the maximization of _entity9_ translation accuracy _/entity9_ , as measured with the _entity10_ _P_ NIST evaluation metric _/entity10_ . _entity11_ Translations _/entity11_ are produced by means of a _entity12_ beam-search decoder _/entity12_ . Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the _entity13_ training data _/entity13_ .	NONE entity10 entity8
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ _C_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ _P_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity13 entity10
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ _P_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ _C_ predicates _/entity17_ .	NONE entity15 entity17
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ _P_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ _C_ noun phrases _/entity20_ .	NONE entity17 entity20
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ _P_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ _C_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity15 entity18
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ _P_ parse trees _/entity2_ , which we call _entity3_ _C_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ words _/entity21_ ) , which is comparable to that of an _entity22_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity2 entity3
It is challenging to translate _entity1_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ _P_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ _C_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity13 entity15
A purely functional implementation of _entity1_ _P_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ _C_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity1 entity3
In this paper we compare two competing approaches to _entity1_ _C_ part-of-speech tagging _/entity1_ , _entity2_ statistical and constraint-based disambiguation _/entity2_ , using _entity3_ French _/entity3_ as our _entity4_ _P_ test language _/entity4_ . We imposed a time limit on our experiment : the amount of time spent on the design of our _entity5_ constraint system _/entity5_ was about the same as the time we used to train and test the easy-to-implement _entity6_ statistical model _/entity6_ . We describe the two systems and compare the results . The _entity7_ accuracy _/entity7_ of the _entity8_ statistical method _/entity8_ is reasonably good , comparable to _entity9_ taggers _/entity9_ for _entity10_ English _/entity10_ . But the _entity11_ constraint-based tagger _/entity11_ seems to be superior even with the limited time we allowed ourselves for _entity12_ rule development _/entity12_ .	NONE entity4 entity1
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ _P_ RCL parsers _/entity13_ . The _entity14_ _C_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ wide coverage English grammar _/entity21_ are given .	NONE entity13 entity14
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ _C_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ _P_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity21 entity18
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ training and test data _/entity5_ with respect to _entity6_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ _C_ training data _/entity7_ labeled with _entity8_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ domain _/entity9_ , _entity10_ _P_ topic _/entity10_ and time .	NONE entity10 entity7
In this paper we introduce a _entity1_ modal language LT _/entity1_ for imposing _entity2_ _P_ constraints _/entity2_ on _entity3_ trees _/entity3_ , and an extension _entity4_ _C_ LT ( LF ) _/entity4_ for imposing _entity5_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ GPSG _/entity9_ can be captured in _entity10_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	NONE entity2 entity4
This paper gives an overall account of a prototype _entity1_ _C_ natural language question answering system _/entity1_ , called _entity2_ _P_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity2 entity1
This paper reports recent research into methods for _entity1_ creating natural language text _/entity1_ . A new _entity2_ _P_ processing paradigm _/entity2_ called _entity3_ Fragment-and-Compose _/entity3_ has been created and an experimental system implemented in it . The _entity4_ _C_ knowledge _/entity4_ to be expressed in _entity5_ text _/entity5_ is first divided into small _entity6_ propositional units _/entity6_ , which are then composed into appropriate combinations and converted into _entity7_ text _/entity7_ . _entity8_ KDS ( Knowledge Delivery System ) _/entity8_ , which embodies this paradigm , has distinct parts devoted to creation of the _entity9_ propositional units _/entity9_ , to organization of the _entity10_ text _/entity10_ , to prevention of _entity11_ excess redundancy _/entity11_ , to creation of combinations of units , to evaluation of these combinations as potential _entity12_ sentences _/entity12_ , to selection of the best among competing combinations , and to creation of the _entity13_ final text _/entity13_ . The _entity14_ Fragment-and-Compose paradigm _/entity14_ and the _entity15_ computational methods _/entity15_ of _entity16_ KDS _/entity16_ are described .	NONE entity2 entity4
The _entity1_ transfer phase _/entity1_ in _entity2_ machine translation ( MT ) systems _/entity2_ has been considered to be more complicated than _entity3_ analysis _/entity3_ and _entity4_ generation _/entity4_ , since it is inherently a conglomeration of individual _entity5_ lexical rules _/entity5_ . Currently some attempts are being made to use _entity6_ case-based reasoning _/entity6_ in _entity7_ _C_ machine translation _/entity7_ , that is , to make decisions on the basis of _entity8_ _P_ translation examples _/entity8_ at appropriate pints in _entity9_ MT _/entity9_ . This paper proposes a new type of _entity10_ transfer system _/entity10_ , called a _entity11_ Similarity-driven Transfer System ( SimTran ) _/entity11_ , for use in such _entity12_ case-based MT ( CBMT ) _/entity12_ .	NONE entity8 entity7
_entity1_ Techniques for automatically training _/entity1_ modules of a _entity2_ natural language generator _/entity2_ have recently been proposed , but a fundamental concern is whether the _entity3_ quality _/entity3_ of _entity4_ utterances _/entity4_ produced with _entity5_ trainable components _/entity5_ can compete with _entity6_ _C_ hand-crafted template-based or rule-based approaches _/entity6_ . In this paper We experimentally evaluate a _entity7_ trainable sentence planner _/entity7_ for a _entity8_ _P_ spoken dialogue system _/entity8_ by eliciting _entity9_ subjective human judgments _/entity9_ . In order to perform an exhaustive comparison , we also evaluate a _entity10_ hand-crafted template-based generation component _/entity10_ , two _entity11_ rule-based sentence planners _/entity11_ , and two _entity12_ baseline sentence planners _/entity12_ . We show that the _entity13_ trainable sentence planner _/entity13_ performs better than the _entity14_ rule-based systems _/entity14_ and the _entity15_ baselines _/entity15_ , and as well as the _entity16_ hand-crafted system _/entity16_ .	NONE entity8 entity6
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ _P_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ _C_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ treebank _/entity12_ more valuable as a _entity13_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity6 entity8
_entity1_ Statistical machine translation ( SMT ) _/entity1_ is currently one of the hot spots in _entity2_ natural language processing _/entity2_ . Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that _entity3_ SMT _/entity3_ gives competitive results to _entity4_ rule-based translation systems _/entity4_ , requiring significantly less development time . This is particularly important when building _entity5_ _C_ translation systems _/entity5_ for new _entity6_ language pairs _/entity6_ or new _entity7_ _P_ domains _/entity7_ . This workshop is intended to give an introduction to _entity8_ statistical machine translation _/entity8_ with a focus on practical considerations . Participants should be able , after attending this workshop , to set out building an _entity9_ SMT system _/entity9_ themselves and achieving good _entity10_ baseline results _/entity10_ in a short time . The tutorial will cover the basics of _entity11_ SMT _/entity11_ : Theory will be put into practice . _entity12_ STTK _/entity12_ , a _entity13_ statistical machine translation tool kit _/entity13_ , will be introduced and used to build a working _entity14_ translation system _/entity14_ . _entity15_ STTK _/entity15_ has been developed by the presenter and co-workers over a number of years and is currently used as the basis of _entity16_ CMU 's SMT system _/entity16_ . It has also successfully been coupled with _entity17_ rule-based and example based machine translation modules _/entity17_ to build a _entity18_ multi engine machine translation system _/entity18_ . The _entity19_ source code _/entity19_ of the _entity20_ tool kit _/entity20_ will be made available .	NONE entity7 entity5
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ _P_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ _C_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity9 entity12
This paper describes a novel _entity1_ system _/entity1_ for _entity2_ acquiring adjectival subcategorization frames _/entity2_ ( _entity3_ scfs _/entity3_ ) and associated frequency information from _entity4_ English _/entity4_ _entity5_ corpus data _/entity5_ . The _entity6_ system _/entity6_ incorporates a _entity7_ decision-tree classifier _/entity7_ for 30 _entity8_ scf types _/entity8_ which tests for the presence of _entity9_ grammatical relations _/entity9_ ( _entity10_ grs _/entity10_ ) in the _entity11_ _C_ output _/entity11_ of a robust _entity12_ _P_ statistical parser _/entity12_ . It uses a powerful _entity13_ pattern-matching language _/entity13_ to classify _entity14_ grs _/entity14_ into _entity15_ frames _/entity15_ hierarchically in a way that mirrors _entity16_ inheritance-based lexica _/entity16_ . The _entity17_ experiments _/entity17_ show that the _entity18_ system _/entity18_ is able to detect _entity19_ scf types _/entity19_ with _entity20_ 70 % precision _/entity20_ and _entity21_ 66 % recall rate _/entity21_ . A new _entity22_ tool _/entity22_ for _entity23_ linguistic annotation _/entity23_ of _entity24_ scfs _/entity24_ in _entity25_ corpus data _/entity25_ is also introduced which can considerably alleviate the process of obtaining _entity26_ training and test data _/entity26_ for _entity27_ subcategorization acquisition _/entity27_ .	NONE entity12 entity11
Following recent developments in the _entity1_ _P_ automatic evaluation _/entity1_ of _entity2_ machine translation _/entity2_ and _entity3_ _C_ document summarization _/entity3_ , we present a similar approach , implemented in a measure called _entity4_ POURPRE _/entity4_ , for _entity5_ automatically evaluating answers to definition questions _/entity5_ . Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response . The lack of automatic methods for _entity6_ scoring system output _/entity6_ is an impediment to progress in the field , which we address with this work . Experiments with the _entity7_ TREC 2003 and TREC 2004 QA tracks _/entity7_ indicate that _entity8_ rankings _/entity8_ produced by our metric correlate highly with _entity9_ official rankings _/entity9_ , and that _entity10_ POURPRE _/entity10_ outperforms direct application of existing metrics .	NONE entity1 entity3
We investigate independent and relevant event-based extractive _entity1_ mutli-document summarization approaches _/entity1_ . In this paper , _entity2_ events _/entity2_ are defined as _entity3_ event terms _/entity3_ and _entity4_ associated event elements _/entity4_ . With independent approach , we identify important _entity5_ contents _/entity5_ by frequency of _entity6_ events _/entity6_ . With relevant approach , we identify important contents by _entity7_ _P_ PageRank algorithm _/entity7_ on the _entity8_ event map _/entity8_ constructed from _entity9_ _C_ documents _/entity9_ . Experimental results are encouraging .	NONE entity7 entity9
The _entity1_ JAVELIN system _/entity1_ integrates a flexible , _entity2_ planning-based architecture _/entity2_ with a variety of _entity3_ language processing modules _/entity3_ to provide an _entity4_ open-domain question answering capability _/entity4_ on _entity5_ free text _/entity5_ . The demonstration will focus on how _entity6_ JAVELIN _/entity6_ processes _entity7_ questions _/entity7_ and retrieves the most likely _entity8_ answer candidates _/entity8_ from the given _entity9_ text corpus _/entity9_ . The operation of the system will be explained in depth through browsing the _entity10_ _C_ repository _/entity10_ of _entity11_ data objects _/entity11_ created by the system during each _entity12_ _P_ question answering session _/entity12_ .	NONE entity12 entity10
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ _P_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ _C_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity16 entity19
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ _C_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ _P_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity20 entity17
We present a novel approach for automatically acquiring _entity1_ English topic signatures _/entity1_ . Given a particular _entity2_ concept _/entity2_ , or _entity3_ word sense _/entity3_ , a _entity4_ _P_ topic signature _/entity4_ is a set of _entity5_ _C_ words _/entity5_ that tend to co-occur with it . _entity6_ Topic signatures _/entity6_ can be useful in a number of _entity7_ Natural Language Processing ( NLP ) applications _/entity7_ , such as _entity8_ Word Sense Disambiguation ( WSD ) _/entity8_ and _entity9_ Text Summarisation _/entity9_ . Our method takes advantage of the different way in which _entity10_ word senses _/entity10_ are lexicalised in _entity11_ English _/entity11_ and _entity12_ Chinese _/entity12_ , and also exploits the large amount of _entity13_ Chinese text _/entity13_ available in _entity14_ corpora _/entity14_ and on the Web . We evaluated the _entity15_ topic signatures _/entity15_ on a _entity16_ WSD task _/entity16_ , where we trained a _entity17_ second-order vector cooccurrence algorithm _/entity17_ on _entity18_ standard WSD datasets _/entity18_ , with promising results .	NONE entity4 entity5
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ _P_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ _C_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity24 entity27
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ news commentary texts _/entity7_ : _entity8_ _P_ Colon _/entity8_ , _entity9_ _C_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity8 entity9
We present a _entity1_ _P_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ _C_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity1 entity4
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ _P_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ _C_ outputs _/entity17_ .	NONE entity15 entity17
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ _P_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ duration model _/entity3_ has reduced the _entity4_ _C_ error rate _/entity4_ by about 10 % in both _entity5_ triphone and semiphone systems _/entity5_ . A new _entity6_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity2 entity4
This paper investigates some _entity1_ computational problems _/entity1_ associated with _entity2_ probabilistic translation models _/entity2_ that have recently been adopted in the literature on _entity3_ machine translation _/entity3_ . These _entity4_ _P_ models _/entity4_ can be viewed as pairs of _entity5_ _C_ probabilistic context-free grammars _/entity5_ working in a 'synchronous ' way . Two _entity6_ hardness _/entity6_ results for the class _entity7_ NP _/entity7_ are reported , along with an _entity8_ exponential time lower-bound _/entity8_ for certain classes of algorithms that are currently used in the literature .	NONE entity4 entity5
This paper proposes an _entity1_ annotating scheme _/entity1_ that encodes _entity2_ honorifics _/entity2_ ( respectful words ) . _entity3_ Honorifics _/entity3_ are used extensively in _entity4_ Japanese _/entity4_ , reflecting the social relationship ( e.g . social ranks and age ) of the _entity5_ referents _/entity5_ . This _entity6_ referential information _/entity6_ is vital for resolving _entity7_ _P_ zero pronouns _/entity7_ and improving _entity8_ machine translation outputs _/entity8_ . Annotating _entity9_ _C_ honorifics _/entity9_ is a complex task that involves identifying a _entity10_ predicate _/entity10_ with _entity11_ honorifics _/entity11_ , assigning _entity12_ ranks _/entity12_ to _entity13_ referents _/entity13_ of the _entity14_ predicate _/entity14_ , calibrating the _entity15_ ranks _/entity15_ , and connecting _entity16_ referents _/entity16_ with their _entity17_ predicates _/entity17_ .	NONE entity7 entity9
The theoretical study of the _entity1_ range concatenation grammar [ RCG ] formalism _/entity1_ has revealed many attractive properties which may be used in _entity2_ NLP _/entity2_ . In particular , _entity3_ range concatenation languages [ RCL ] _/entity3_ can be parsed in _entity4_ polynomial time _/entity4_ and many classical _entity5_ grammatical formalisms _/entity5_ can be translated into equivalent _entity6_ RCGs _/entity6_ without increasing their _entity7_ worst-case parsing time complexity _/entity7_ . For example , after _entity8_ translation _/entity8_ into an equivalent _entity9_ RCG _/entity9_ , any _entity10_ tree adjoining grammar _/entity10_ can be parsed in _entity11_ O ( n6 ) time _/entity11_ . In this paper , we study a _entity12_ parsing technique _/entity12_ whose purpose is to improve the practical efficiency of _entity13_ RCL parsers _/entity13_ . The _entity14_ non-deterministic parsing choices _/entity14_ of the _entity15_ main parser _/entity15_ for a _entity16_ language L _/entity16_ are directed by a _entity17_ guide _/entity17_ which uses the _entity18_ _P_ shared derivation forest _/entity18_ output by a prior _entity19_ RCL parser _/entity19_ for a suitable _entity20_ superset of L _/entity20_ . The results of a practical evaluation of this method on a _entity21_ _C_ wide coverage English grammar _/entity21_ are given .	NONE entity18 entity21
To support engaging human users in robust , _entity1_ mixed-initiative speech dialogue interactions _/entity1_ which reach beyond current capabilities in _entity2_ dialogue systems _/entity2_ , the _entity3_ DARPA Communicator program _/entity3_ [ 1 ] is funding the development of a _entity4_ distributed message-passing infrastructure _/entity4_ for _entity5_ _P_ dialogue systems _/entity5_ which all _entity6_ Communicator _/entity6_ participants are using . In this presentation , we describe the features of and _entity7_ requirements _/entity7_ for a genuinely useful _entity8_ _C_ software infrastructure _/entity8_ for this purpose .	NONE entity5 entity8
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ _P_ languages _/entity5_ . For various reasons , _entity6_ _C_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity5 entity6
With performance above 97 % _entity1_ accuracy _/entity1_ for _entity2_ newspaper text _/entity2_ , _entity3_ part of speech ( pos ) tagging _/entity3_ might be considered a solved problem . Previous studies have shown that allowing the _entity4_ parser _/entity4_ to resolve _entity5_ pos tag ambiguity _/entity5_ does not improve performance . However , for _entity6_ grammar formalisms _/entity6_ which use more _entity7_ fine-grained grammatical categories _/entity7_ , for example _entity8_ tag _/entity8_ and _entity9_ ccg _/entity9_ , _entity10_ tagging accuracy _/entity10_ is much lower . In fact , for these _entity11_ formalisms _/entity11_ , premature _entity12_ _C_ ambiguity resolution _/entity12_ makes _entity13_ parsing _/entity13_ infeasible . We describe a _entity14_ _P_ multi-tagging approach _/entity14_ which maintains a suitable level of _entity15_ lexical category ambiguity _/entity15_ for accurate and efficient _entity16_ ccg parsing _/entity16_ . We extend this _entity17_ multi-tagging approach _/entity17_ to the _entity18_ pos level _/entity18_ to overcome errors introduced by automatically assigned _entity19_ pos tags _/entity19_ . Although _entity20_ pos tagging accuracy _/entity20_ seems high , maintaining some _entity21_ pos tag ambiguity _/entity21_ in the _entity22_ language processing pipeline _/entity22_ results in more accurate _entity23_ ccg supertagging _/entity23_ .	NONE entity14 entity12
This paper proposes a generic _entity1_ mathematical formalism _/entity1_ for the combination of various _entity2_ structures _/entity2_ : _entity3_ strings _/entity3_ , _entity4_ trees _/entity4_ , _entity5_ dags _/entity5_ , _entity6_ graphs _/entity6_ , and products of them . The _entity7_ polarization _/entity7_ of the objects of the _entity8_ elementary structures _/entity8_ controls the _entity9_ saturation _/entity9_ of the final _entity10_ structure _/entity10_ . This formalism is both elementary and powerful enough to strongly simulate many _entity11_ grammar formalisms _/entity11_ , such as _entity12_ _P_ rewriting systems _/entity12_ , _entity13_ dependency grammars _/entity13_ , _entity14_ TAG _/entity14_ , _entity15_ _C_ HPSG _/entity15_ and _entity16_ LFG _/entity16_ .	NONE entity12 entity15
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ _C_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ _P_ TUIT _/entity19_ and its capabilities .	NONE entity19 entity18
We investigate the _entity1_ verbal and nonverbal means _/entity1_ for _entity2_ grounding _/entity2_ , and propose a design for _entity3_ embodied conversational agents _/entity3_ that relies on both kinds of _entity4_ signals _/entity4_ to establish _entity5_ common ground _/entity5_ in _entity6_ _P_ human-computer interaction _/entity6_ . We analyzed _entity7_ _C_ eye gaze _/entity7_ , _entity8_ head nods _/entity8_ and _entity9_ attentional focus _/entity9_ in the context of a _entity10_ direction-giving task _/entity10_ . The distribution of _entity11_ nonverbal behaviors _/entity11_ differed depending on the type of _entity12_ dialogue move _/entity12_ being grounded , and the overall pattern reflected a monitoring of lack of _entity13_ negative feedback _/entity13_ . Based on these results , we present an _entity14_ ECA _/entity14_ that uses _entity15_ verbal and nonverbal grounding acts _/entity15_ to update _entity16_ dialogue state _/entity16_ .	NONE entity6 entity7
This paper presents necessary and sufficient conditions for the use of _entity1_ demonstrative expressions _/entity1_ in _entity2_ English _/entity2_ and discusses implications for current _entity3_ discourse processing algorithms _/entity3_ . We examine a broad range of _entity4_ texts _/entity4_ to show how the distribution of _entity5_ demonstrative forms and functions _/entity5_ is _entity6_ genre dependent _/entity6_ . This research is part of a larger study of _entity7_ _P_ anaphoric expressions _/entity7_ , the results of which will be incorporated into a _entity8_ _C_ natural language generation system _/entity8_ .	NONE entity7 entity8
In this paper , we improve an _entity1_ _C_ unsupervised learning method _/entity1_ using the _entity2_ _P_ Expectation-Maximization ( EM ) algorithm _/entity2_ proposed by Nigam et al . for _entity3_ text classification problems _/entity3_ in order to apply it to _entity4_ word sense disambiguation ( WSD ) problems _/entity4_ . The improved method stops the _entity5_ EM algorithm _/entity5_ at the _entity6_ optimum iteration number _/entity6_ . To estimate that number , we propose two methods . In experiments , we solved 50 _entity7_ noun WSD problems _/entity7_ in the _entity8_ Japanese Dictionary Task in SENSEVAL2 _/entity8_ . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for _entity9_ verb WSD problems _/entity9_ .	NONE entity2 entity1
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ _P_ rules _/entity14_ of other _entity15_ _C_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	PART_WHOLE entity14 entity15
This paper considers the problem of automatic assessment of _entity1_ _P_ local coherence _/entity1_ . We present a novel _entity2_ _C_ entity-based representation _/entity2_ of _entity3_ discourse _/entity3_ which is inspired by _entity4_ Centering Theory _/entity4_ and can be computed automatically from _entity5_ raw text _/entity5_ . We view _entity6_ coherence assessment _/entity6_ as a _entity7_ ranking learning problem _/entity7_ and show that the proposed _entity8_ discourse representation _/entity8_ supports the effective learning of a _entity9_ ranking function _/entity9_ . Our experiments demonstrate that the _entity10_ induced model _/entity10_ achieves significantly higher _entity11_ accuracy _/entity11_ than a _entity12_ state-of-the-art coherence model _/entity12_ .	NONE entity1 entity2
We propose a method that automatically generates _entity1_ paraphrase _/entity1_ sets from _entity2_ seed sentences _/entity2_ to be used as _entity3_ reference sets _/entity3_ in objective _entity4_ machine translation evaluation measures _/entity4_ like _entity5_ BLEU _/entity5_ and _entity6_ NIST _/entity6_ . We measured the quality of the _entity7_ paraphrases _/entity7_ produced in an experiment , i.e. , ( i ) their _entity8_ grammaticality _/entity8_ : at least 99 % correct _entity9_ sentences _/entity9_ ; ( ii ) their _entity10_ equivalence in meaning _/entity10_ : at least 96 % correct _entity11_ paraphrases _/entity11_ either by _entity12_ meaning equivalence _/entity12_ or _entity13_ entailment _/entity13_ ; and , ( iii ) the amount of internal _entity14_ lexical and syntactical variation _/entity14_ in a set of _entity15_ _C_ paraphrases _/entity15_ : slightly superior to that of _entity16_ hand-produced sets _/entity16_ . The _entity17_ _P_ paraphrase _/entity17_ sets produced by this method thus seem adequate as _entity18_ reference sets _/entity18_ to be used for _entity19_ MT evaluation _/entity19_ .	NONE entity17 entity15
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ _P_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ _C_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity15 entity17
This paper proposes a practical approach employing _entity1_ n-gram models _/entity1_ and _entity2_ error-correction rules _/entity2_ for _entity3_ _C_ Thai key prediction _/entity3_ and _entity4_ _P_ Thai-English language identification _/entity4_ . The paper also proposes _entity5_ rule-reduction algorithm _/entity5_ applying _entity6_ mutual information _/entity6_ to reduce the _entity7_ error-correction rules _/entity7_ . Our algorithm reported more than 99 % _entity8_ accuracy _/entity8_ in both _entity9_ language identification _/entity9_ and _entity10_ key prediction _/entity10_ .	NONE entity4 entity3
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ _C_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ _P_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity10 entity8
This paper introduces _entity1_ primitive Optimality Theory ( OTP ) _/entity1_ , a linguistically motivated formalization of _entity2_ OT _/entity2_ . _entity3_ OTP _/entity3_ specifies the _entity4_ class of autosegmental representations _/entity4_ , the _entity5_ universal generator Gen _/entity5_ , and the two simple families of _entity6_ permissible constraints _/entity6_ . In contrast to less restricted _entity7_ theories _/entity7_ using _entity8_ Generalized Alignment _/entity8_ , _entity9_ OTP _/entity9_ 's optimal _entity10_ surface forms _/entity10_ can be generated with _entity11_ finite-state methods _/entity11_ adapted from ( Ellison , 1994 ) . Unfortunately these _entity12_ methods _/entity12_ take _entity13_ _P_ time exponential on the size of the grammar _/entity13_ . Indeed the _entity14_ generation problem _/entity14_ is shown _entity15_ _C_ NP-complete _/entity15_ in this sense . However , techniques are discussed for making _entity16_ Ellison 's approach _/entity16_ fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a _entity17_ grammar _/entity17_ fragment of moderate size . One avenue for future improvements is a new _entity18_ finite-state notion _/entity18_ , _entity19_ factored automata _/entity19_ , where _entity20_ regular languages _/entity20_ are represented compactly via _entity21_ formal intersections of FSAs _/entity21_ .	NONE entity13 entity15
In this paper , we propose a novel _entity1_ Cooperative Model _/entity1_ for _entity2_ natural language understanding _/entity2_ in a _entity3_ dialogue system _/entity3_ . We build this based on both _entity4_ Finite State Model ( FSM ) _/entity4_ and _entity5_ Statistical Learning Model ( SLM ) _/entity5_ . _entity6_ _P_ FSM _/entity6_ provides two strategies for _entity7_ language understanding _/entity7_ and have a high accuracy but little robustness and flexibility . _entity8_ Statistical approach _/entity8_ is much more robust but less accurate . _entity9_ _C_ Cooperative Model _/entity9_ incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies .	NONE entity6 entity9
This paper proposes _entity1_ document oriented preference sets ( DoPS ) _/entity1_ for the disambiguation of the _entity2_ dependency structure _/entity2_ of _entity3_ sentences _/entity3_ . The _entity4_ DoPS system _/entity4_ extracts preference knowledge from a _entity5_ target document _/entity5_ or other _entity6_ documents _/entity6_ automatically . _entity7_ Sentence ambiguities _/entity7_ can be resolved by using domain targeted preference knowledge without using complicated large _entity8_ _P_ knowledgebases _/entity8_ . _entity9_ Implementation _/entity9_ and _entity10_ _C_ empirical results _/entity10_ are described for the the analysis of _entity11_ dependency structures _/entity11_ of _entity12_ Japanese patent claim sentences _/entity12_ .	NONE entity8 entity10
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ _P_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ _C_ predictive performance _/entity8_ of our _entity9_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity6 entity8
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ _P_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ _C_ MBR decoders _/entity16_ on a _entity17_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ loss functions _/entity20_ .	NONE entity13 entity16
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ _P_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ _C_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ domains _/entity19_ via _entity20_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity8 entity11
This paper presents an _entity1_ unsupervised learning approach _/entity1_ to building a _entity2_ non-English ( Arabic ) stemmer _/entity2_ . The _entity3_ stemming model _/entity3_ is based on _entity4_ statistical machine translation _/entity4_ and it uses an _entity5_ English stemmer _/entity5_ and a small ( 10K sentences ) _entity6_ parallel corpus _/entity6_ as its sole _entity7_ training resources _/entity7_ . No _entity8_ parallel text _/entity8_ is needed after the _entity9_ training phase _/entity9_ . _entity10_ Monolingual , unannotated text _/entity10_ can be used to further improve the _entity11_ stemmer _/entity11_ by allowing it to adapt to a desired _entity12_ domain _/entity12_ or _entity13_ genre _/entity13_ . Examples and results will be given for _entity14_ Arabic _/entity14_ , but the approach is applicable to any _entity15_ language _/entity15_ that needs _entity16_ _C_ affix removal _/entity16_ . Our _entity17_ _P_ resource-frugal approach _/entity17_ results in 87.5 % _entity18_ agreement _/entity18_ with a state of the art , proprietary _entity19_ Arabic stemmer _/entity19_ built using _entity20_ rules _/entity20_ , _entity21_ affix lists _/entity21_ , and _entity22_ human annotated text _/entity22_ , in addition to an _entity23_ unsupervised component _/entity23_ . _entity24_ Task-based evaluation _/entity24_ using _entity25_ Arabic information retrieval _/entity25_ indicates an improvement of 22-38 % in _entity26_ average precision _/entity26_ over _entity27_ unstemmed text _/entity27_ , and 96 % of the performance of the proprietary _entity28_ stemmer _/entity28_ above .	NONE entity17 entity16
In this paper we study a set of problems that are of considerable importance to _entity1_ Statistical Machine Translation ( SMT ) _/entity1_ but which have not been addressed satisfactorily by the _entity2_ SMT research community _/entity2_ . Over the last decade , a variety of _entity3_ SMT algorithms _/entity3_ have been built and empirically tested whereas little is known about the _entity4_ computational complexity _/entity4_ of some of the fundamental problems of _entity5_ SMT _/entity5_ . Our work aims at providing useful insights into the the _entity6_ computational complexity _/entity6_ of those problems . We prove that while _entity7_ IBM Models 1-2 _/entity7_ are conceptually and computationally simple , computations involving the higher ( and more useful ) _entity8_ models _/entity8_ are _entity9_ hard _/entity9_ . Since it is unlikely that there exists a _entity10_ polynomial time solution _/entity10_ for any of these _entity11_ hard problems _/entity11_ ( unless _entity12_ P = NP _/entity12_ and _entity13_ P # P = P _/entity13_ ) , our results highlight and justify the need for developing _entity14_ _P_ polynomial time approximations _/entity14_ for these computations . We also discuss some practical ways of dealing with _entity15_ _C_ complexity _/entity15_ .	NONE entity14 entity15
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ _P_ KL-ONE _/entity11_ which represents the _entity12_ _C_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	MODEL-FEATURE entity11 entity12
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ _C_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ _P_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity29 entity26
We propose a new _entity1_ phrase-based translation model _/entity1_ and _entity2_ decoding algorithm _/entity2_ that enables us to evaluate and compare several , previously proposed _entity3_ phrase-based translation models _/entity3_ . Within our framework , we carry out a large number of experiments to understand better and explain why _entity4_ phrase-based models _/entity4_ outperform _entity5_ word-based models _/entity5_ . Our empirical results , which hold for all examined _entity6_ language pairs _/entity6_ , suggest that the highest levels of performance can be obtained through relatively simple means : _entity7_ heuristic learning _/entity7_ of _entity8_ phrase translations _/entity8_ from _entity9_ word-based alignments _/entity9_ and _entity10_ lexical weighting _/entity10_ of _entity11_ _P_ phrase translations _/entity11_ . Surprisingly , learning _entity12_ phrases _/entity12_ longer than three _entity13_ _C_ words _/entity13_ and learning _entity14_ phrases _/entity14_ from _entity15_ high-accuracy word-level alignment models _/entity15_ does not have a strong impact on performance . Learning only _entity16_ syntactically motivated phrases _/entity16_ degrades the performance of our systems .	NONE entity11 entity13
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ beliefs _/entity5_ , _entity6_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ _C_ speaker _/entity23_ leeway in forming an _entity24_ _P_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity24 entity23
Methods developed for _entity1_ spelling correction _/entity1_ for _entity2_ languages _/entity2_ like _entity3_ English _/entity3_ ( see the review by Kukich ( Kukich , 1992 ) ) are not readily applicable to _entity4_ agglutinative languages _/entity4_ . This poster presents an approach to _entity5_ _C_ spelling correction _/entity5_ in _entity6_ agglutinative languages _/entity6_ that is based on _entity7_ _P_ two-level morphology _/entity7_ and a _entity8_ dynamic-programming based search algorithm _/entity8_ . After an overview of our approach , we present results from experiments with _entity9_ spelling correction _/entity9_ in _entity10_ Turkish _/entity10_ .	NONE entity7 entity5
_entity1_ _P_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ _C_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity1 entity2
The _entity1_ translation _/entity1_ of _entity2_ English text _/entity2_ into _entity3_ _C_ American Sign Language ( ASL ) animation _/entity3_ tests the limits of _entity4_ traditional MT architectural designs _/entity4_ . A new _entity5_ semantic representation _/entity5_ is proposed that uses _entity6_ _P_ virtual reality 3D scene modeling software _/entity6_ to produce _entity7_ spatially complex ASL phenomena _/entity7_ called `` _entity8_ classifier predicates _/entity8_ . '' The model acts as an _entity9_ interlingua _/entity9_ within a new _entity10_ multi-pathway MT architecture design _/entity10_ that also incorporates _entity11_ transfer _/entity11_ and _entity12_ direct approaches _/entity12_ into a single system .	NONE entity6 entity3
We apply a _entity1_ decision tree based approach _/entity1_ to _entity2_ pronoun resolution _/entity2_ in _entity3_ spoken dialogue _/entity3_ . Our system deals with _entity4_ pronouns _/entity4_ with _entity5_ NP- and non-NP-antecedents _/entity5_ . We present a set of _entity6_ features _/entity6_ designed for _entity7_ pronoun resolution _/entity7_ in _entity8_ _C_ spoken dialogue _/entity8_ and determine the most promising _entity9_ features _/entity9_ . We evaluate the system on twenty _entity10_ Switchboard dialogues _/entity10_ and show that it compares well to _entity11_ _P_ Byron 's ( 2002 ) manually tuned system _/entity11_ .	NONE entity11 entity8
In this paper , we improve an _entity1_ unsupervised learning method _/entity1_ using the _entity2_ Expectation-Maximization ( EM ) algorithm _/entity2_ proposed by Nigam et al . for _entity3_ text classification problems _/entity3_ in order to apply it to _entity4_ word sense disambiguation ( WSD ) problems _/entity4_ . The improved method stops the _entity5_ EM algorithm _/entity5_ at the _entity6_ _C_ optimum iteration number _/entity6_ . To estimate that number , we propose two methods . In experiments , we solved 50 _entity7_ noun WSD problems _/entity7_ in the _entity8_ _P_ Japanese Dictionary Task in SENSEVAL2 _/entity8_ . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for _entity9_ verb WSD problems _/entity9_ .	NONE entity8 entity6
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ inflection _/entity5_ such as _entity6_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ _C_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ _P_ word classification _/entity20_ .	NONE entity20 entity19
This paper presents a _entity1_ corpus study _/entity1_ that explores the extent to which captions contribute to recognizing the intended message of an _entity2_ information graphic _/entity2_ . It then presents an implemented _entity3_ graphic interpretation system _/entity3_ that takes into account a variety of _entity4_ _P_ communicative signals _/entity4_ , and an evaluation study showing that evidence obtained from _entity5_ shallow processing _/entity5_ of the graphic 's caption has a significant impact on the system 's success . This work is part of a larger project whose goal is to provide _entity6_ sight-impaired users _/entity6_ with effective access to _entity7_ _C_ information graphics _/entity7_ .	NONE entity4 entity7
It is well-known that there are _entity1_ polysemous words _/entity1_ like _entity2_ sentence _/entity2_ whose _entity3_ meaning _/entity3_ or _entity4_ sense _/entity4_ depends on the context of use . We have recently reported on two new _entity5_ word-sense disambiguation systems _/entity5_ , one trained on _entity6_ bilingual material _/entity6_ ( the _entity7_ Canadian Hansards _/entity7_ ) and the other trained on _entity8_ monolingual material _/entity8_ ( _entity9_ Roget 's Thesaurus _/entity9_ and _entity10_ Grolier 's Encyclopedia _/entity10_ ) . As this work was nearing completion , we observed a very strong _entity11_ discourse _/entity11_ effect . That is , if a _entity12_ polysemous word _/entity12_ such as _entity13_ sentence _/entity13_ appears two or more times in a _entity14_ _C_ well-written discourse _/entity14_ , it is extremely likely that they will all share the same _entity15_ _P_ sense _/entity15_ . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share _entity16_ sense _/entity16_ in the same _entity17_ discourse _/entity17_ is extremely strong ( 98 % ) . This result can be used as an additional source of _entity18_ constraint _/entity18_ for improving the performance of the _entity19_ word-sense disambiguation algorithm _/entity19_ . In addition , it could also be used to help evaluate _entity20_ disambiguation algorithms _/entity20_ that did not make use of the _entity21_ discourse constraint _/entity21_ .	NONE entity15 entity14
This paper describes a recently collected _entity1_ _P_ spoken language corpus _/entity1_ for the _entity2_ ATIS ( Air Travel Information System ) domain _/entity2_ . This data collection effort has been co-ordinated by _entity3_ _C_ MADCOW ( Multi-site ATIS Data COllection Working group ) _/entity3_ . We summarize the motivation for this effort , the goals , the implementation of a _entity4_ multi-site data collection paradigm _/entity4_ , and the accomplishments of _entity5_ MADCOW _/entity5_ in monitoring the _entity6_ collection _/entity6_ and distribution of 12,000 _entity7_ utterances _/entity7_ of _entity8_ spontaneous speech _/entity8_ from five sites for use in a _entity9_ multi-site common evaluation of speech , natural language and spoken language _/entity9_ .	NONE entity1 entity3
This paper defines a _entity1_ generative probabilistic model _/entity1_ of _entity2_ parse trees _/entity2_ , which we call _entity3_ PCFG-LA _/entity3_ . This _entity4_ model _/entity4_ is an extension of _entity5_ PCFG _/entity5_ in which _entity6_ non-terminal symbols _/entity6_ are augmented with _entity7_ latent variables _/entity7_ . Finegrained _entity8_ CFG rules _/entity8_ are automatically induced from a _entity9_ parsed corpus _/entity9_ by _entity10_ training _/entity10_ a _entity11_ PCFG-LA model _/entity11_ using an _entity12_ EM-algorithm _/entity12_ . Because exact _entity13_ parsing _/entity13_ with a _entity14_ PCFG-LA _/entity14_ is _entity15_ NP-hard _/entity15_ , several _entity16_ approximations _/entity16_ are described and empirically compared . In experiments using the _entity17_ Penn WSJ corpus _/entity17_ , our automatically trained _entity18_ model _/entity18_ gave a _entity19_ performance _/entity19_ of 86.6 % ( F1 , _entity20_ sentences _/entity20_ _ 40 _entity21_ _P_ words _/entity21_ ) , which is comparable to that of an _entity22_ _C_ unlexicalized PCFG parser _/entity22_ created using extensive _entity23_ manual feature selection _/entity23_ .	NONE entity21 entity22
This paper describes a new , _entity1_ large scale discourse-level annotation _/entity1_ project - the _entity2_ _P_ Penn Discourse TreeBank ( PDTB ) _/entity2_ . We present an approach to annotating a level of _entity3_ discourse structure _/entity3_ that is based on identifying _entity4_ discourse connectives _/entity4_ and their _entity5_ _C_ arguments _/entity5_ . The _entity6_ PDTB _/entity6_ is being built directly on top of the _entity7_ Penn TreeBank _/entity7_ and _entity8_ Propbank _/entity8_ , thus supporting the extraction of useful _entity9_ syntactic and semantic features _/entity9_ and providing a richer substrate for the development and evaluation of _entity10_ practical algorithms _/entity10_ . We provide a detailed preliminary analysis of _entity11_ inter-annotator agreement _/entity11_ - both the _entity12_ level of agreement _/entity12_ and the types of _entity13_ inter-annotator variation _/entity13_ .	NONE entity2 entity5
Despite much recent progress on accurate _entity1_ semantic role labeling _/entity1_ , previous work has largely used _entity2_ independent classifiers _/entity2_ , possibly combined with separate _entity3_ label sequence models _/entity3_ via _entity4_ Viterbi decoding _/entity4_ . This stands in stark contrast to the linguistic observation that a _entity5_ core argument frame _/entity5_ is a joint structure , with strong _entity6_ dependencies _/entity6_ between _entity7_ _C_ arguments _/entity7_ . We show how to build a _entity8_ joint model _/entity8_ of _entity9_ argument frames _/entity9_ , incorporating novel _entity10_ _P_ features _/entity10_ that model these interactions into _entity11_ discriminative log-linear models _/entity11_ . This system achieves an _entity12_ error reduction _/entity12_ of 22 % on all _entity13_ arguments _/entity13_ and 32 % on _entity14_ core arguments _/entity14_ over a state-of-the art independent _entity15_ classifier _/entity15_ for _entity16_ gold-standard parse trees _/entity16_ on _entity17_ PropBank _/entity17_ .	NONE entity10 entity7
This paper proposes a generic _entity1_ mathematical formalism _/entity1_ for the combination of various _entity2_ structures _/entity2_ : _entity3_ strings _/entity3_ , _entity4_ trees _/entity4_ , _entity5_ dags _/entity5_ , _entity6_ _P_ graphs _/entity6_ , and products of them . The _entity7_ polarization _/entity7_ of the objects of the _entity8_ _C_ elementary structures _/entity8_ controls the _entity9_ saturation _/entity9_ of the final _entity10_ structure _/entity10_ . This formalism is both elementary and powerful enough to strongly simulate many _entity11_ grammar formalisms _/entity11_ , such as _entity12_ rewriting systems _/entity12_ , _entity13_ dependency grammars _/entity13_ , _entity14_ TAG _/entity14_ , _entity15_ HPSG _/entity15_ and _entity16_ LFG _/entity16_ .	NONE entity6 entity8
It is challenging to translate _entity1_ _P_ names _/entity1_ and _entity2_ technical terms _/entity2_ across _entity3_ languages _/entity3_ with different _entity4_ _C_ alphabets _/entity4_ and _entity5_ sound inventories _/entity5_ . These items are commonly transliterated , i.e. , replaced with approximate _entity6_ phonetic equivalents _/entity6_ . For example , computer in _entity7_ English _/entity7_ comes out as ~ i/l : : : '= -- ~ -- ( konpyuutaa ) in _entity8_ Japanese _/entity8_ . Translating such items from _entity9_ Japanese _/entity9_ back to _entity10_ English _/entity10_ is even more challenging , and of practical interest , as transliterated items make up the bulk of _entity11_ text phrases _/entity11_ not found in _entity12_ bilingual dictionaries _/entity12_ . We describe and evaluate a method for performing _entity13_ backwards transliterations _/entity13_ by _entity14_ machine _/entity14_ . This method uses a _entity15_ generative model _/entity15_ , incorporating several distinct stages in the _entity16_ transliteration process _/entity16_ .	NONE entity1 entity4
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ _C_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ _P_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity5 entity3
_entity1_ Sentiment Classification _/entity1_ seeks to identify a piece of _entity2_ text _/entity2_ according to its author 's general feeling toward their _entity3_ subject _/entity3_ , be it positive or negative . Traditional _entity4_ machine learning techniques _/entity4_ have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the _entity5_ _C_ training and test data _/entity5_ with respect to _entity6_ topic _/entity6_ . This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with _entity7_ training data _/entity7_ labeled with _entity8_ _P_ emoticons _/entity8_ , which has the potential of being independent of _entity9_ domain _/entity9_ , _entity10_ topic _/entity10_ and time .	NONE entity8 entity5
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ _C_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ _P_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity9 entity8
We describe a new method for the representation of _entity1_ NLP structures _/entity1_ within _entity2_ reranking approaches _/entity2_ . We make use of a _entity3_ conditional log-linear model _/entity3_ , with _entity4_ hidden variables _/entity4_ representing the _entity5_ assignment _/entity5_ of _entity6_ lexical items _/entity6_ to _entity7_ word clusters _/entity7_ or _entity8_ word senses _/entity8_ . The model learns to automatically make these _entity9_ assignments _/entity9_ based on a _entity10_ discriminative training criterion _/entity10_ . _entity11_ Training _/entity11_ and _entity12_ decoding _/entity12_ with the model requires summing over an exponential number of _entity13_ hidden-variable assignments _/entity13_ : the required summations can be computed efficiently and exactly using _entity14_ dynamic programming _/entity14_ . As a case study , we apply the model to _entity15_ parse reranking _/entity15_ . The model gives an _entity16_ F-measure improvement _/entity16_ of ~1.25 % beyond the _entity17_ base parser _/entity17_ , and an ~0.25 % improvement beyond _entity18_ _C_ Collins ( 2000 ) reranker _/entity18_ . Although our experiments are focused on _entity19_ parsing _/entity19_ , the techniques described generalize naturally to _entity20_ NLP structures _/entity20_ other than _entity21_ _P_ parse trees _/entity21_ .	NONE entity21 entity18
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ _C_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ _P_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity17 entity15
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ _P_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ _C_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity18 entity19
This paper presents a new approach to _entity1_ statistical sentence generation _/entity1_ in which alternative _entity2_ _P_ phrases _/entity2_ are represented as packed sets of _entity3_ _C_ trees _/entity3_ , or _entity4_ forests _/entity4_ , and then ranked statistically to choose the best one . This representation offers advantages in compactness and in the ability to represent _entity5_ syntactic information _/entity5_ . It also facilitates more efficient _entity6_ statistical ranking _/entity6_ than a previous approach to _entity7_ statistical generation _/entity7_ . An efficient _entity8_ ranking algorithm _/entity8_ is described , together with experimental results showing significant improvements over simple enumeration or a _entity9_ lattice-based approach _/entity9_ .	MODEL-FEATURE entity2 entity3
In order to boost the _entity1_ _P_ translation quality _/entity1_ of _entity2_ _C_ EBMT _/entity2_ based on a small-sized _entity3_ bilingual corpus _/entity3_ , we use an out-of-domain _entity4_ bilingual corpus _/entity4_ and , in addition , the _entity5_ language model _/entity5_ of an in-domain _entity6_ monolingual corpus _/entity6_ . We conducted experiments with an _entity7_ EBMT system _/entity7_ . The two _entity8_ evaluation measures _/entity8_ of the _entity9_ BLEU score _/entity9_ and the _entity10_ NIST score _/entity10_ demonstrated the effect of using an out-of-domain _entity11_ bilingual corpus _/entity11_ and the possibility of using the _entity12_ language model _/entity12_ .	NONE entity1 entity2
Motivated by the success of _entity1_ ensemble methods _/entity1_ in _entity2_ machine learning _/entity2_ and other areas of _entity3_ natural language processing _/entity3_ , we developed a _entity4_ multi-strategy and multi-source approach to question answering _/entity4_ which is based on combining the results from different _entity5_ _C_ answering agents _/entity5_ searching for _entity6_ answers _/entity6_ in multiple _entity7_ _P_ corpora _/entity7_ . The _entity8_ answering agents _/entity8_ adopt fundamentally different strategies , one utilizing primarily _entity9_ knowledge-based mechanisms _/entity9_ and the other adopting _entity10_ statistical techniques _/entity10_ . We present our _entity11_ multi-level answer resolution algorithm _/entity11_ that combines results from the _entity12_ answering agents _/entity12_ at the _entity13_ question , passage , and/or answer levels _/entity13_ . Experiments evaluating the effectiveness of our _entity14_ answer resolution algorithm _/entity14_ show a 35.0 % relative improvement over our _entity15_ baseline system _/entity15_ in the number of _entity16_ questions correctly answered _/entity16_ , and a 32.8 % improvement according to the _entity17_ average precision metric _/entity17_ .	NONE entity7 entity5
This paper presents a _entity1_ machine learning approach _/entity1_ to _entity2_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ corpus-based sample _/entity5_ and formulate them as _entity6_ _P_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ clauses _/entity7_ to create a set of _entity8_ domain independent features _/entity8_ to annotate an _entity9_ _C_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity6 entity9
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ _C_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ _P_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity44 entity41
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ _P_ patterns _/entity4_ derived from the _entity5_ _C_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity4 entity5
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ _C_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ _P_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity10 entity8
This report describes _entity1_ Paul _/entity1_ , a _entity2_ computer text generation system _/entity2_ designed to create _entity3_ cohesive text _/entity3_ through the use of _entity4_ lexical substitutions _/entity4_ . Specifically , this system is designed to deterministically choose between _entity5_ pronominalization _/entity5_ , _entity6_ superordinate substitution _/entity6_ , and definite _entity7_ noun phrase reiteration _/entity7_ . The system identifies a strength of _entity8_ antecedence recovery _/entity8_ for each of the _entity9_ lexical substitutions _/entity9_ , and matches them against the _entity10_ _P_ strength of potential antecedence _/entity10_ of each element in the _entity11_ _C_ text _/entity11_ to select the proper _entity12_ substitutions _/entity12_ for these elements .	NONE entity10 entity11
Robust _entity1_ natural language interpretation _/entity1_ requires strong _entity2_ _P_ semantic domain models _/entity2_ , _entity3_ fail-soft recovery heuristics _/entity3_ , and very flexible _entity4_ _C_ control structures _/entity4_ . Although _entity5_ single-strategy parsers _/entity5_ have met with a measure of success , a _entity6_ multi-strategy approach _/entity6_ is shown to provide a much higher degree of flexibility , redundancy , and ability to bring _entity7_ task-specific domain knowledge _/entity7_ ( in addition to _entity8_ general linguistic knowledge _/entity8_ ) to bear on both _entity9_ grammatical and ungrammatical input _/entity9_ . A _entity10_ parsing algorithm _/entity10_ is presented that integrates several different _entity11_ parsing strategies _/entity11_ , with _entity12_ case-frame instantiation _/entity12_ dominating . Each of these _entity13_ parsing strategies _/entity13_ exploits different _entity14_ types of knowledge _/entity14_ ; and their combination provides a strong framework in which to process _entity15_ conjunctions _/entity15_ , _entity16_ fragmentary input _/entity16_ , and _entity17_ ungrammatical structures _/entity17_ , as well as less exotic , _entity18_ grammatically correct input _/entity18_ . Several _entity19_ specific heuristics _/entity19_ for handling _entity20_ ungrammatical input _/entity20_ are presented within this _entity21_ multi-strategy framework _/entity21_ .	NONE entity2 entity4
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ _C_ word _/entity22_ _entity23_ _P_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity23 entity22
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ _P_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ _C_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity4 entity6
_entity1_ GLOSSER _/entity1_ is designed to support reading and learning to read in a foreign _entity2_ language _/entity2_ . There are four _entity3_ language pairs _/entity3_ currently supported by _entity4_ _P_ GLOSSER _/entity4_ : _entity5_ English-Bulgarian _/entity5_ , _entity6_ _C_ English-Estonian _/entity6_ , _entity7_ English-Hungarian _/entity7_ and _entity8_ French-Dutch _/entity8_ . The program is operational on UNIX and Windows '95 platforms , and has undergone a pilot user-study . A demonstration ( in UNIX ) for _entity9_ Applied Natural Language Processing _/entity9_ emphasizes components put to novel technical uses in _entity10_ intelligent computer-assisted morphological analysis ( ICALL ) _/entity10_ , including _entity11_ disambiguated morphological analysis _/entity11_ and _entity12_ lemmatized indexing _/entity12_ for an _entity13_ aligned bilingual corpus _/entity13_ of _entity14_ word examples _/entity14_ .	NONE entity4 entity6
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ _P_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ classification-based representation techniques _/entity7_ can be applied to _entity8_ _C_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ semantic interpretation _/entity13_ was first explored in the _entity14_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	NONE entity5 entity8
In this paper , we compare the performance of a state-of-the-art _entity1_ statistical parser _/entity1_ ( Bikel , 2004 ) in parsing _entity2_ written and spoken language _/entity2_ and in generating _entity3_ sub-categorization cues _/entity3_ from _entity4_ written and spoken language _/entity4_ . Although _entity5_ Bikel 's parser _/entity5_ achieves a higher _entity6_ accuracy _/entity6_ for parsing _entity7_ written language _/entity7_ , it achieves a higher _entity8_ accuracy _/entity8_ when extracting _entity9_ subcategorization cues _/entity9_ from _entity10_ spoken language _/entity10_ . Our experiments also show that current technology for _entity11_ _C_ extracting subcategorization frames _/entity11_ initially designed for _entity12_ _P_ written texts _/entity12_ works equally well for _entity13_ spoken language _/entity13_ . Additionally , we explore the utility of _entity14_ punctuation _/entity14_ in helping _entity15_ parsing _/entity15_ and _entity16_ extraction _/entity16_ of _entity17_ subcategorization cues _/entity17_ . Our experiments show that _entity18_ punctuation _/entity18_ is of little help in parsing _entity19_ spoken language _/entity19_ and extracting _entity20_ subcategorization cues _/entity20_ from _entity21_ spoken language _/entity21_ . This indicates that there is no need to add _entity22_ punctuation _/entity22_ in transcribing _entity23_ spoken corpora _/entity23_ simply in order to help _entity24_ parsers _/entity24_ .	NONE entity12 entity11
This paper discusses two problems that arise in the _entity1_ Generation _/entity1_ of _entity2_ Referring Expressions _/entity2_ : ( a ) _entity3_ numeric-valued attributes _/entity3_ , such as size or location ; ( b ) _entity4_ perspective-taking _/entity4_ in _entity5_ _P_ reference _/entity5_ . Both problems , it is argued , can be resolved if some structure is imposed on the available knowledge prior to _entity6_ content determination _/entity6_ . We describe a _entity7_ _C_ clustering algorithm _/entity7_ which is sufficiently general to be applied to these diverse problems , discuss its application , and evaluate its performance .	NONE entity5 entity7
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ grammar coverage problems _/entity3_ we use a _entity4_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ board _/entity6_ with a single _entity7_ _P_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ _C_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity7 entity9
_entity1_ CriterionSM Online Essay Evaluation Service _/entity1_ includes a capability that labels _entity2_ sentences _/entity2_ in student _entity3_ writing _/entity3_ with _entity4_ essay-based discourse elements _/entity4_ ( e.g. , _entity5_ thesis statements _/entity5_ ) . We describe a new system that enhances _entity6_ Criterion _/entity6_ 's capability , by evaluating multiple aspects of _entity7_ coherence _/entity7_ in _entity8_ essays _/entity8_ . This system identifies _entity9_ features _/entity9_ of _entity10_ sentences _/entity10_ based on _entity11_ semantic similarity measures _/entity11_ and _entity12_ discourse structure _/entity12_ . A _entity13_ _P_ support vector machine _/entity13_ uses these _entity14_ features _/entity14_ to capture _entity15_ breakdowns in coherence _/entity15_ due to relatedness to the _entity16_ _C_ essay question _/entity16_ and relatedness between _entity17_ discourse elements _/entity17_ . _entity18_ Intra-sentential quality _/entity18_ is evaluated with _entity19_ rule-based heuristics _/entity19_ . Results indicate that the system yields higher performance than a _entity20_ baseline _/entity20_ on all three aspects .	NONE entity13 entity16
_entity1_ Topical blog post retrieval _/entity1_ is the task of ranking _entity2_ blog posts _/entity2_ with respect to their _entity3_ relevance _/entity3_ for a given _entity4_ topic _/entity4_ . To improve _entity5_ topical blog post retrieval _/entity5_ we incorporate _entity6_ textual credibility indicators _/entity6_ in the _entity7_ retrieval process _/entity7_ . We consider two groups of _entity8_ _C_ indicators _/entity8_ : post level ( determined using information about individual _entity9_ blog posts _/entity9_ only ) and blog level ( determined using information from the underlying _entity10_ blogs _/entity10_ ) . We describe how to estimate these _entity11_ _P_ indicators _/entity11_ and how to integrate them into a _entity12_ retrieval approach _/entity12_ based on _entity13_ language models _/entity13_ . Experiments on the _entity14_ TREC Blog track test set _/entity14_ show that both groups of _entity15_ credibility indicators _/entity15_ significantly improve _entity16_ retrieval effectiveness _/entity16_ ; the best performance is achieved when combining them .	NONE entity11 entity8
_entity1_ Listen-Communicate-Show ( LCS ) _/entity1_ is a new paradigm for _entity2_ _P_ human interaction with data sources _/entity2_ . We integrate a _entity3_ _C_ spoken language understanding system _/entity3_ with _entity4_ intelligent mobile agents _/entity4_ that mediate between _entity5_ users _/entity5_ and _entity6_ information sources _/entity6_ . We have built and will demonstrate an application of this approach called _entity7_ LCS-Marine _/entity7_ . Using _entity8_ LCS-Marine _/entity8_ , tactical personnel can converse with their logistics system to place a supply or information request . The request is passed to a _entity9_ mobile , intelligent agent _/entity9_ for execution at the appropriate _entity10_ database _/entity10_ . _entity11_ Requestors _/entity11_ can also instruct the system to notify them when the status of a _entity12_ request _/entity12_ changes or when a _entity13_ request _/entity13_ is complete . We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in _entity14_ new domains _/entity14_ .	NONE entity2 entity3
We present an efficient algorithm for _entity1_ chart-based phrase structure parsing _/entity1_ of _entity2_ natural language _/entity2_ that is tailored to the problem of extracting specific information from _entity3_ unrestricted texts _/entity3_ where many of the _entity4_ words _/entity4_ are unknown and much of the _entity5_ text _/entity5_ is irrelevant to the task . The _entity6_ parser _/entity6_ gains algorithmic efficiency through a _entity7_ reduction _/entity7_ of its _entity8_ search space _/entity8_ . As each new _entity9_ edge _/entity9_ is added to the _entity10_ chart _/entity10_ , the algorithm checks only the topmost of the _entity11_ edges _/entity11_ adjacent to it , rather than all such _entity12_ edges _/entity12_ as in conventional treatments . The resulting _entity13_ spanning edges _/entity13_ are insured to be the correct ones by carefully controlling the order in which _entity14_ edges _/entity14_ are introduced so that every final _entity15_ constituent _/entity15_ covers the longest possible _entity16_ span _/entity16_ . This is facilitated through the use of _entity17_ phrase boundary heuristics _/entity17_ based on the placement of _entity18_ function words _/entity18_ , and by _entity19_ _P_ heuristic rules _/entity19_ that permit certain kinds of _entity20_ _C_ phrases _/entity20_ to be deduced despite the presence of _entity21_ unknown words _/entity21_ . A further _entity22_ reduction in the search space _/entity22_ is achieved by using _entity23_ semantic _/entity23_ rather than _entity24_ syntactic categories _/entity24_ on the _entity25_ terminal and non-terminal edges _/entity25_ , thereby reducing the amount of _entity26_ ambiguity _/entity26_ and thus the number of _entity27_ edges _/entity27_ , since only _entity28_ edges _/entity28_ with a valid _entity29_ semantic _/entity29_ interpretation are ever introduced .	NONE entity19 entity20
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ _P_ human language learners _/entity2_ , to the _entity3_ _C_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity2 entity3
This paper introduces a method for _entity1_ computational analysis of move structures _/entity1_ in _entity2_ abstracts _/entity2_ of _entity3_ research articles _/entity3_ . In our approach , _entity4_ sentences _/entity4_ in a given _entity5_ abstract _/entity5_ are analyzed and labeled with a specific _entity6_ move _/entity6_ in light of various _entity7_ rhetorical functions _/entity7_ . The method involves automatically gathering a large number of _entity8_ abstracts _/entity8_ from the _entity9_ Web _/entity9_ and building a _entity10_ language model _/entity10_ of _entity11_ _C_ abstract moves _/entity11_ . We also present a prototype _entity12_ concordancer _/entity12_ , _entity13_ CARE _/entity13_ , which exploits the _entity14_ _P_ move-tagged abstracts _/entity14_ for _entity15_ digital learning _/entity15_ . This system provides a promising approach to _entity16_ Web-based computer-assisted academic writing _/entity16_ .	NONE entity14 entity11
This paper presents a _entity1_ maximum entropy word alignment algorithm _/entity1_ for _entity2_ Arabic-English _/entity2_ based on _entity3_ supervised training data _/entity3_ . We demonstrate that it is feasible to create _entity4_ training material _/entity4_ for problems in _entity5_ machine translation _/entity5_ and that a mixture of _entity6_ supervised and unsupervised methods _/entity6_ yields superior _entity7_ _P_ performance _/entity7_ . The _entity8_ probabilistic model _/entity8_ used in the _entity9_ alignment _/entity9_ directly models the _entity10_ _C_ link decisions _/entity10_ . Significant improvement over traditional _entity11_ word alignment techniques _/entity11_ is shown as well as improvement on several _entity12_ machine translation tests _/entity12_ . Performance of the algorithm is contrasted with _entity13_ human annotation performance _/entity13_ .	NONE entity7 entity10
An efficient _entity1_ bit-vector-based CKY-style parser _/entity1_ for _entity2_ context-free parsing _/entity2_ is presented . The _entity3_ parser _/entity3_ computes a compact _entity4_ parse forest representation _/entity4_ of the complete set of possible _entity5_ analyses for large treebank grammars _/entity5_ and long _entity6_ input sentences _/entity6_ . The _entity7_ _C_ parser _/entity7_ uses _entity8_ _P_ bit-vector operations _/entity8_ to parallelise the _entity9_ basic parsing operations _/entity9_ . The _entity10_ parser _/entity10_ is particularly useful when all analyses are needed rather than just the most probable one .	USAGE entity8 entity7
_entity1_ Coedition _/entity1_ of a _entity2_ natural language text _/entity2_ and its representation in some _entity3_ interlingual form _/entity3_ seems the best and simplest way to share _entity4_ text revision _/entity4_ across _entity5_ languages _/entity5_ . For various reasons , _entity6_ UNL graphs _/entity6_ are the best candidates in this context . We are developing a _entity7_ _C_ prototype _/entity7_ where , in the simplest _entity8_ sharing scenario _/entity8_ , naive users interact directly with the _entity9_ _P_ text _/entity9_ in their _entity10_ language ( L0 ) _/entity10_ , and indirectly with the associated _entity11_ graph _/entity11_ . The modified _entity12_ graph _/entity12_ is then sent to the _entity13_ UNL-L0 deconverter _/entity13_ and the result shown . If is is satisfactory , the errors were probably due to the _entity14_ graph _/entity14_ , not to the _entity15_ deconverter _/entity15_ , and the _entity16_ graph _/entity16_ is sent to _entity17_ deconverters _/entity17_ in other _entity18_ languages _/entity18_ . Versions in some other _entity19_ languages _/entity19_ known by the user may be displayed , so that improvement sharing is visible and encouraging . As new versions are added with appropriate _entity20_ tags _/entity20_ and _entity21_ attributes _/entity21_ in the _entity22_ original multilingual document _/entity22_ , nothing is ever lost , and cooperative working on a _entity23_ document _/entity23_ is rendered feasible . On the internal side , liaisons are established between elements of the _entity24_ text _/entity24_ and the _entity25_ graph _/entity25_ by using broadly available resources such as a _entity26_ LO-English or better a L0-UNL dictionary _/entity26_ , a _entity27_ morphosyntactic parser of L0 _/entity27_ , and a _entity28_ canonical graph2tree transformation _/entity28_ . Establishing a `` best '' correspondence between the `` _entity29_ UNL-tree+L0 _/entity29_ `` and the `` _entity30_ MS-L0 structure _/entity30_ `` , a _entity31_ lattice _/entity31_ , may be done using the _entity32_ dictionary _/entity32_ and trying to align the _entity33_ tree _/entity33_ and the selected _entity34_ trajectory _/entity34_ with as few _entity35_ crossing liaisons _/entity35_ as possible . A central goal of this research is to merge approaches from _entity36_ pivot MT _/entity36_ , _entity37_ interactive MT _/entity37_ , and _entity38_ multilingual text authoring _/entity38_ .	NONE entity9 entity7
This paper discusses a _entity1_ _C_ decision-tree approach _/entity1_ to the problem of assigning _entity2_ probabilities _/entity2_ to _entity3_ _P_ words _/entity3_ following a given _entity4_ text _/entity4_ . In contrast with previous _entity5_ decision-tree language model attempts _/entity5_ , an algorithm for selecting _entity6_ nearly optimal questions _/entity6_ is considered . The model is to be tested on a standard task , _entity7_ The Wall Street Journal _/entity7_ , allowing a fair comparison with the well-known _entity8_ tri-gram model _/entity8_ .	NONE entity3 entity1
We give an analysis of _entity1_ ellipsis resolution _/entity1_ in terms of a straightforward _entity2_ discourse copying algorithm _/entity2_ that correctly predicts a wide range of phenomena . The treatment does not suffer from problems inherent in _entity3_ identity-of-relations analyses _/entity3_ . Furthermore , in contrast to the approach of Dalrymple et al . [ 1991 ] , the treatment directly encodes the intuitive distinction between _entity4_ full NPs _/entity4_ and the _entity5_ _P_ referential elements _/entity5_ that corefer with them through what we term _entity6_ role linking _/entity6_ . The correct _entity7_ _C_ predictions _/entity7_ for several problematic examples of _entity8_ ellipsis _/entity8_ naturally result . Finally , the analysis extends directly to other _entity9_ discourse copying phenomena _/entity9_ .	NONE entity5 entity7
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ _C_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ _P_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity8 entity6
A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature , practical applications rather than `` toy '' implementations . Evaluation is also crucial to assessing competing claims and identifying promising technical approaches . While work in _entity1_ speech recognition ( SR ) _/entity1_ has a history of evaluation methodologies that permit comparison among various systems , until recently no methodology existed for either developers of _entity2_ natural language ( NL ) interfaces _/entity2_ or researchers in _entity3_ speech understanding ( SU ) _/entity3_ to evaluate and compare the systems they developed . Recently considerable progress has been made by a number of groups involved in the _entity4_ DARPA Spoken Language Systems ( SLS ) program _/entity4_ to agree on a methodology for comparative evaluation of _entity5_ SLS systems _/entity5_ , and that methodology has been put into practice several times in comparative tests of several _entity6_ SLS systems _/entity6_ . These evaluations are probably the only _entity7_ NL evaluations _/entity7_ other than the series of _entity8_ _C_ Message Understanding Conferences _/entity8_ ( Sundheim , 1989 ; Sundheim , 1991 ) to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems ( Palmer et al. , 1989 ; Neal et al. , 1991 ) . This paper describes a practical _entity9_ _P_ `` black-box '' methodology _/entity9_ for automatic evaluation of _entity10_ question-answering NL systems _/entity10_ . While each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and it can be used with either _entity11_ speech or text input _/entity11_ . The particular characteristics of the approach are described in the following section : subsequent sections present its implementation in the _entity12_ DARPA SLS community _/entity12_ , and some problems and directions for future development .	NONE entity9 entity8
Sources of _entity1_ training data _/entity1_ suitable for _entity2_ _C_ language modeling _/entity2_ of _entity3_ _P_ conversational speech _/entity3_ are limited . In this paper , we show how _entity4_ training data _/entity4_ can be supplemented with _entity5_ text _/entity5_ from the _entity6_ web _/entity6_ filtered to match the _entity7_ style _/entity7_ and/or _entity8_ topic _/entity8_ of the target _entity9_ recognition task _/entity9_ , but also that it is possible to get bigger performance gains from the _entity10_ data _/entity10_ by using _entity11_ class-dependent interpolation _/entity11_ of _entity12_ N-grams _/entity12_ .	NONE entity3 entity2
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ _C_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ _P_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity22 entity21
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ _C_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ _P_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity18 entity15
In this paper we present a novel , customizable : _entity1_ IE paradigm _/entity1_ that takes advantage of _entity2_ predicate-argument structures _/entity2_ . We also introduce a new way of automatically identifying _entity3_ predicate argument structures _/entity3_ , which is central to our _entity4_ _P_ IE paradigm _/entity4_ . It is based on : ( 1 ) an extended set of _entity5_ features _/entity5_ ; and ( 2 ) _entity6_ inductive decision tree learning _/entity6_ . The experimental results prove our claim that accurate _entity7_ _C_ predicate-argument structures _/entity7_ enable high quality _entity8_ IE _/entity8_ results .	NONE entity4 entity7
_entity1_ Automatic estimation _/entity1_ of _entity2_ word significance _/entity2_ oriented for _entity3_ speech-based Information Retrieval ( IR ) _/entity3_ is addressed . Since the _entity4_ significance _/entity4_ of _entity5_ words _/entity5_ differs in _entity6_ IR _/entity6_ , _entity7_ _P_ automatic speech recognition ( ASR ) performance _/entity7_ has been evaluated based on _entity8_ weighted word error rate ( WWER ) _/entity8_ , which gives a _entity9_ _C_ weight _/entity9_ on errors from the viewpoint of _entity10_ IR _/entity10_ , instead of _entity11_ word error rate ( WER ) _/entity11_ , which treats all _entity12_ words _/entity12_ uniformly . A _entity13_ decoding strategy _/entity13_ that minimizes _entity14_ WWER _/entity14_ based on a _entity15_ Minimum Bayes-Risk framework _/entity15_ has been shown , and the reduction of errors on both _entity16_ ASR _/entity16_ and _entity17_ IR _/entity17_ has been reported . In this paper , we propose an _entity18_ automatic estimation method _/entity18_ for _entity19_ word significance ( weights ) _/entity19_ based on its influence on _entity20_ IR _/entity20_ . Specifically , _entity21_ weights _/entity21_ are estimated so that _entity22_ evaluation measures _/entity22_ of _entity23_ ASR _/entity23_ and _entity24_ IR _/entity24_ are equivalent . We apply the proposed method to a _entity25_ speech-based information retrieval system _/entity25_ , which is a typical _entity26_ IR system _/entity26_ , and show that the method works well .	NONE entity7 entity9
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ _P_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ _C_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity14 entity15
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ temporal expressions _/entity2_ in _entity3_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ temporal expressions _/entity4_ in a novel _entity5_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ expressions _/entity6_ fits well with our _entity7_ constraint-based representation _/entity7_ of time , _entity8_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ _P_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ _C_ baseline _/entity10_ , and compares favorably with some of the closely related work .	COMPARE entity9 entity10
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ machine translation _/entity2_ assign high _entity3_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ _P_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ _C_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity11 entity14
Recently , we initiated a project to develop a _entity1_ phonetically-based spoken language understanding system _/entity1_ called _entity2_ SUMMIT _/entity2_ . In contrast to many of the past efforts that make use of _entity3_ _C_ heuristic rules _/entity3_ whose development requires intense _entity4_ knowledge engineering _/entity4_ , our approach attempts to express the _entity5_ _P_ speech knowledge _/entity5_ within a formal framework using well-defined mathematical tools . In our system , _entity6_ features _/entity6_ and _entity7_ decision strategies _/entity7_ are discovered and trained automatically , using a large body of _entity8_ speech data _/entity8_ . This paper describes the system , and documents its current performance .	NONE entity5 entity3
This paper presents a _entity1_ _C_ machine learning approach _/entity1_ to _entity2_ _P_ bare slice disambiguation _/entity2_ in _entity3_ dialogue _/entity3_ . We extract a set of _entity4_ heuristic principles _/entity4_ from a _entity5_ corpus-based sample _/entity5_ and formulate them as _entity6_ probabilistic Horn clauses _/entity6_ . We then use the predicates of such _entity7_ clauses _/entity7_ to create a set of _entity8_ domain independent features _/entity8_ to annotate an _entity9_ input dataset _/entity9_ , and run two different _entity10_ machine learning algorithms _/entity10_ : SLIPPER , a _entity11_ rule-based learning algorithm _/entity11_ , and TiMBL , a _entity12_ memory-based system _/entity12_ . Both learners perform well , yielding similar _entity13_ success rates _/entity13_ of approx 90 % . The results show that the _entity14_ features _/entity14_ in terms of which we formulate our _entity15_ heuristic principles _/entity15_ have significant predictive power , and that _entity16_ rules _/entity16_ that closely resemble our _entity17_ Horn clauses _/entity17_ can be learnt automatically from these _entity18_ features _/entity18_ .	NONE entity2 entity1
This paper describes a domain independent strategy for the _entity1_ multimedia articulation of answers _/entity1_ elicited by a _entity2_ natural language interface _/entity2_ to _entity3_ database query applications _/entity3_ . _entity4_ Multimedia answers _/entity4_ include _entity5_ videodisc images _/entity5_ and heuristically-produced complete _entity6_ _C_ sentences _/entity6_ in _entity7_ text _/entity7_ or _entity8_ text-to-speech form _/entity8_ . _entity9_ _P_ Deictic reference _/entity9_ and _entity10_ feedback _/entity10_ about the _entity11_ discourse _/entity11_ are enabled . The _entity12_ interface _/entity12_ thus presents the application as cooperative and conversational .	NONE entity9 entity6
A major axis of research at LIMSI is directed at _entity1_ multilingual , speaker-independent , large vocabulary speech dictation _/entity1_ . In this paper the _entity2_ LIMSI recognizer _/entity2_ which was evaluated in the _entity3_ ARPA NOV93 CSR test _/entity3_ is described , and experimental results on the _entity4_ WSJ and BREF corpora _/entity4_ under closely matched conditions are reported . For both _entity5_ corpora _/entity5_ _entity6_ word recognition experiments _/entity6_ were carried out with _entity7_ vocabularies _/entity7_ containing up to 20k _entity8_ words _/entity8_ . The recognizer makes use of _entity9_ continuous density HMM _/entity9_ with _entity10_ _P_ Gaussian mixture _/entity10_ for _entity11_ acoustic modeling _/entity11_ and _entity12_ _C_ n-gram statistics _/entity12_ estimated on the _entity13_ newspaper texts _/entity13_ for _entity14_ language modeling _/entity14_ . The recognizer uses a _entity15_ time-synchronous graph-search strategy _/entity15_ which is shown to still be viable with a 20k-word vocabulary when used with _entity16_ bigram back-off language models _/entity16_ . A second _entity17_ forward pass _/entity17_ , which makes use of a _entity18_ word graph _/entity18_ generated with the _entity19_ bigram _/entity19_ , incorporates a _entity20_ trigram language model _/entity20_ . _entity21_ Acoustic modeling _/entity21_ uses _entity22_ cepstrum-based features _/entity22_ , _entity23_ context-dependent phone models ( intra and interword ) _/entity23_ , _entity24_ phone duration models _/entity24_ , and _entity25_ sex-dependent models _/entity25_ .	NONE entity10 entity12
This paper presents a novel _entity1_ _P_ ensemble learning approach _/entity1_ to resolving _entity2_ German pronouns _/entity2_ . _entity3_ _C_ Boosting _/entity3_ , the method in question , combines the moderately accurate _entity4_ hypotheses _/entity4_ of several _entity5_ classifiers _/entity5_ to form a highly accurate one . Experiments show that this approach is superior to a single _entity6_ decision-tree classifier _/entity6_ . Furthermore , we present a _entity7_ standalone system _/entity7_ that resolves _entity8_ pronouns _/entity8_ in _entity9_ unannotated text _/entity9_ by using a fully automatic sequence of _entity10_ preprocessing modules _/entity10_ that mimics the _entity11_ manual annotation process _/entity11_ . Although the system performs well within a limited _entity12_ textual domain _/entity12_ , further research is needed to make it effective for _entity13_ open-domain question answering _/entity13_ and _entity14_ text summarisation _/entity14_ .	NONE entity1 entity3
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ _C_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ _P_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ electronic discussions _/entity5_ that influence the _entity6_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity3 entity2
This paper presents an approach to the _entity1_ unsupervised learning _/entity1_ of _entity2_ parts of speech _/entity2_ which uses both _entity3_ _C_ morphological and syntactic information _/entity3_ . While the _entity4_ model _/entity4_ is more complex than those which have been employed for _entity5_ unsupervised learning _/entity5_ of _entity6_ _P_ POS tags in English _/entity6_ , which use only _entity7_ syntactic information _/entity7_ , the variety of _entity8_ languages _/entity8_ in the world requires that we consider _entity9_ morphology _/entity9_ as well . In many _entity10_ languages _/entity10_ , _entity11_ morphology _/entity11_ provides better clues to a word 's category than _entity12_ word order _/entity12_ . We present the _entity13_ computational model _/entity13_ for _entity14_ POS learning _/entity14_ , and present results for applying it to _entity15_ Bulgarian _/entity15_ , a _entity16_ Slavic language _/entity16_ with relatively _entity17_ free word order _/entity17_ and _entity18_ rich morphology _/entity18_ .	NONE entity6 entity3
Automatic _entity1_ _C_ evaluation metrics _/entity1_ for _entity2_ _P_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity2 entity1
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ _C_ high-quality data _/entity7_ that allow the comparison of both _entity8_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ _P_ different-quality references _/entity9_ on _entity10_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ automatic metrics _/entity11_ used within _entity12_ MT _/entity12_ are also discussed in this regard .	NONE entity9 entity7
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ _P_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ _C_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity13 entity14
This paper presents the results of a study on the _entity1_ semantic constraints _/entity1_ imposed on _entity2_ lexical choice _/entity2_ by certain _entity3_ contextual indicators _/entity3_ . We show how such _entity4_ indicators _/entity4_ are computed and how _entity5_ correlations _/entity5_ between them and the _entity6_ choice of a noun phrase description _/entity6_ of a _entity7_ named entity _/entity7_ can be automatically established using _entity8_ supervised learning _/entity8_ . Based on this _entity9_ correlation _/entity9_ , we have developed a technique for _entity10_ automatic lexical choice _/entity10_ of _entity11_ descriptions _/entity11_ of _entity12_ entities _/entity12_ in _entity13_ text generation _/entity13_ . We discuss the underlying relationship between the _entity14_ _P_ pragmatics _/entity14_ of choosing an appropriate _entity15_ description _/entity15_ that serves a specific purpose in the _entity16_ _C_ automatically generated text _/entity16_ and the _entity17_ semantics _/entity17_ of the _entity18_ description _/entity18_ itself . We present our work in the framework of the more general concept of reuse of _entity19_ linguistic structures _/entity19_ that are automatically extracted from _entity20_ large corpora _/entity20_ . We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method .	NONE entity14 entity16
This article is devoted to the problem of _entity1_ quantifying noun groups _/entity1_ in _entity2_ German _/entity2_ . After a thorough description of the phenomena , the results of _entity3_ corpus-based investigations _/entity3_ are described . Moreover , some examples are given that underline the necessity of integrating some kind of information other than _entity4_ grammar sensu stricto _/entity4_ into the _entity5_ treebank _/entity5_ . We argue that a more sophisticated and fine-grained _entity6_ annotation _/entity6_ in the _entity7_ tree-bank _/entity7_ would have very positve effects on _entity8_ stochastic parsers _/entity8_ trained on the _entity9_ tree-bank _/entity9_ and on _entity10_ grammars _/entity10_ induced from the _entity11_ treebank _/entity11_ , and it would make the _entity12_ _C_ treebank _/entity12_ more valuable as a _entity13_ _P_ source of data _/entity13_ for _entity14_ theoretical linguistic investigations _/entity14_ . The information gained from _entity15_ corpus research _/entity15_ and the analyses that are proposed are realized in the framework of _entity16_ SILVA _/entity16_ , a _entity17_ parsing _/entity17_ and _entity18_ extraction tool _/entity18_ for _entity19_ German text corpora _/entity19_ .	NONE entity13 entity12
In this paper we describe a systematic approach for creating a _entity1_ dialog management system _/entity1_ based on a _entity2_ _C_ Construct Algebra _/entity2_ , a _entity3_ _P_ collection of relations and operations _/entity3_ on a _entity4_ task representation _/entity4_ . These _entity5_ relations and operations _/entity5_ are _entity6_ analytical components _/entity6_ for building higher level abstractions called _entity7_ dialog motivators _/entity7_ . The _entity8_ dialog manager _/entity8_ , consisting of a _entity9_ collection of dialog motivators _/entity9_ , is entirely built using the _entity10_ Construct Algebra _/entity10_ .	NONE entity3 entity2
The task of _entity1_ machine translation ( MT ) evaluation _/entity1_ is closely related to the task of _entity2_ sentence-level semantic equivalence classification _/entity2_ . This paper investigates the utility of applying standard _entity3_ MT evaluation methods ( BLEU , NIST , WER and PER ) _/entity3_ to building _entity4_ _P_ classifiers _/entity4_ to predict _entity5_ semantic equivalence _/entity5_ and _entity6_ entailment _/entity6_ . We also introduce a novel _entity7_ _C_ classification method _/entity7_ based on _entity8_ PER _/entity8_ which leverages _entity9_ part of speech information _/entity9_ of the _entity10_ words _/entity10_ contributing to the _entity11_ word matches and non-matches _/entity11_ in the _entity12_ sentence _/entity12_ . Our results show that _entity13_ MT evaluation techniques _/entity13_ are able to produce useful _entity14_ features _/entity14_ for _entity15_ paraphrase classification _/entity15_ and to a lesser extent _entity16_ entailment _/entity16_ . Our _entity17_ technique _/entity17_ gives a substantial improvement in _entity18_ paraphrase classification accuracy _/entity18_ over all of the other _entity19_ models _/entity19_ used in the experiments .	NONE entity4 entity7
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ _C_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ _P_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity4 entity2
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ _C_ translation knowledge _/entity24_ extracted from the _entity25_ _P_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity25 entity24
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ grammar coverage problems _/entity3_ we use a _entity4_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ _P_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ _C_ SUN 4 _/entity8_ for _entity9_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity6 entity8
This study presents a _entity1_ method to automatically acquire paraphrases _/entity1_ using _entity2_ bilingual corpora _/entity2_ , which utilizes the _entity3_ bilingual dependency relations _/entity3_ obtained by projecting a _entity4_ monolingual dependency parse _/entity4_ onto the other language sentence based on _entity5_ statistical alignment techniques _/entity5_ . Since the _entity6_ paraphrasing method _/entity6_ is capable of clearly disambiguating the _entity7_ sense _/entity7_ of an original _entity8_ phrase _/entity8_ using the _entity9_ bilingual context _/entity9_ of _entity10_ dependency relation _/entity10_ , it would be possible to obtain interchangeable _entity11_ paraphrases _/entity11_ under a given _entity12_ context _/entity12_ . Also , we provide an advanced method to acquire _entity13_ generalized translation knowledge _/entity13_ using the extracted _entity14_ _P_ paraphrases _/entity14_ . We applied the method to acquire the _entity15_ generalized translation knowledge _/entity15_ for _entity16_ _C_ Korean-English translation _/entity16_ . Through experiments with _entity17_ parallel corpora _/entity17_ of a _entity18_ Korean and English language pairs _/entity18_ , we show that our _entity19_ paraphrasing method _/entity19_ effectively extracts _entity20_ paraphrases _/entity20_ with high _entity21_ precision _/entity21_ , 94.3 % and 84.6 % respectively for _entity22_ Korean _/entity22_ and _entity23_ English _/entity23_ , and the _entity24_ translation knowledge _/entity24_ extracted from the _entity25_ bilingual corpora _/entity25_ could be generalized successfully using the _entity26_ paraphrases _/entity26_ with the 12.5 % _entity27_ compression ratio _/entity27_ .	NONE entity14 entity16
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ _C_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ _P_ language _/entity33_ of interest .	NONE entity33 entity31
In this paper we explore a new _entity1_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ _P_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ _C_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity16 entity18
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ ACE 2003 corpus _/entity9_ shows that the _entity10_ _P_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ _C_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity10 entity13
The present paper reports on a preparatory research for building a _entity1_ language corpus annotation scenario _/entity1_ capturing the _entity2_ _P_ discourse relations _/entity2_ in _entity3_ Czech _/entity3_ . We primarily focus on the description of the _entity4_ _C_ syntactically motivated relations _/entity4_ in _entity5_ discourse _/entity5_ , basing our findings on the theoretical background of the _entity6_ Prague Dependency Treebank 2.0 _/entity6_ and the _entity7_ Penn Discourse Treebank 2 _/entity7_ . Our aim is to revisit the present-day _entity8_ syntactico-semantic ( tectogrammatical ) annotation _/entity8_ in the _entity9_ Prague Dependency Treebank _/entity9_ , extend it for the purposes of a _entity10_ sentence-boundary-crossing representation _/entity10_ and eventually to design a new , _entity11_ discourse level _/entity11_ of _entity12_ annotation _/entity12_ . In this paper , we propose a feasible process of such a transfer , comparing the possibilities the _entity13_ Praguian dependency-based approach _/entity13_ offers with the _entity14_ Penn discourse annotation _/entity14_ based primarily on the analysis and classification of _entity15_ discourse connectives _/entity15_ .	NONE entity2 entity4
In the second year of _entity1_ evaluations _/entity1_ of the _entity2_ ARPA HLT Machine Translation ( MT ) Initiative _/entity2_ , methodologies developed and tested in 1992 were applied to the _entity3_ 1993 MT test runs _/entity3_ . The current methodology optimizes the inherently _entity4_ subjective judgments _/entity4_ on _entity5_ translation accuracy and quality _/entity5_ by channeling the _entity6_ judgments _/entity6_ of _entity7_ non-translators _/entity7_ into many _entity8_ data points _/entity8_ which reflect both the comparison of the _entity9_ performance _/entity9_ of the _entity10_ research MT systems _/entity10_ with _entity11_ production MT systems _/entity11_ and against the _entity12_ _C_ performance _/entity12_ of _entity13_ _P_ novice translators _/entity13_ . This paper discusses the three _entity14_ evaluation methods _/entity14_ used in the _entity15_ 1993 evaluation _/entity15_ , the results of the evaluations , and preliminary characterizations of the _entity16_ Winter 1994 evaluation _/entity16_ , now underway . The efforts under discussion focus on measuring the progress of _entity17_ core MT technology _/entity17_ and increasing the sensitivity and _entity18_ portability _/entity18_ of _entity19_ MT evaluation methodology _/entity19_ .	NONE entity13 entity12
The applicability of many current _entity1_ information extraction techniques _/entity1_ is severely limited by the need for _entity2_ supervised training data _/entity2_ . We demonstrate that for certain _entity3_ field structured extraction tasks _/entity3_ , such as classified advertisements and bibliographic citations , small amounts of _entity4_ prior knowledge _/entity4_ can be used to learn effective models in a primarily unsupervised fashion . Although _entity5_ hidden Markov models ( HMMs ) _/entity5_ provide a suitable _entity6_ generative model _/entity6_ for _entity7_ field structured text _/entity7_ , general _entity8_ unsupervised HMM learning _/entity8_ fails to learn useful structure in either of our domains . However , one can dramatically improve the quality of the learned structure by exploiting simple _entity9_ _P_ prior knowledge _/entity9_ of the desired solutions . In both domains , we found that _entity10_ unsupervised methods _/entity10_ can attain _entity11_ accuracies _/entity11_ with 400 _entity12_ _C_ unlabeled examples _/entity12_ comparable to those attained by _entity13_ supervised methods _/entity13_ on 50 _entity14_ labeled examples _/entity14_ , and that _entity15_ semi-supervised methods _/entity15_ can make good use of small amounts of _entity16_ labeled data _/entity16_ .	NONE entity9 entity12
Most state-of-the-art _entity1_ evaluation measures _/entity1_ for _entity2_ _P_ machine translation _/entity2_ assign high _entity3_ _C_ costs _/entity3_ to movements of _entity4_ word _/entity4_ blocks . In many cases though such movements still result in correct or almost correct _entity5_ sentences _/entity5_ . In this paper , we will present a new _entity6_ evaluation measure _/entity6_ which explicitly models _entity7_ block reordering _/entity7_ as an _entity8_ edit operation _/entity8_ . Our _entity9_ measure _/entity9_ can be exactly calculated in _entity10_ quadratic time _/entity10_ . Furthermore , we will show how some _entity11_ evaluation measures _/entity11_ can be improved by the introduction of _entity12_ word-dependent substitution costs _/entity12_ . The correlation of the new _entity13_ measure _/entity13_ with _entity14_ human judgment _/entity14_ has been investigated systematically on two different _entity15_ language pairs _/entity15_ . The experimental results will show that it significantly outperforms state-of-the-art approaches in _entity16_ sentence-level correlation _/entity16_ . Results from experiments with _entity17_ word dependent substitution costs _/entity17_ will demonstrate an additional increase of correlation between _entity18_ automatic evaluation measures _/entity18_ and _entity19_ human judgment _/entity19_ .	NONE entity2 entity3
_entity1_ Metagrammatical formalisms _/entity1_ that combine _entity2_ _P_ context-free phrase structure rules _/entity2_ and _entity3_ metarules ( MPS grammars ) _/entity3_ allow concise statement of generalizations about the _entity4_ _C_ syntax _/entity4_ of _entity5_ natural languages _/entity5_ . _entity6_ Unconstrained MPS grammars _/entity6_ , unfortunately , are not computationally safe . We evaluate several proposals for constraining them , basing our assessment on _entity7_ computational tractability and explanatory adequacy _/entity7_ . We show that none of them satisfies both criteria , and suggest new directions for research on alternative _entity8_ metagrammatical formalisms _/entity8_ .	NONE entity2 entity4
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ feature structure-directed generation _/entity2_ , developed for a _entity3_ _C_ dialogue translation system _/entity3_ . The system utilizes _entity4_ typed feature structures _/entity4_ to control the _entity5_ _P_ top-down derivation _/entity5_ in a declarative way . This _entity6_ generation system _/entity6_ also uses _entity7_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ derivation tree _/entity8_ . The _entity9_ grammar _/entity9_ for this _entity10_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	NONE entity5 entity3
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ _P_ parse forest _/entity9_ . For _entity10_ _C_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity9 entity10
Methods developed for _entity1_ spelling correction _/entity1_ for _entity2_ _C_ languages _/entity2_ like _entity3_ _P_ English _/entity3_ ( see the review by Kukich ( Kukich , 1992 ) ) are not readily applicable to _entity4_ agglutinative languages _/entity4_ . This poster presents an approach to _entity5_ spelling correction _/entity5_ in _entity6_ agglutinative languages _/entity6_ that is based on _entity7_ two-level morphology _/entity7_ and a _entity8_ dynamic-programming based search algorithm _/entity8_ . After an overview of our approach , we present results from experiments with _entity9_ spelling correction _/entity9_ in _entity10_ Turkish _/entity10_ .	NONE entity3 entity2
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ _C_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ _P_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ rule application _/entity15_ . First , it uses several kinds of _entity16_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity13 entity10
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ _P_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ _C_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity6 entity7
This paper presents the results of automatically inducing a _entity1_ _P_ Combinatory Categorial Grammar ( CCG ) lexicon _/entity1_ from a _entity2_ _C_ Turkish dependency treebank _/entity2_ . The fact that _entity3_ Turkish _/entity3_ is an _entity4_ agglutinating free word order language _/entity4_ presents a challenge for _entity5_ language theories _/entity5_ . We explored possible ways to obtain a _entity6_ compact lexicon _/entity6_ , consistent with _entity7_ CCG principles _/entity7_ , from a _entity8_ treebank _/entity8_ which is an order of magnitude smaller than _entity9_ Penn WSJ _/entity9_ .	NONE entity1 entity2
We present _entity1_ Minimum Bayes-Risk ( MBR ) decoding _/entity1_ for _entity2_ statistical machine translation _/entity2_ . This statistical approach aims to minimize _entity3_ expected loss _/entity3_ of _entity4_ translation errors _/entity4_ under _entity5_ loss functions _/entity5_ that measure _entity6_ translation performance _/entity6_ . We describe a hierarchy of _entity7_ loss functions _/entity7_ that incorporate different levels of _entity8_ linguistic information _/entity8_ from _entity9_ word strings _/entity9_ , _entity10_ word-to-word alignments _/entity10_ from an _entity11_ MT system _/entity11_ , and _entity12_ syntactic structure _/entity12_ from _entity13_ parse-trees _/entity13_ of _entity14_ source and target language sentences _/entity14_ . We report the _entity15_ performance _/entity15_ of the _entity16_ MBR decoders _/entity16_ on a _entity17_ _C_ Chinese-to-English translation task _/entity17_ . Our results show that _entity18_ MBR decoding _/entity18_ can be used to tune _entity19_ statistical MT performance _/entity19_ for specific _entity20_ _P_ loss functions _/entity20_ .	NONE entity20 entity17
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ _C_ deterministic finite automaton _/entity12_ . This _entity13_ _P_ semantics _/entity13_ for _entity14_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity13 entity12
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ CCRs _/entity3_ are _entity4_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ categories _/entity5_ in _entity6_ local trees _/entity6_ which allow the _entity7_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ _P_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ _C_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	NONE entity10 entity11
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ _C_ word segmentation _/entity8_ and an _entity9_ _P_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity9 entity8
_entity1_ Manual acquisition _/entity1_ of _entity2_ semantic constraints _/entity2_ in broad domains is very expensive . This paper presents an automatic scheme for collecting statistics on _entity3_ cooccurrence patterns _/entity3_ in a large _entity4_ corpus _/entity4_ . To a large extent , these statistics reflect _entity5_ semantic constraints _/entity5_ and thus are used to disambiguate _entity6_ anaphora references _/entity6_ and _entity7_ syntactic ambiguities _/entity7_ . The scheme was implemented by gathering statistics on the output of other linguistic tools . An experiment was performed to resolve _entity8_ references _/entity8_ of the _entity9_ pronoun `` it '' _/entity9_ in _entity10_ sentences _/entity10_ that were randomly selected from the _entity11_ _P_ corpus _/entity11_ . The results of the experiment show that in most of the cases the _entity12_ _C_ cooccurrence statistics _/entity12_ indeed reflect the _entity13_ semantic constraints _/entity13_ and thus provide a basis for a useful _entity14_ disambiguation tool _/entity14_ .	NONE entity11 entity12
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ _P_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ _C_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity9 entity10
In the _entity1_ Chinese language _/entity1_ , a _entity2_ verb _/entity2_ may have its _entity3_ dependents _/entity3_ on its left , right or on both sides . The _entity4_ _C_ ambiguity resolution _/entity4_ of _entity5_ right-side dependencies _/entity5_ is essential for _entity6_ _P_ dependency parsing _/entity6_ of _entity7_ sentences _/entity7_ with two or more _entity8_ verbs _/entity8_ . Previous works on _entity9_ shift-reduce dependency parsers _/entity9_ may not guarantee the _entity10_ connectivity _/entity10_ of a _entity11_ dependency tree _/entity11_ due to their weakness at resolving the _entity12_ right-side dependencies _/entity12_ . This paper proposes a _entity13_ two-phase shift-reduce dependency parser _/entity13_ based on _entity14_ SVM learning _/entity14_ . The _entity15_ left-side dependents _/entity15_ and _entity16_ right-side nominal dependents _/entity16_ are detected in Phase I , and _entity17_ right-side verbal dependents _/entity17_ are decided in Phase II . In experimental evaluation , our proposed method outperforms previous _entity18_ shift-reduce dependency parsers _/entity18_ for the _entity19_ Chine language _/entity19_ , showing improvement of _entity20_ dependency accuracy _/entity20_ by 10.08 % .	NONE entity6 entity4
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ _C_ accuracy _/entity5_ of a _entity6_ _P_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	RESULT entity6 entity5
This paper examines what kind of _entity1_ similarity _/entity1_ between _entity2_ words _/entity2_ can be represented by what kind of _entity3_ word vectors _/entity3_ in the _entity4_ vector space model _/entity4_ . Through two experiments , three _entity5_ methods for constructing word vectors _/entity5_ , i.e. , _entity6_ LSA-based , cooccurrence-based and dictionary-based methods _/entity6_ , were compared in terms of the ability to represent two kinds of _entity7_ similarity _/entity7_ , i.e. , _entity8_ taxonomic similarity _/entity8_ and _entity9_ associative similarity _/entity9_ . The result of the comparison was that the _entity10_ dictionary-based word vectors _/entity10_ better reflect _entity11_ _P_ taxonomic similarity _/entity11_ , while the _entity12_ LSA-based and the cooccurrence-based word vectors _/entity12_ better reflect _entity13_ _C_ associative similarity _/entity13_ .	NONE entity11 entity13
This paper addresses the problem of identifying likely _entity1_ topics _/entity1_ of _entity2_ texts _/entity2_ by their position in the _entity3_ _P_ text _/entity3_ . It describes the automated _entity4_ _C_ training _/entity4_ and evaluation of an _entity5_ Optimal Position Policy _/entity5_ , a method of locating the likely positions of _entity6_ topic-bearing sentences _/entity6_ based on _entity7_ genre-specific regularities _/entity7_ of _entity8_ discourse structure _/entity8_ . This method can be used in applications such as _entity9_ information retrieval _/entity9_ , _entity10_ routing _/entity10_ , and _entity11_ text summarization _/entity11_ .	NONE entity3 entity4
The purpose of this research is to test the efficacy of applying _entity1_ automated evaluation techniques _/entity1_ , originally devised for the evaluation of _entity2_ human language learners _/entity2_ , to the _entity3_ output _/entity3_ of _entity4_ machine translation ( MT ) systems _/entity4_ . We believe that these _entity5_ evaluation techniques _/entity5_ will provide information about both the _entity6_ human language learning process _/entity6_ , the _entity7_ translation process _/entity7_ and the _entity8_ development _/entity8_ of _entity9_ machine translation systems _/entity9_ . This , the first experiment in a series of experiments , looks at the _entity10_ intelligibility _/entity10_ of _entity11_ MT output _/entity11_ . A _entity12_ language learning experiment _/entity12_ showed that _entity13_ assessors _/entity13_ can differentiate _entity14_ native from non-native language essays _/entity14_ in less than 100 _entity15_ _C_ words _/entity15_ . Even more illuminating was the factors on which the _entity16_ _P_ assessors _/entity16_ made their decisions . We tested this to see if similar criteria could be elicited from duplicating the experiment using _entity17_ machine translation output _/entity17_ . Subjects were given a set of up to six extracts of _entity18_ translated newswire text _/entity18_ . Some of the extracts were _entity19_ expert human translations _/entity19_ , others were _entity20_ machine translation outputs _/entity20_ . The subjects were given three minutes per extract to determine whether they believed the sample output to be an _entity21_ expert human translation _/entity21_ or a _entity22_ machine translation _/entity22_ . Additionally , they were asked to mark the _entity23_ word _/entity23_ at which they made this decision . The results of this experiment , along with a preliminary analysis of the factors involved in the decision making process will be presented here .	NONE entity16 entity15
A purely functional implementation of _entity1_ LR-parsers _/entity1_ is given , together with a simple _entity2_ correctness proof _/entity2_ . It is presented as a generalization of the _entity3_ recursive descent parser _/entity3_ . For _entity4_ _P_ non-LR grammars _/entity4_ the time-complexity of our _entity5_ _C_ parser _/entity5_ is cubic if the functions that constitute the _entity6_ parser _/entity6_ are implemented as _entity7_ memo-functions _/entity7_ , i.e . functions that memorize the results of previous invocations . _entity8_ Memo-functions _/entity8_ also facilitate a simple way to construct a very compact representation of the _entity9_ parse forest _/entity9_ . For _entity10_ LR ( 0 ) grammars _/entity10_ , our algorithm is closely related to the _entity11_ recursive ascent parsers _/entity11_ recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] . _entity12_ Extended CF grammars _/entity12_ ( _entity13_ grammars _/entity13_ with _entity14_ regular expressions _/entity14_ at the right hand side ) can be parsed with a simple modification of the _entity15_ LR-parser _/entity15_ for normal _entity16_ CF grammars _/entity16_ .	NONE entity4 entity5
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ _C_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ _P_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity9 entity6
We describe a novel technique and implemented system for constructing a _entity1_ subcategorization dictionary _/entity1_ from _entity2_ textual corpora _/entity2_ . Each _entity3_ dictionary entry _/entity3_ encodes the _entity4_ relative frequency of occurrence _/entity4_ of a comprehensive set of _entity5_ subcategorization classes _/entity5_ for _entity6_ _C_ English _/entity6_ . An initial experiment , on a sample of 14 _entity7_ verbs _/entity7_ which exhibit _entity8_ multiple complementation patterns _/entity8_ , demonstrates that the technique achieves _entity9_ _P_ accuracy _/entity9_ comparable to previous approaches , which are all limited to a highly restricted set of _entity10_ subcategorization classes _/entity10_ . We also demonstrate that a _entity11_ subcategorization dictionary _/entity11_ built with the system improves the _entity12_ accuracy _/entity12_ of a _entity13_ parser _/entity13_ by an appreciable amount	NONE entity9 entity6
We describe a three-tiered approach for evaluation of _entity1_ _C_ spoken dialogue systems _/entity1_ . The three tiers measure _entity2_ _P_ user satisfaction _/entity2_ , _entity3_ system support of mission success _/entity3_ and _entity4_ component performance _/entity4_ . We describe our use of this approach in numerous fielded _entity5_ user studies _/entity5_ conducted with the U.S. military .	NONE entity2 entity1
We discuss _entity1_ maximum a posteriori estimation _/entity1_ of _entity2_ continuous density hidden Markov models ( CDHMM ) _/entity2_ . The classical _entity3_ MLE reestimation algorithms _/entity3_ , namely the _entity4_ forward-backward algorithm _/entity4_ and the _entity5_ segmental k-means algorithm _/entity5_ , are expanded and _entity6_ reestimation formulas _/entity6_ are given for _entity7_ _P_ HMM with Gaussian mixture observation densities _/entity7_ . Because of its adaptive nature , _entity8_ _C_ Bayesian learning _/entity8_ serves as a unified approach for the following four _entity9_ speech recognition _/entity9_ applications , namely _entity10_ parameter smoothing _/entity10_ , _entity11_ speaker adaptation _/entity11_ , _entity12_ speaker group modeling _/entity12_ and _entity13_ corrective training _/entity13_ . New experimental results on all four applications are provided to show the effectiveness of the _entity14_ MAP estimation approach _/entity14_ .	NONE entity7 entity8
A central problem of _entity1_ word sense disambiguation ( WSD ) _/entity1_ is the lack of _entity2_ manually sense-tagged data _/entity2_ required for _entity3_ supervised learning _/entity3_ . In this paper , we evaluate an approach to automatically acquire _entity4_ sense-tagged training data _/entity4_ from _entity5_ English-Chinese parallel corpora _/entity5_ , which are then used for disambiguating the _entity6_ nouns _/entity6_ in the _entity7_ SENSEVAL-2 English lexical sample task _/entity7_ . Our investigation reveals that this _entity8_ method of acquiring sense-tagged data _/entity8_ is promising . On a subset of the most difficult _entity9_ _P_ SENSEVAL-2 nouns _/entity9_ , the _entity10_ accuracy _/entity10_ difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that _entity11_ manually sense-tagged data _/entity11_ have in their _entity12_ _C_ sense coverage _/entity12_ . Our analysis also highlights the importance of the issue of _entity13_ domain dependence _/entity13_ in evaluating _entity14_ WSD programs _/entity14_ .	NONE entity9 entity12
Two themes have evolved in _entity1_ speech and text image processing _/entity1_ work at _entity2_ Xerox PARC _/entity2_ that expand and redefine the role of _entity3_ recognition technology _/entity3_ in _entity4_ document-oriented applications _/entity4_ . One is the development of systems that provide functionality similar to that of _entity5_ text processors _/entity5_ but operate directly on _entity6_ audio and scanned image data _/entity6_ . A second , related theme is the use of _entity7_ speech and text-image recognition _/entity7_ to retrieve arbitrary , user-specified information from _entity8_ documents with signal content _/entity8_ . This paper discusses three research initiatives at _entity9_ PARC _/entity9_ that exemplify these themes : a _entity10_ text-image editor _/entity10_ [ 1 ] , a _entity11_ wordspotter _/entity11_ for _entity12_ _P_ voice editing and indexing _/entity12_ [ 12 ] , and a _entity13_ decoding framework _/entity13_ for _entity14_ scanned-document content retrieval _/entity14_ [ 4 ] . The discussion focuses on key concepts embodied in the research that enable novel _entity15_ _C_ signal-based document processing functionality _/entity15_ .	NONE entity12 entity15
This paper presents an algorithm for selecting an appropriate _entity1_ classifier word _/entity1_ for a _entity2_ noun _/entity2_ . In _entity3_ Thai language _/entity3_ , it frequently happens that there is fluctuation in the choice of _entity4_ classifier _/entity4_ for a given _entity5_ concrete noun _/entity5_ , both from the point of view of the whole _entity6_ speech community _/entity6_ and _entity7_ individual speakers _/entity7_ . Basically , there is no exact rule for _entity8_ classifier selection _/entity8_ . As far as we can do in the _entity9_ rule-based approach _/entity9_ is to give a _entity10_ default rule _/entity10_ to pick up a corresponding _entity11_ classifier _/entity11_ of each _entity12_ noun _/entity12_ . Registration of _entity13_ classifier _/entity13_ for each _entity14_ noun _/entity14_ is limited to the _entity15_ type of unit classifier _/entity15_ because other types are open due to the meaning of representation . We propose a _entity16_ corpus-based method _/entity16_ ( Biber,1993 ; Nagao,1993 ; Smadja,1993 ) which generates _entity17_ Noun Classifier Associations ( NCA ) _/entity17_ to overcome the problems in _entity18_ classifier assignment _/entity18_ and _entity19_ semantic construction of noun phrase _/entity19_ . The _entity20_ NCA _/entity20_ is created statistically from a large _entity21_ _C_ corpus _/entity21_ and recomposed under _entity22_ concept hierarchy constraints _/entity22_ and _entity23_ _P_ frequency of occurrences _/entity23_ .	NONE entity23 entity21
One of the major problems one is faced with when decomposing _entity1_ words _/entity1_ into their _entity2_ constituent parts _/entity2_ is _entity3_ ambiguity _/entity3_ : the _entity4_ generation _/entity4_ of multiple _entity5_ analyses _/entity5_ for one _entity6_ input word _/entity6_ , many of which are implausible . In order to deal with _entity7_ ambiguity _/entity7_ , the _entity8_ _P_ MORphological PArser MORPA _/entity8_ is provided with a _entity9_ _C_ probabilistic context-free grammar ( PCFG ) _/entity9_ , i.e . it combines a _entity10_ `` conventional '' context-free morphological grammar _/entity10_ to filter out _entity11_ ungrammatical segmentations _/entity11_ with a _entity12_ probability-based scoring function _/entity12_ which determines the likelihood of each successful _entity13_ parse _/entity13_ . Consequently , remaining _entity14_ analyses _/entity14_ can be ordered along a scale of plausibility . Test performance data will show that a _entity15_ PCFG _/entity15_ yields good results in _entity16_ morphological parsing _/entity16_ . _entity17_ MORPA _/entity17_ is a fully implemented _entity18_ parser _/entity18_ developed for use in a _entity19_ text-to-speech conversion system _/entity19_ .	NONE entity8 entity9
In this paper , we compare the relative effects of _entity1_ segment order _/entity1_ , _entity2_ segmentation _/entity2_ and _entity3_ segment contiguity _/entity3_ on the _entity4_ retrieval performance _/entity4_ of a _entity5_ translation memory system _/entity5_ . We take a selection of both _entity6_ bag-of-words and segment order-sensitive string comparison methods _/entity6_ , and run each over both _entity7_ character- and word-segmented data _/entity7_ , in combination with a range of _entity8_ local segment contiguity models _/entity8_ ( in the form of _entity9_ N-grams _/entity9_ ) . Over two distinct _entity10_ datasets _/entity10_ , we find that _entity11_ indexing _/entity11_ according to simple _entity12_ _P_ character bigrams _/entity12_ produces a _entity13_ retrieval accuracy _/entity13_ superior to any of the tested _entity14_ _C_ word N-gram models _/entity14_ . Further , in their optimum _entity15_ configuration _/entity15_ , _entity16_ bag-of-words methods _/entity16_ are shown to be equivalent to _entity17_ segment order-sensitive methods _/entity17_ in terms of _entity18_ retrieval accuracy _/entity18_ , but much faster . We also provide evidence that our findings are scalable .	NONE entity12 entity14
This paper presents a _entity1_ maximum entropy word alignment algorithm _/entity1_ for _entity2_ Arabic-English _/entity2_ based on _entity3_ supervised training data _/entity3_ . We demonstrate that it is feasible to create _entity4_ training material _/entity4_ for problems in _entity5_ machine translation _/entity5_ and that a mixture of _entity6_ supervised and unsupervised methods _/entity6_ yields superior _entity7_ performance _/entity7_ . The _entity8_ probabilistic model _/entity8_ used in the _entity9_ alignment _/entity9_ directly models the _entity10_ _P_ link decisions _/entity10_ . Significant improvement over traditional _entity11_ _C_ word alignment techniques _/entity11_ is shown as well as improvement on several _entity12_ machine translation tests _/entity12_ . Performance of the algorithm is contrasted with _entity13_ human annotation performance _/entity13_ .	NONE entity10 entity11
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ structure _/entity2_ and _entity3_ _C_ surface representation _/entity3_ of _entity4_ _P_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity4 entity3
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ _P_ corpus of sentences _/entity3_ in the domain of the _entity4_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ _C_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity3 entity5
This paper introduces a method for _entity1_ computational analysis of move structures _/entity1_ in _entity2_ abstracts _/entity2_ of _entity3_ research articles _/entity3_ . In our approach , _entity4_ sentences _/entity4_ in a given _entity5_ abstract _/entity5_ are analyzed and labeled with a specific _entity6_ move _/entity6_ in light of various _entity7_ rhetorical functions _/entity7_ . The method involves automatically gathering a large number of _entity8_ _P_ abstracts _/entity8_ from the _entity9_ Web _/entity9_ and building a _entity10_ _C_ language model _/entity10_ of _entity11_ abstract moves _/entity11_ . We also present a prototype _entity12_ concordancer _/entity12_ , _entity13_ CARE _/entity13_ , which exploits the _entity14_ move-tagged abstracts _/entity14_ for _entity15_ digital learning _/entity15_ . This system provides a promising approach to _entity16_ Web-based computer-assisted academic writing _/entity16_ .	NONE entity8 entity10
This paper describes novel and practical _entity1_ Japanese parsers _/entity1_ that uses _entity2_ _P_ decision trees _/entity2_ . First , we construct a single _entity3_ decision tree _/entity3_ to estimate _entity4_ modification probabilities _/entity4_ ; how one _entity5_ _C_ phrase _/entity5_ tends to modify another . Next , we introduce a _entity6_ boosting algorithm _/entity6_ in which several _entity7_ decision trees _/entity7_ are constructed and then combined for _entity8_ probability estimation _/entity8_ . The two constructed _entity9_ parsers _/entity9_ are evaluated by using the _entity10_ EDR Japanese annotated corpus _/entity10_ . The single-tree method outperforms the _entity11_ conventional Japanese stochastic methods _/entity11_ by 4 % . Moreover , the boosting version is shown to have significant advantages ; 1 ) better _entity12_ parsing accuracy _/entity12_ than its single-tree counterpart for any amount of _entity13_ training data _/entity13_ and 2 ) no _entity14_ over-fitting to data _/entity14_ for various _entity15_ iterations _/entity15_ .	NONE entity2 entity5
The goal of this research is to develop a _entity1_ spoken language system _/entity1_ that will demonstrate the usefulness of _entity2_ voice input _/entity2_ for _entity3_ interactive problem solving _/entity3_ . The system will accept _entity4_ _P_ continuous speech _/entity4_ , and will handle _entity5_ multiple speakers _/entity5_ without _entity6_ explicit speaker enrollment _/entity6_ . Combining _entity7_ _C_ speech recognition _/entity7_ and _entity8_ natural language processing _/entity8_ to achieve _entity9_ speech understanding _/entity9_ , the system will be demonstrated in an _entity10_ application domain _/entity10_ relevant to the DoD . The objective of this project is to develop a _entity11_ robust and high-performance speech recognition system _/entity11_ using a _entity12_ segment-based approach _/entity12_ to _entity13_ phonetic recognition _/entity13_ . The _entity14_ recognition system _/entity14_ will eventually be integrated with _entity15_ natural language processing _/entity15_ to achieve _entity16_ spoken language understanding _/entity16_ .	NONE entity4 entity7
`` To explain complex phenomena , an _entity1_ explanation system _/entity1_ must be able to select information from a formal representation of _entity2_ domain knowledge _/entity2_ , organize the selected information into _entity3_ multisentential discourse plans _/entity3_ , and realize the _entity4_ discourse plans _/entity4_ in text . Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for _entity5_ explanation _/entity5_ , empirical results have been limited . This paper reports on a seven-year effort to empirically study _entity6_ explanation generation _/entity6_ from _entity7_ _C_ semantically rich , large-scale knowledge bases _/entity7_ . In particular , it describes a _entity8_ robust explanation system _/entity8_ that constructs _entity9_ multisentential and multi-paragraph explanations _/entity9_ from the a _entity10_ _P_ large-scale knowledge base _/entity10_ in the domain of botanical anatomy , physiology , and development . We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system . In this evaluation , scored within `` '' half a grade '' '' of domain experts , and its performance exceeded that of one of the domain experts . ''	NONE entity10 entity7
In this paper , we address the problem of combining several _entity1_ language models ( LMs ) _/entity1_ . We find that simple _entity2_ interpolation methods _/entity2_ , like _entity3_ log-linear and linear interpolation _/entity3_ , improve the _entity4_ performance _/entity4_ but fall short of the _entity5_ performance _/entity5_ of an _entity6_ _P_ oracle _/entity6_ . The _entity7_ oracle _/entity7_ knows the _entity8_ reference word string _/entity8_ and selects the _entity9_ _C_ word string _/entity9_ with the best _entity10_ performance _/entity10_ ( typically , _entity11_ word or semantic error rate _/entity11_ ) from a list of _entity12_ word strings _/entity12_ , where each _entity13_ word string _/entity13_ has been obtained by using a different _entity14_ LM _/entity14_ . Actually , the _entity15_ oracle _/entity15_ acts like a _entity16_ dynamic combiner _/entity16_ with _entity17_ hard decisions _/entity17_ using the _entity18_ reference _/entity18_ . We provide experimental results that clearly show the need for a _entity19_ dynamic language model combination _/entity19_ to improve the _entity20_ performance _/entity20_ further . We suggest a method that mimics the behavior of the _entity21_ oracle _/entity21_ using a _entity22_ neural network _/entity22_ or a _entity23_ decision tree _/entity23_ . The method amounts to tagging _entity24_ LMs _/entity24_ with _entity25_ confidence measures _/entity25_ and picking the best _entity26_ hypothesis _/entity26_ corresponding to the _entity27_ LM _/entity27_ with the best _entity28_ confidence _/entity28_ .	NONE entity6 entity9
We propose a framework to derive the _entity1_ distance _/entity1_ between _entity2_ concepts _/entity2_ from _entity3_ distributional measures of word co-occurrences _/entity3_ . We use the _entity4_ _P_ categories _/entity4_ in a published _entity5_ thesaurus _/entity5_ as _entity6_ coarse-grained concepts _/entity6_ , allowing all possible _entity7_ _C_ distance values _/entity7_ to be stored in a _entity8_ concept-concept matrix _/entity8_ roughly.01 % the size of that created by existing measures . We show that the newly proposed _entity9_ concept-distance measures _/entity9_ outperform _entity10_ traditional distributional word-distance measures _/entity10_ in the tasks of ( 1 ) ranking _entity11_ word pairs _/entity11_ in order of _entity12_ semantic distance _/entity12_ , and ( 2 ) correcting _entity13_ real-word spelling errors _/entity13_ . In the latter task , of all the _entity14_ WordNet-based measures _/entity14_ , only that proposed by Jiang and Conrath outperforms the best _entity15_ distributional concept-distance measures _/entity15_ .	NONE entity4 entity7
This paper concerns the _entity1_ discourse understanding process _/entity1_ in _entity2_ spoken dialogue systems _/entity2_ . This process enables the system to understand _entity3_ user utterances _/entity3_ based on the _entity4_ context _/entity4_ of a _entity5_ _C_ dialogue _/entity5_ . Since multiple _entity6_ candidates _/entity6_ for the _entity7_ _P_ understanding _/entity7_ result can be obtained for a _entity8_ user utterance _/entity8_ due to the _entity9_ ambiguity _/entity9_ of _entity10_ speech understanding _/entity10_ , it is not appropriate to decide on a single _entity11_ understanding _/entity11_ result after each _entity12_ user utterance _/entity12_ . By holding multiple _entity13_ candidates _/entity13_ for _entity14_ understanding _/entity14_ results and resolving the _entity15_ ambiguity _/entity15_ as the _entity16_ dialogue _/entity16_ progresses , the _entity17_ discourse understanding accuracy _/entity17_ can be improved . This paper proposes a method for resolving this _entity18_ ambiguity _/entity18_ based on _entity19_ statistical information _/entity19_ obtained from _entity20_ dialogue corpora _/entity20_ . Unlike conventional methods that use _entity21_ hand-crafted rules _/entity21_ , the proposed method enables easy design of the _entity22_ discourse understanding process _/entity22_ . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple _entity23_ candidates _/entity23_ for _entity24_ understanding _/entity24_ results is effective .	NONE entity7 entity5
_entity1_ Spelling-checkers _/entity1_ have become an integral part of most _entity2_ text processing software _/entity2_ . From different reasons among which the speed of processing prevails they are usually based on _entity3_ dictionaries of word forms _/entity3_ instead of _entity4_ words _/entity4_ . This approach is sufficient for languages with little _entity5_ _P_ inflection _/entity5_ such as _entity6_ _C_ English _/entity6_ , but fails for _entity7_ highly inflective languages _/entity7_ such as _entity8_ Czech _/entity8_ , _entity9_ Russian _/entity9_ , _entity10_ Slovak _/entity10_ or other _entity11_ Slavonic languages _/entity11_ . We have developed a special method for describing _entity12_ inflection _/entity12_ for the purpose of building _entity13_ spelling-checkers _/entity13_ for such languages . The speed of the resulting program lies somewhere in the middle of the scale of existing _entity14_ spelling-checkers _/entity14_ for _entity15_ English _/entity15_ and the main _entity16_ dictionary _/entity16_ fits into the standard _entity17_ 360K floppy _/entity17_ , whereas the number of recognized _entity18_ word forms _/entity18_ exceeds 6 million ( for _entity19_ Czech _/entity19_ ) . Further , a special method has been developed for easy _entity20_ word classification _/entity20_ .	MODEL-FEATURE entity5 entity6
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ _P_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ _C_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity3 entity6
This paper introduces a _entity1_ system for categorizing unknown words _/entity1_ . The _entity2_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ _P_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ _C_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity6 entity9
_entity1_ Semantic theories _/entity1_ of _entity2_ natural language _/entity2_ associate _entity3_ meanings _/entity3_ with _entity4_ utterances _/entity4_ by providing _entity5_ meanings _/entity5_ for _entity6_ lexical items _/entity6_ and _entity7_ rules _/entity7_ for determining the _entity8_ meaning _/entity8_ of larger _entity9_ units _/entity9_ given the _entity10_ meanings _/entity10_ of their parts . Traditionally , _entity11_ meanings _/entity11_ are combined via _entity12_ function composition _/entity12_ , which works well when _entity13_ constituent structure trees _/entity13_ are used to guide _entity14_ semantic composition _/entity14_ . More recently , the _entity15_ functional structure _/entity15_ of _entity16_ LFG _/entity16_ has been used to provide the _entity17_ syntactic information _/entity17_ necessary for constraining _entity18_ derivations _/entity18_ of _entity19_ meaning _/entity19_ in a _entity20_ cross-linguistically uniform format _/entity20_ . It has been difficult , however , to reconcile this _entity21_ _P_ approach _/entity21_ with the combination of _entity22_ _C_ meanings _/entity22_ by _entity23_ function composition _/entity23_ . In contrast to _entity24_ compositional approaches _/entity24_ , we present a _entity25_ deductive approach _/entity25_ to assembling _entity26_ meanings _/entity26_ , based on _entity27_ reasoning with constraints _/entity27_ , which meshes well with the unordered nature of _entity28_ information _/entity28_ in the _entity29_ functional structure _/entity29_ . Our use of _entity30_ linear logic _/entity30_ as a 'glue ' for assembling _entity31_ meanings _/entity31_ also allows for a coherent treatment of _entity32_ modification _/entity32_ as well as of the _entity33_ LFG _/entity33_ requirements of _entity34_ completeness _/entity34_ and _entity35_ coherence _/entity35_ .	NONE entity21 entity22
We describe the methods and hardware that we are using to produce a real-time demonstration of an _entity1_ integrated Spoken Language System _/entity1_ . We describe algorithms that greatly reduce the computation needed to compute the _entity2_ N-Best sentence hypotheses _/entity2_ . To avoid _entity3_ grammar coverage problems _/entity3_ we use a _entity4_ fully-connected first-order statistical class grammar _/entity4_ . The _entity5_ speech-search algorithm _/entity5_ is implemented on a _entity6_ _C_ board _/entity6_ with a single _entity7_ Intel i860 chip _/entity7_ , which provides a factor of 5 speed-up over a _entity8_ SUN 4 _/entity8_ for _entity9_ _P_ straight C code _/entity9_ . The _entity10_ board _/entity10_ plugs directly into the _entity11_ VME bus _/entity11_ of the _entity12_ SUN4 _/entity12_ , which controls the system and contains the _entity13_ natural language system _/entity13_ and _entity14_ application back end _/entity14_ .	NONE entity9 entity6
In this paper we explore a new _entity1_ _P_ theory of discourse structure _/entity1_ that stresses the role of _entity2_ purpose _/entity2_ and _entity3_ processing _/entity3_ in _entity4_ _C_ discourse _/entity4_ . In this theory , _entity5_ discourse structure _/entity5_ is composed of three separate but interrelated components : the structure of the sequence of _entity6_ utterances _/entity6_ ( called the _entity7_ linguistic structure _/entity7_ ) , a structure of _entity8_ purposes _/entity8_ ( called the _entity9_ intentional structure _/entity9_ ) , and the state of _entity10_ focus of attention _/entity10_ ( called the _entity11_ attentional state _/entity11_ ) . The _entity12_ linguistic structure _/entity12_ consists of segments of the _entity13_ discourse _/entity13_ into which the _entity14_ utterances _/entity14_ naturally aggregate . The _entity15_ intentional structure _/entity15_ captures the _entity16_ discourse-relevant purposes _/entity16_ , expressed in each of the _entity17_ linguistic segments _/entity17_ as well as relationships among them . The _entity18_ attentional state _/entity18_ is an abstraction of the _entity19_ focus of attention _/entity19_ of the _entity20_ participants _/entity20_ as the _entity21_ discourse _/entity21_ unfolds . The _entity22_ attentional state _/entity22_ , being dynamic , records the objects , properties , and relations that are salient at each point of the _entity23_ discourse _/entity23_ . The distinction among these components is essential to provide an adequate explanation of such _entity24_ discourse phenomena _/entity24_ as _entity25_ cue phrases _/entity25_ , _entity26_ referring expressions _/entity26_ , and _entity27_ interruptions _/entity27_ . The _entity28_ theory of attention , intention , and aggregation of utterances _/entity28_ is illustrated in the paper with a number of example _entity29_ discourses _/entity29_ . Various properties of _entity30_ discourse _/entity30_ are described , and explanations for the behaviour of _entity31_ cue phrases _/entity31_ , _entity32_ referring expressions _/entity32_ , and _entity33_ interruptions _/entity33_ are explored . This _entity34_ theory _/entity34_ provides a framework for describing the processing of _entity35_ utterances _/entity35_ in a _entity36_ discourse _/entity36_ . _entity37_ Discourse processing _/entity37_ requires recognizing how the _entity38_ utterances _/entity38_ of the _entity39_ discourse _/entity39_ aggregate into _entity40_ segments _/entity40_ , recognizing the _entity41_ intentions _/entity41_ expressed in the _entity42_ discourse _/entity42_ and the relationships among _entity43_ intentions _/entity43_ , and tracking the _entity44_ discourse _/entity44_ through the operation of the mechanisms associated with _entity45_ attentional state _/entity45_ . This processing description specifies in these _entity46_ recognition tasks _/entity46_ the role of information from the _entity47_ discourse _/entity47_ and from the _entity48_ participants _/entity48_ ' knowledge of the domain .	NONE entity1 entity4
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ unsupervised word alignment component _/entity9_ . We align a _entity10_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ _C_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ _P_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity16 entity13
In this paper we deal with a recently developed _entity1_ large Czech MWE database _/entity1_ containing at the moment 160 000 _entity2_ MWEs _/entity2_ ( treated as _entity3_ lexical units _/entity3_ ) . It was compiled from various resources such as _entity4_ encyclopedias _/entity4_ and _entity5_ dictionaries _/entity5_ , public _entity6_ databases _/entity6_ of _entity7_ proper names _/entity7_ and _entity8_ toponyms _/entity8_ , _entity9_ collocations _/entity9_ obtained from _entity10_ Czech WordNet _/entity10_ , lists of _entity11_ botanical and zoological terms _/entity11_ and others . We describe the structure of the _entity12_ database _/entity12_ and give basic types of _entity13_ MWEs _/entity13_ according to domains they belong to . We compare the built _entity14_ MWEs database _/entity14_ with the _entity15_ corpus data _/entity15_ from _entity16_ Czech National Corpus _/entity16_ ( approx . 100 mil . tokens ) and present results of this comparison in the paper . These _entity17_ MWEs _/entity17_ have not been obtained from the _entity18_ corpus _/entity18_ since their frequencies in it are rather low . To obtain a more complete list of _entity19_ MWEs _/entity19_ we propose and use a technique exploiting the _entity20_ Word Sketch Engine _/entity20_ , which allows us to work with _entity21_ statistical parameters _/entity21_ such as frequency of _entity22_ MWEs _/entity22_ and their components as well as with the _entity23_ _P_ salience _/entity23_ for the whole _entity24_ MWEs _/entity24_ . We also discuss exploitation of the _entity25_ _C_ database _/entity25_ for working out a more adequate _entity26_ tagging _/entity26_ and _entity27_ lemmatization _/entity27_ . The final goal is to be able to recognize _entity28_ MWEs _/entity28_ in _entity29_ corpus text _/entity29_ and lemmatize them as complete _entity30_ lexical units _/entity30_ , i. e. to make _entity31_ tagging _/entity31_ and _entity32_ lemmatization _/entity32_ more adequate .	NONE entity23 entity25
_entity1_ English _/entity1_ is shown to be trans-context-free on the basis of _entity2_ coordinations _/entity2_ of the respectively type that involve _entity3_ strictly syntactic cross-serial agreement _/entity3_ . The _entity4_ agreement _/entity4_ in question involves _entity5_ number _/entity5_ in _entity6_ nouns _/entity6_ and _entity7_ reflexive pronouns _/entity7_ and is syntactic rather than semantic in nature because _entity8_ _C_ grammatical number _/entity8_ in _entity9_ English _/entity9_ , like _entity10_ _P_ grammatical gender _/entity10_ in _entity11_ languages _/entity11_ such as _entity12_ French _/entity12_ , is partly arbitrary . The formal proof , which makes crucial use of the _entity13_ Interchange Lemma _/entity13_ of Ogden et al. , is so constructed as to be valid even if _entity14_ English _/entity14_ is presumed to contain _entity15_ grammatical sentences _/entity15_ in which respectively operates across a pair of _entity16_ coordinate phrases _/entity16_ one of whose members has fewer _entity17_ conjuncts _/entity17_ than the other ; it thus goes through whatever the facts may be regarding _entity18_ constructions _/entity18_ with unequal numbers of _entity19_ conjuncts _/entity19_ in the _entity20_ scope _/entity20_ of respectively , whereas other _entity21_ arguments _/entity21_ have foundered on this problem .	NONE entity10 entity8
Towards deep analysis of _entity1_ compositional classes of paraphrases _/entity1_ , we have examined a _entity2_ _P_ class-oriented framework _/entity2_ for collecting _entity3_ paraphrase examples _/entity3_ , in which _entity4_ sentential paraphrases _/entity4_ are collected for each _entity5_ _C_ paraphrase class _/entity5_ separately by means of _entity6_ automatic candidate generation _/entity6_ and _entity7_ manual judgement _/entity7_ . Our preliminary experiments on building a _entity8_ paraphrase corpus _/entity8_ have so far been producing promising results , which we have evaluated according to _entity9_ cost-efficiency _/entity9_ , _entity10_ exhaustiveness _/entity10_ , and _entity11_ reliability _/entity11_ .	NONE entity2 entity5
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ dialog systems _/entity2_ within reach . However , the improved _entity3_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ _P_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ _C_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity4 entity7
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ temporal expressions _/entity2_ in _entity3_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ temporal expressions _/entity4_ in a novel _entity5_ _P_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ _C_ expressions _/entity6_ fits well with our _entity7_ constraint-based representation _/entity7_ of time , _entity8_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ baseline _/entity10_ , and compares favorably with some of the closely related work .	NONE entity5 entity6
The goal of this work is the enrichment of _entity1_ human-machine interactions _/entity1_ in a _entity2_ natural language environment _/entity2_ . Because a _entity3_ speaker _/entity3_ and _entity4_ listener _/entity4_ can not be assured to have the same _entity5_ _P_ beliefs _/entity5_ , _entity6_ _C_ contexts _/entity6_ , _entity7_ perceptions _/entity7_ , _entity8_ backgrounds _/entity8_ , or _entity9_ goals _/entity9_ , at each point in a _entity10_ conversation _/entity10_ , difficulties and mistakes arise when a _entity11_ listener _/entity11_ interprets a _entity12_ speaker 's utterance _/entity12_ . These mistakes can lead to various kinds of misunderstandings between _entity13_ speaker _/entity13_ and _entity14_ listener _/entity14_ , including _entity15_ reference failures _/entity15_ or failure to understand the _entity16_ speaker 's intention _/entity16_ . We call these misunderstandings _entity17_ miscommunication _/entity17_ . Such mistakes can slow , and possibly break down , _entity18_ communication _/entity18_ . Our goal is to recognize and isolate such _entity19_ miscommunications _/entity19_ and circumvent them . This paper highlights a particular class of _entity20_ miscommunication _/entity20_ -- - _entity21_ reference problems _/entity21_ -- - by describing a case study and techniques for avoiding _entity22_ failures of reference _/entity22_ . We want to illustrate a framework less restrictive than earlier ones by allowing a _entity23_ speaker _/entity23_ leeway in forming an _entity24_ utterance _/entity24_ about a task and in determining the conversational vehicle to deliver it . The paper also promotes a new view for _entity25_ extensional reference _/entity25_ .	NONE entity5 entity6
Both _entity1_ rhetorical structure _/entity1_ and _entity2_ punctuation _/entity2_ have been helpful in _entity3_ discourse processing _/entity3_ . Based on a _entity4_ corpus annotation project _/entity4_ , this paper reports the _entity5_ discursive usage _/entity5_ of 6 _entity6_ Chinese punctuation marks _/entity6_ in _entity7_ _P_ news commentary texts _/entity7_ : _entity8_ Colon _/entity8_ , _entity9_ _C_ Dash _/entity9_ , _entity10_ Ellipsis _/entity10_ , _entity11_ Exclamation Mark _/entity11_ , _entity12_ Question Mark _/entity12_ , and _entity13_ Semicolon _/entity13_ . The _entity14_ rhetorical patterns _/entity14_ of these marks are compared against _entity15_ patterns _/entity15_ around _entity16_ cue phrases _/entity16_ in general . Results show that these _entity17_ Chinese punctuation marks _/entity17_ , though fewer in number than _entity18_ cue phrases _/entity18_ , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in _entity19_ Chinese texts _/entity19_ .	NONE entity7 entity9
This paper discusses the application of _entity1_ Unification Categorial Grammar ( UCG ) _/entity1_ to the framework of _entity2_ Isomorphic Grammars _/entity2_ for _entity3_ Machine Translation _/entity3_ pioneered by Landsbergen . The _entity4_ Isomorphic Grammars approach to MT _/entity4_ involves developing the _entity5_ grammars _/entity5_ of the _entity6_ Source and Target languages _/entity6_ in parallel , in order to ensure that _entity7_ SL _/entity7_ and _entity8_ TL _/entity8_ expressions which stand in the _entity9_ translation relation _/entity9_ have _entity10_ isomorphic derivations _/entity10_ . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited , obviating the need for answers to _entity11_ _P_ semantic questions _/entity11_ that we do not yet have . _entity12_ Semantic _/entity12_ and other information may still be incorporated , but as constraints on the _entity13_ _C_ translation relation _/entity13_ , not as levels of _entity14_ textual representation _/entity14_ . After introducing this approach to _entity15_ MT system _/entity15_ design , and the basics of _entity16_ monolingual UCG _/entity16_ , we will show how the two can be integrated , and present an example from an implemented _entity17_ bi-directional English-Spanish fragment _/entity17_ . Finally we will present some outstanding problems with the approach .	NONE entity11 entity13
State-of-the-art _entity1_ Question Answering ( QA ) systems _/entity1_ are very sensitive to variations in the phrasing of an _entity2_ information need _/entity2_ . Finding the preferred _entity3_ language _/entity3_ for such a _entity4_ need _/entity4_ is a valuable task . We investigate that claim by adopting a simple _entity5_ MT-based paraphrasing technique _/entity5_ and evaluating _entity6_ _P_ QA system _/entity6_ performance on _entity7_ paraphrased questions _/entity7_ . We found a potential increase of 35 % in _entity8_ _C_ MRR _/entity8_ with respect to the original _entity9_ question _/entity9_ .	NONE entity6 entity8
While _entity1_ paraphrasing _/entity1_ is critical both for _entity2_ _C_ interpretation and generation of natural language _/entity2_ , current systems use manual or semi-automatic methods to collect _entity3_ _P_ paraphrases _/entity3_ . We present an _entity4_ unsupervised learning algorithm _/entity4_ for _entity5_ identification of paraphrases _/entity5_ from a _entity6_ corpus of multiple English translations _/entity6_ of the same _entity7_ source text _/entity7_ . Our approach yields _entity8_ phrasal and single word lexical paraphrases _/entity8_ as well as _entity9_ syntactic paraphrases _/entity9_ .	NONE entity3 entity2
This paper summarizes the formalism of _entity1_ Category Cooccurrence Restrictions ( CCRs ) _/entity1_ and describes two _entity2_ parsing algorithms _/entity2_ that interpret it . _entity3_ CCRs _/entity3_ are _entity4_ Boolean conditions _/entity4_ on the cooccurrence of _entity5_ categories _/entity5_ in _entity6_ local trees _/entity6_ which allow the _entity7_ statement of generalizations _/entity7_ which can not be captured in other current _entity8_ _P_ syntax formalisms _/entity8_ . The use of _entity9_ CCRs _/entity9_ leads to _entity10_ _C_ syntactic descriptions _/entity10_ formulated entirely with _entity11_ restrictive statements _/entity11_ . The paper shows how conventional algorithms for the analysis of _entity12_ context free languages _/entity12_ can be adapted to the _entity13_ CCR formalism _/entity13_ . Special attention is given to the part of the _entity14_ parser _/entity14_ that checks the fulfillment of _entity15_ logical well-formedness conditions _/entity15_ on _entity16_ trees _/entity16_ .	NONE entity8 entity10
This article deals with the _entity1_ interpretation _/entity1_ of _entity2_ _C_ conceptual operations _/entity2_ underlying the communicative use of _entity3_ _P_ natural language ( NL ) _/entity3_ within the _entity4_ Structured Inheritance Network ( SI-Nets ) paradigm _/entity4_ . The operations are reduced to _entity5_ functions _/entity5_ of a _entity6_ formal language _/entity6_ , thus changing the level of abstraction of the operations to be performed on _entity7_ SI-Nets _/entity7_ . In this sense , operations on _entity8_ SI-Nets _/entity8_ are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the _entity9_ conceptual system _/entity9_ of _entity10_ NL _/entity10_ . For this purpose , we have designed a version of _entity11_ KL-ONE _/entity11_ which represents the _entity12_ epistemological level _/entity12_ , while the new experimental language , _entity13_ KL-Conc _/entity13_ , represents the _entity14_ conceptual level _/entity14_ . KL-Conc would seem to be a more natural and intuitive way of interacting with _entity15_ SI-Nets _/entity15_ .	NONE entity3 entity2
A method of _entity1_ sense resolution _/entity1_ is proposed that is based on _entity2_ WordNet _/entity2_ , an on-line _entity3_ lexical database _/entity3_ that incorporates _entity4_ semantic relations _/entity4_ ( _entity5_ synonymy _/entity5_ , _entity6_ antonymy _/entity6_ , _entity7_ hyponymy _/entity7_ , _entity8_ meronymy _/entity8_ , _entity9_ causal and troponymic entailment _/entity9_ ) as _entity10_ labeled pointers _/entity10_ between _entity11_ word senses _/entity11_ . With _entity12_ WordNet _/entity12_ , it is easy to retrieve sets of _entity13_ semantically related words _/entity13_ , a facility that will be used for _entity14_ sense resolution _/entity14_ during _entity15_ text processing _/entity15_ , as follows . When a _entity16_ word _/entity16_ with multiple _entity17_ senses _/entity17_ is encountered , one of two procedures will be followed . Either , ( 1 ) _entity18_ words _/entity18_ related in _entity19_ meaning _/entity19_ to the _entity20_ alternative senses _/entity20_ of the _entity21_ polysemous word _/entity21_ will be retrieved ; new _entity22_ strings _/entity22_ will be derived by substituting these related _entity23_ words _/entity23_ into the _entity24_ context _/entity24_ of the _entity25_ polysemous word _/entity25_ ; a large _entity26_ textual corpus _/entity26_ will then be searched for these _entity27_ derived strings _/entity27_ ; and that _entity28_ sense _/entity28_ will be chosen that corresponds to the _entity29_ derived string _/entity29_ that is found most often in the _entity30_ corpus _/entity30_ . Or , ( 2 ) the _entity31_ context _/entity31_ of the _entity32_ _C_ polysemous word _/entity32_ will be used as a key to search a large _entity33_ corpus _/entity33_ ; all _entity34_ _P_ words _/entity34_ found to occur in that _entity35_ context _/entity35_ will be noted ; _entity36_ WordNet _/entity36_ will then be used to estimate the _entity37_ semantic distance _/entity37_ from those _entity38_ words _/entity38_ to the _entity39_ alternative senses _/entity39_ of the _entity40_ polysemous word _/entity40_ ; and that _entity41_ sense _/entity41_ will be chosen that is closest in _entity42_ meaning _/entity42_ to other _entity43_ words _/entity43_ occurring in the same _entity44_ context _/entity44_ If successful , this procedure could have practical applications to problems of _entity45_ information retrieval _/entity45_ , _entity46_ mechanical translation _/entity46_ , _entity47_ intelligent tutoring systems _/entity47_ , and elsewhere .	NONE entity34 entity32
The _entity1_ PRC Adaptive Knowledge-based Text Understanding System ( PAKTUS ) _/entity1_ has been under development as an Independent Research and Development project at PRC since 1984 . The objective is a generic system of tools , including a _entity2_ core English lexicon _/entity2_ , _entity3_ grammar _/entity3_ , and concept representations , for building _entity4_ natural language processing ( NLP ) systems _/entity4_ for _entity5_ text understanding _/entity5_ . Systems built with _entity6_ PAKTUS _/entity6_ are intended to generate input to knowledge based systems ordata base systems . Input to the _entity7_ NLP system _/entity7_ is typically derived from an existing _entity8_ _P_ electronic message stream _/entity8_ , such as a news wire . _entity9_ PAKTUS _/entity9_ supports the adaptation of the generic core to a variety of domains : _entity10_ _C_ JINTACCS messages _/entity10_ , _entity11_ RAINFORM messages _/entity11_ , _entity12_ news reports _/entity12_ about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring _entity13_ sublanguage and domain-specific grammar _/entity13_ , _entity14_ words , conceptual mappings _/entity14_ , and _entity15_ discourse patterns _/entity15_ . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .	NONE entity8 entity10
_entity1_ Unification-based grammar formalisms _/entity1_ use structures containing sets of _entity2_ features _/entity2_ to describe _entity3_ linguistic objects _/entity3_ . Although _entity4_ computational algorithms for unification of feature structures _/entity4_ have been worked out in experimental research , these algorithms become quite complicated , and a more precise description of _entity5_ feature structures _/entity5_ is desirable . We have developed a _entity6_ model _/entity6_ in which descriptions of _entity7_ feature structures _/entity7_ can be regarded as _entity8_ logical formulas _/entity8_ , and interpreted by sets of _entity9_ directed graphs _/entity9_ which satisfy them . These _entity10_ graphs _/entity10_ are , in fact , _entity11_ transition graphs _/entity11_ for a special type of _entity12_ _P_ deterministic finite automaton _/entity12_ . This _entity13_ semantics _/entity13_ for _entity14_ _C_ feature structures _/entity14_ extends the ideas of Pereira and Shieber [ 11 ] , by providing an interpretation for values which are specified by _entity15_ disjunctions _/entity15_ and _entity16_ path values _/entity16_ embedded within _entity17_ disjunctions _/entity17_ . Our interpretation differs from that of Pereira and Shieber by using a _entity18_ logical model _/entity18_ in place of a _entity19_ denotational semantics _/entity19_ . This _entity20_ logical model _/entity20_ yields a calculus of _entity21_ equivalences _/entity21_ , which can be used to simplify _entity22_ formulas _/entity22_ . _entity23_ Unification _/entity23_ is attractive , because of its generality , but it is often computationally inefficient . Our _entity24_ model _/entity24_ allows a careful examination of the _entity25_ computational complexity _/entity25_ of _entity26_ unification _/entity26_ . We have shown that the _entity27_ consistency problem _/entity27_ for _entity28_ formulas _/entity28_ with _entity29_ disjunctive values _/entity29_ is _entity30_ NP-complete _/entity30_ . To deal with this _entity31_ complexity _/entity31_ , we describe how _entity32_ disjunctive _/entity32_ values can be specified in a way which delays _entity33_ expansion _/entity33_ to _entity34_ disjunctive normal form _/entity34_ .	NONE entity12 entity14
The work presented in this paper is the first step in a project which aims to cluster and summarise _entity1_ electronic discussions _/entity1_ in the context of _entity2_ help-desk applications _/entity2_ . The eventual objective of this project is to use these _entity3_ _P_ summaries _/entity3_ to assist help-desk users and operators . In this paper , we identify _entity4_ features _/entity4_ of _entity5_ electronic discussions _/entity5_ that influence the _entity6_ _C_ clustering process _/entity6_ , and offer a _entity7_ filtering mechanism _/entity7_ that removes undesirable _entity8_ influences _/entity8_ . We tested the _entity9_ clustering and filtering processes _/entity9_ on _entity10_ electronic newsgroup discussions _/entity10_ , and evaluated their _entity11_ performance _/entity11_ by means of two experiments : _entity12_ coarse-level clustering _/entity12_ simple _entity13_ information retrieval _/entity13_ .	NONE entity3 entity6
This paper gives an overall account of a prototype _entity1_ natural language question answering system _/entity1_ , called _entity2_ Chat-80 _/entity2_ . _entity3_ Chat-80 _/entity3_ has been designed to be both efficient and easily adaptable to a variety of applications . The system is implemented entirely in _entity4_ Prolog _/entity4_ , a _entity5_ _C_ programming language _/entity5_ based on _entity6_ logic _/entity6_ . With the aid of a _entity7_ _P_ logic-based grammar formalism _/entity7_ called _entity8_ extraposition grammars _/entity8_ , _entity9_ Chat-80 _/entity9_ translates _entity10_ English questions _/entity10_ into the _entity11_ Prolog _/entity11_ _entity12_ subset of logic _/entity12_ . The resulting _entity13_ logical expression _/entity13_ is then transformed by a _entity14_ planning algorithm _/entity14_ into efficient _entity15_ Prolog _/entity15_ , cf . _entity16_ query optimisation _/entity16_ in a _entity17_ relational database _/entity17_ . Finally , the _entity18_ Prolog form _/entity18_ is executed to yield the answer .	NONE entity7 entity5
This paper reports on two contributions to _entity1_ large vocabulary continuous speech recognition _/entity1_ . First , we present a new paradigm for _entity2_ speaker-independent ( SI ) training _/entity2_ of _entity3_ hidden Markov models ( HMM ) _/entity3_ , which uses a large amount of _entity4_ speech _/entity4_ from a few _entity5_ speakers _/entity5_ instead of the traditional practice of using a little _entity6_ speech _/entity6_ from many _entity7_ speakers _/entity7_ . In addition , combination of the _entity8_ training speakers _/entity8_ is done by averaging the _entity9_ statistics _/entity9_ of _entity10_ independently trained models _/entity10_ rather than the usual pooling of all the _entity11_ speech data _/entity11_ from many _entity12_ speakers _/entity12_ prior to _entity13_ training _/entity13_ . With only 12 _entity14_ training speakers _/entity14_ for _entity15_ SI recognition _/entity15_ , we achieved a 7.5 % _entity16_ word error rate _/entity16_ on a standard _entity17_ grammar _/entity17_ and _entity18_ test set _/entity18_ from the _entity19_ DARPA Resource Management corpus _/entity19_ . This _entity20_ performance _/entity20_ is comparable to our best condition for this test suite , using 109 _entity21_ training speakers _/entity21_ . Second , we show a significant improvement for _entity22_ _C_ speaker adaptation ( SA ) _/entity22_ using the new _entity23_ SI corpus _/entity23_ and a small amount of _entity24_ _P_ speech _/entity24_ from the new ( target ) _entity25_ speaker _/entity25_ . A _entity26_ probabilistic spectral mapping _/entity26_ is estimated independently for each _entity27_ training ( reference ) speaker _/entity27_ and the _entity28_ target speaker _/entity28_ . Each _entity29_ reference model _/entity29_ is transformed to the _entity30_ space _/entity30_ of the _entity31_ target speaker _/entity31_ and combined by _entity32_ averaging _/entity32_ . Using only 40 _entity33_ utterances _/entity33_ from the _entity34_ target speaker _/entity34_ for _entity35_ adaptation _/entity35_ , the _entity36_ error rate _/entity36_ dropped to 4.1 % -- - a 45 % reduction in error compared to the _entity37_ SI _/entity37_ result .	NONE entity24 entity22
In this paper , we want to show how the _entity1_ morphological component _/entity1_ of an existing _entity2_ _C_ NLP-system for Dutch ( Dutch Medical Language Processor - DMLP ) _/entity2_ has been extended in order to produce output that is compatible with the _entity3_ _P_ language independent modules _/entity3_ of the _entity4_ LSP-MLP system ( Linguistic String Project - Medical Language Processor ) _/entity4_ of the New York University . The former can take advantage of the _entity5_ language independent developments _/entity5_ of the latter , while focusing on _entity6_ idiosyncrasies _/entity6_ for _entity7_ Dutch _/entity7_ . This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a _entity8_ patient discharge summary ( PDS ) _/entity8_ by means of modern _entity9_ HyperText Mark-Up Language ( HTML ) technology _/entity9_ . Such an application can be of use for medical administrative purposes in a hospital environment .	NONE entity3 entity2
In this paper we introduce a _entity1_ modal language LT _/entity1_ for imposing _entity2_ _P_ constraints _/entity2_ on _entity3_ trees _/entity3_ , and an extension _entity4_ LT ( LF ) _/entity4_ for imposing _entity5_ _C_ constraints _/entity5_ on _entity6_ trees decorated with feature structures _/entity6_ . The motivation for introducing these _entity7_ languages _/entity7_ is to provide tools for formalising _entity8_ grammatical frameworks _/entity8_ perspicuously , and the paper illustrates this by showing how the leading ideas of _entity9_ GPSG _/entity9_ can be captured in _entity10_ LT ( LF ) _/entity10_ . In addition , the role of _entity11_ modal languages _/entity11_ ( and in particular , what we have called as _entity12_ constraint formalisms _/entity12_ for linguistic theorising is discussed in some detail .	NONE entity2 entity5
_entity1_ Metagrammatical formalisms _/entity1_ that combine _entity2_ context-free phrase structure rules _/entity2_ and _entity3_ metarules ( MPS grammars ) _/entity3_ allow concise statement of generalizations about the _entity4_ _P_ syntax _/entity4_ of _entity5_ natural languages _/entity5_ . _entity6_ _C_ Unconstrained MPS grammars _/entity6_ , unfortunately , are not computationally safe . We evaluate several proposals for constraining them , basing our assessment on _entity7_ computational tractability and explanatory adequacy _/entity7_ . We show that none of them satisfies both criteria , and suggest new directions for research on alternative _entity8_ metagrammatical formalisms _/entity8_ .	NONE entity4 entity6
_entity1_ Information extraction techniques _/entity1_ automatically create _entity2_ _C_ structured databases _/entity2_ from _entity3_ unstructured data sources _/entity3_ , such as the Web or _entity4_ newswire documents _/entity4_ . Despite the successes of these systems , _entity5_ _P_ accuracy _/entity5_ will always be imperfect . For many reasons , it is highly desirable to accurately estimate the _entity6_ confidence _/entity6_ the system has in the correctness of each _entity7_ extracted field _/entity7_ . The _entity8_ information extraction system _/entity8_ we evaluate is based on a _entity9_ linear-chain conditional random field ( CRF ) _/entity9_ , a _entity10_ probabilistic model _/entity10_ which has performed well on _entity11_ information extraction tasks _/entity11_ because of its ability to capture arbitrary , overlapping _entity12_ features _/entity12_ of the _entity13_ input _/entity13_ in a _entity14_ Markov model _/entity14_ . We implement several techniques to estimate the _entity15_ confidence _/entity15_ of both _entity16_ extracted fields _/entity16_ and entire _entity17_ multi-field records _/entity17_ , obtaining an _entity18_ average precision _/entity18_ of 98 % for retrieving correct _entity19_ fields _/entity19_ and 87 % for multi-field records .	NONE entity5 entity2
Dividing _entity1_ sentences _/entity1_ in _entity2_ chunks of words _/entity2_ is a useful preprocessing step for _entity3_ _P_ parsing _/entity3_ , _entity4_ _C_ information extraction _/entity4_ and _entity5_ information retrieval _/entity5_ . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' _entity6_ data representation _/entity6_ for _entity7_ chunking _/entity7_ by converting it to a _entity8_ tagging task _/entity8_ . In this paper we will examine seven different _entity9_ data representations _/entity9_ for the problem of recognizing _entity10_ noun phrase chunks _/entity10_ . We will show that the _entity11_ data representation choice _/entity11_ has a minor influence on _entity12_ chunking performance _/entity12_ . However , equipped with the most suitable _entity13_ data representation _/entity13_ , our _entity14_ memory-based learning chunker _/entity14_ was able to improve the best published _entity15_ chunking results _/entity15_ for a _entity16_ standard data set _/entity16_ .	NONE entity3 entity4
In this paper , we improve an _entity1_ unsupervised learning method _/entity1_ using the _entity2_ Expectation-Maximization ( EM ) algorithm _/entity2_ proposed by Nigam et al . for _entity3_ _P_ text classification problems _/entity3_ in order to apply it to _entity4_ _C_ word sense disambiguation ( WSD ) problems _/entity4_ . The improved method stops the _entity5_ EM algorithm _/entity5_ at the _entity6_ optimum iteration number _/entity6_ . To estimate that number , we propose two methods . In experiments , we solved 50 _entity7_ noun WSD problems _/entity7_ in the _entity8_ Japanese Dictionary Task in SENSEVAL2 _/entity8_ . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for _entity9_ verb WSD problems _/entity9_ .	NONE entity3 entity4
Recent advances in _entity1_ Automatic Speech Recognition technology _/entity1_ have put the goal of naturally sounding _entity2_ _P_ dialog systems _/entity2_ within reach . However , the improved _entity3_ _C_ speech recognition _/entity3_ has brought to light a new problem : as _entity4_ dialog systems _/entity4_ understand more of what the _entity5_ user _/entity5_ tells them , they need to be more sophisticated at responding to the _entity6_ user _/entity6_ . The issue of _entity7_ system response _/entity7_ to _entity8_ users _/entity8_ has been extensively studied by the _entity9_ natural language generation community _/entity9_ , though rarely in the context of _entity10_ dialog systems _/entity10_ . We show how research in _entity11_ generation _/entity11_ can be adapted to _entity12_ dialog systems _/entity12_ , and how the high cost of hand-crafting _entity13_ knowledge-based generation systems _/entity13_ can be overcome by employing _entity14_ machine learning techniques _/entity14_ .	NONE entity2 entity3
The _entity1_ JAVELIN system _/entity1_ integrates a flexible , _entity2_ planning-based architecture _/entity2_ with a variety of _entity3_ language processing modules _/entity3_ to provide an _entity4_ _P_ open-domain question answering capability _/entity4_ on _entity5_ free text _/entity5_ . The demonstration will focus on how _entity6_ JAVELIN _/entity6_ processes _entity7_ _C_ questions _/entity7_ and retrieves the most likely _entity8_ answer candidates _/entity8_ from the given _entity9_ text corpus _/entity9_ . The operation of the system will be explained in depth through browsing the _entity10_ repository _/entity10_ of _entity11_ data objects _/entity11_ created by the system during each _entity12_ question answering session _/entity12_ .	NONE entity4 entity7
_entity1_ Link detection _/entity1_ has been regarded as a core technology for the _entity2_ Topic Detection and Tracking tasks _/entity2_ of _entity3_ new event detection _/entity3_ . In this paper we formulate _entity4_ story link detection _/entity4_ and _entity5_ _C_ new event detection _/entity5_ as _entity6_ information retrieval task _/entity6_ and hypothesize on the impact of _entity7_ _P_ precision _/entity7_ and _entity8_ recall _/entity8_ on both systems . Motivated by these arguments , we introduce a number of new performance enhancing techniques including _entity9_ part of speech tagging _/entity9_ , new _entity10_ similarity measures _/entity10_ and expanded _entity11_ stop lists _/entity11_ . Experimental results validate our hypothesis .	NONE entity7 entity5
Existing techniques extract _entity1_ term candidates _/entity1_ by looking for _entity2_ internal and contextual information _/entity2_ associated with _entity3_ domain specific terms _/entity3_ . The algorithms always face the dilemma that fewer _entity4_ features _/entity4_ are not enough to distinguish _entity5_ _C_ terms _/entity5_ from _entity6_ non-terms _/entity6_ whereas more _entity7_ features _/entity7_ lead to more conflicts among selected _entity8_ _P_ features _/entity8_ . This paper presents a novel approach for _entity9_ term extraction _/entity9_ based on _entity10_ delimiters _/entity10_ which are much more stable and domain independent . The proposed approach is not as sensitive to _entity11_ term frequency _/entity11_ as that of previous works . This approach has no strict limit or _entity12_ hard rules _/entity12_ and thus they can deal with all kinds of _entity13_ terms _/entity13_ . It also requires no prior _entity14_ domain knowledge _/entity14_ and no additional _entity15_ training _/entity15_ to adapt to new _entity16_ domains _/entity16_ . Consequently , the proposed approach can be applied to different _entity17_ domains _/entity17_ easily and it is especially useful for _entity18_ resource-limited domains _/entity18_ . Evaluations conducted on two different _entity19_ domains _/entity19_ for _entity20_ Chinese term extraction _/entity20_ show significant improvements over existing techniques which verifies its efficiency and domain independent nature . Experiments on _entity21_ new term extraction _/entity21_ indicate that the proposed approach can also serve as an effective tool for _entity22_ domain lexicon expansion _/entity22_ .	NONE entity8 entity5
Two styles of performing _entity1_ inference _/entity1_ in _entity2_ semantic networks _/entity2_ are presented and compared . _entity3_ Path-based inference _/entity3_ allows an _entity4_ arc _/entity4_ or a _entity5_ path of arcs _/entity5_ between two given _entity6_ nodes _/entity6_ to be inferred from the existence of another specified _entity7_ path _/entity7_ between the same two _entity8_ nodes _/entity8_ . _entity9_ Path-based inference rules _/entity9_ may be written using a _entity10_ binary relational calculus notation _/entity10_ . _entity11_ Node-based inference _/entity11_ allows a _entity12_ structure _/entity12_ of _entity13_ nodes _/entity13_ to be inferred from the existence of an instance of a pattern of _entity14_ node structures _/entity14_ . _entity15_ Node-based inference rules _/entity15_ can be constructed in a _entity16_ semantic network _/entity16_ using a variant of a _entity17_ predicate calculus notation _/entity17_ . _entity18_ Path-based inference _/entity18_ is more efficient , while _entity19_ node-based inference _/entity19_ is more general . A method is described of combining the two styles in a single system in order to take advantage of the strengths of each . Applications of _entity20_ _P_ path-based inference rules _/entity20_ to the representation of the _entity21_ extensional equivalence _/entity21_ of _entity22_ intensional concepts _/entity22_ , and to the _entity23_ _C_ explication _/entity23_ of _entity24_ inheritance _/entity24_ in _entity25_ hierarchies _/entity25_ are sketched .	NONE entity20 entity23
_entity1_ We present a novel method for _/entity1_ discovering parallel sentences _entity2_ in _/entity2_ comparable , non-parallel corpora _entity3_ . We train a _/entity3_ maximum entropy classifier _entity4_ that , given a pair of _/entity4_ sentences _entity5_ , can reliably determine whether or not they are _/entity5_ translations _entity6_ of each other . Using this approach , we extract _/entity6_ parallel data _entity7_ from large _/entity7_ Chinese , Arabic , and English non-parallel newspaper corpora _entity8_ . We evaluate the _/entity8_ quality of the extracted data _entity9_ by showing that it improves the performance of a state-of-the-art _/entity9_ statistical machine translation system _entity10_ . We also show that a good-quality _/entity10_ MT system _entity11_ can be built from scratch by starting with a very small _/entity11_ parallel corpus _entity12_ _C_ ( 100,000 _/entity12_ words _entity13_ _P_ ) and exploiting a large _/entity13_ non-parallel corpus _entity14_ . Thus , our method can be applied with great benefit to _/entity14_ language pairs _entity15_ for which only scarce _/entity15_ resources _entity16_ are available . _/entity16_	NONE entity13 entity12
_entity1_ A deterministic parser _/entity1_ is under development which represents a departure from _entity2_ traditional deterministic parsers _/entity2_ in that it combines both _entity3_ symbolic and connectionist components _/entity3_ . The connectionist component is trained either from _entity4_ patterns _/entity4_ derived from the _entity5_ rules _/entity5_ of a _entity6_ deterministic grammar _/entity6_ . The development and evolution of such a _entity7_ hybrid architecture _/entity7_ has lead to a _entity8_ parser _/entity8_ which is superior to any _entity9_ known deterministic parser _/entity9_ . Experiments are described and powerful _entity10_ training techniques _/entity10_ are demonstrated that permit _entity11_ decision-making _/entity11_ by the _entity12_ connectionist component _/entity12_ in the _entity13_ parsing process _/entity13_ . This approach has permitted some simplifications to the _entity14_ rules _/entity14_ of other _entity15_ deterministic parsers _/entity15_ , including the elimination of _entity16_ rule packets _/entity16_ and priorities . Furthermore , _entity17_ _C_ parsing _/entity17_ is performed more robustly and with more tolerance for error . Data are presented which show how a _entity18_ connectionist ( neural ) network _/entity18_ trained with _entity19_ linguistic rules _/entity19_ can parse both _entity20_ _P_ expected ( grammatical ) sentences _/entity20_ as well as some novel ( ungrammatical or lexically ambiguous ) sentences .	NONE entity20 entity17
Reducing _entity1_ _P_ language model ( LM ) size _/entity1_ is a critical issue when applying a _entity2_ LM _/entity2_ to realistic applications which have memory constraints . In this paper , three measures are studied for the purpose of _entity3_ _C_ LM pruning _/entity3_ . They are probability , _entity4_ rank _/entity4_ , and _entity5_ entropy _/entity5_ . We evaluated the performance of the three _entity6_ pruning criteria _/entity6_ in a real application of _entity7_ Chinese text input _/entity7_ in terms of _entity8_ character error rate ( CER ) _/entity8_ . We first present an empirical comparison , showing that _entity9_ rank _/entity9_ performs the best in most cases . We also show that the high-performance of _entity10_ rank _/entity10_ lies in its strong correlation with _entity11_ error rate _/entity11_ . We then present a novel method of combining two criteria in _entity12_ model pruning _/entity12_ . Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same _entity13_ CER _/entity13_ .	NONE entity1 entity3
We introduce a new _entity1_ interactive corpus exploration tool _/entity1_ called _entity2_ InfoMagnets _/entity2_ . _entity3_ _P_ InfoMagnets _/entity3_ aims at making _entity4_ exploratory corpus analysis _/entity4_ accessible to researchers who are not experts in _entity5_ _C_ text mining _/entity5_ . As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between _entity6_ language _/entity6_ and _entity7_ behavioral patterns _/entity7_ in two distinct domains : _entity8_ tutorial dialogue _/entity8_ ( Kumar et al. , submitted ) and _entity9_ on-line communities _/entity9_ ( Arguello et al. , 2006 ) . As an _entity10_ educational tool _/entity10_ , it has been used as part of a unit on _entity11_ protocol analysis _/entity11_ in an _entity12_ Educational Research Methods course _/entity12_ .	NONE entity3 entity5
In the second year of _entity1_ evaluations _/entity1_ of the _entity2_ ARPA HLT Machine Translation ( MT ) Initiative _/entity2_ , methodologies developed and tested in 1992 were applied to the _entity3_ 1993 MT test runs _/entity3_ . The current methodology optimizes the inherently _entity4_ subjective judgments _/entity4_ on _entity5_ translation accuracy and quality _/entity5_ by channeling the _entity6_ judgments _/entity6_ of _entity7_ non-translators _/entity7_ into many _entity8_ data points _/entity8_ which reflect both the comparison of the _entity9_ performance _/entity9_ of the _entity10_ _C_ research MT systems _/entity10_ with _entity11_ production MT systems _/entity11_ and against the _entity12_ _P_ performance _/entity12_ of _entity13_ novice translators _/entity13_ . This paper discusses the three _entity14_ evaluation methods _/entity14_ used in the _entity15_ 1993 evaluation _/entity15_ , the results of the evaluations , and preliminary characterizations of the _entity16_ Winter 1994 evaluation _/entity16_ , now underway . The efforts under discussion focus on measuring the progress of _entity17_ core MT technology _/entity17_ and increasing the sensitivity and _entity18_ portability _/entity18_ of _entity19_ MT evaluation methodology _/entity19_ .	NONE entity12 entity10
_entity1_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ language-independent system _/entity2_ for _entity3_ _P_ automatic discovery of text _/entity3_ in _entity4_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ _C_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ words _/entity9_ per _entity10_ language _/entity10_ .	NONE entity3 entity6
This paper proposes an approach to _entity1_ full parsing _/entity1_ suitable for _entity2_ Information Extraction _/entity2_ from _entity3_ texts _/entity3_ . Sequences of cascades of _entity4_ rules _/entity4_ deterministically analyze the _entity5_ _C_ text _/entity5_ , building _entity6_ unambiguous structures _/entity6_ . Initially basic _entity7_ _P_ chunks _/entity7_ are analyzed ; then _entity8_ argumental relations _/entity8_ are recognized ; finally _entity9_ modifier attachment _/entity9_ is performed and the _entity10_ global parse tree _/entity10_ is built . The approach was proven to work for three _entity11_ languages _/entity11_ and different _entity12_ domains _/entity12_ . It was implemented in the _entity13_ IE module _/entity13_ of _entity14_ FACILE , a EU project for multilingual text classification and IE _/entity14_ .	NONE entity7 entity5
We describe a set of _entity1_ supervised machine learning _/entity1_ experiments centering on the construction of _entity2_ statistical models _/entity2_ of _entity3_ WH-questions _/entity3_ . These _entity4_ models _/entity4_ , which are built from _entity5_ shallow linguistic features _/entity5_ of _entity6_ _C_ questions _/entity6_ , are employed to predict target variables which represent a _entity7_ user 's informational goals _/entity7_ . We report on different aspects of the _entity8_ predictive performance _/entity8_ of our _entity9_ _P_ models _/entity9_ , including the influence of various _entity10_ training and testing factors _/entity10_ on _entity11_ predictive performance _/entity11_ , and examine the relationships among the target variables .	NONE entity9 entity6
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ _C_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ _P_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity25 entity22
One of the claimed benefits of _entity1_ Tree Adjoining Grammars _/entity1_ is that they have an _entity2_ _C_ extended domain of locality ( EDOL ) _/entity2_ . We consider how this can be exploited to limit the need for _entity3_ _P_ feature structure unification _/entity3_ during _entity4_ parsing _/entity4_ . We compare two wide-coverage _entity5_ lexicalized grammars of English _/entity5_ , _entity6_ LEXSYS _/entity6_ and _entity7_ XTAG _/entity7_ , finding that the two _entity8_ grammars _/entity8_ exploit _entity9_ EDOL _/entity9_ in different ways .	NONE entity3 entity2
_entity1_ Recognition of proper nouns _/entity1_ in _entity2_ Japanese text _/entity2_ has been studied as a part of the more general problem of _entity3_ morphological analysis _/entity3_ in _entity4_ Japanese text processing _/entity4_ ( [ 1 ] [ 2 ] ) . It has also been studied in the framework of _entity5_ Japanese information extraction _/entity5_ ( [ 3 ] ) in recent years . Our approach to the Multi-lingual Evaluation Task ( MET ) for _entity6_ Japanese text _/entity6_ is to consider the given task as a _entity7_ morphological analysis problem _/entity7_ in _entity8_ Japanese _/entity8_ . Our _entity9_ morphological analyzer _/entity9_ has done all the necessary work for the _entity10_ recognition and classification of proper names , numerical and temporal expressions , i.e . Named Entity ( NE ) items _/entity10_ in the _entity11_ Japanese text _/entity11_ . The _entity12_ analyzer _/entity12_ is called `` Amorph '' . Amorph recognizes _entity13_ NE items _/entity13_ in two stages : _entity14_ dictionary lookup _/entity14_ and _entity15_ _P_ rule application _/entity15_ . First , it uses several kinds of _entity16_ _C_ dictionaries _/entity16_ to segment and tag _entity17_ Japanese character strings _/entity17_ . Second , based on the information resulting from the _entity18_ dictionary lookup stage _/entity18_ , a set of _entity19_ rules _/entity19_ is applied to the _entity20_ segmented strings _/entity20_ in order to identify _entity21_ NE items _/entity21_ . When a _entity22_ segment _/entity22_ is found to be an _entity23_ NE item _/entity23_ , this information is added to the _entity24_ segment _/entity24_ and it is used to generate the final output .	NONE entity15 entity16
This paper describes a particular approach to _entity1_ parsing _/entity1_ that utilizes recent advances in _entity2_ unification-based parsing _/entity2_ and in _entity3_ classification-based knowledge representation _/entity3_ . As _entity4_ _P_ unification-based grammatical frameworks _/entity4_ are extended to handle richer descriptions of _entity5_ linguistic information _/entity5_ , they begin to share many of the properties that have been developed in _entity6_ KL-ONE-like knowledge representation systems _/entity6_ . This commonality suggests that some of the _entity7_ _C_ classification-based representation techniques _/entity7_ can be applied to _entity8_ unification-based linguistic descriptions _/entity8_ . This merging supports the integration of _entity9_ semantic and syntactic information _/entity9_ into the same system , simultaneously subject to the same types of processes , in an efficient manner . The result is expected to be more _entity10_ efficient parsing _/entity10_ due to the increased organization of knowledge . The use of a _entity11_ KL-ONE style representation _/entity11_ for _entity12_ parsing _/entity12_ and _entity13_ semantic interpretation _/entity13_ was first explored in the _entity14_ PSI-KLONE system _/entity14_ [ 2 ] , in which _entity15_ parsing _/entity15_ is characterized as an inference process called _entity16_ incremental description refinement _/entity16_ .	NONE entity4 entity7
We directly investigate a subject of much recent debate : do _entity1_ word sense disambigation models _/entity1_ help _entity2_ statistical machine translation _/entity2_ _entity3_ quality _/entity3_ ? We present empirical results casting doubt on this common , but unproved , assumption . Using a state-of-the-art _entity4_ _C_ Chinese word sense disambiguation model _/entity4_ to choose _entity5_ translation candidates _/entity5_ for a typical _entity6_ IBM statistical MT system _/entity6_ , we find that _entity7_ _P_ word sense disambiguation _/entity7_ does not yield significantly better _entity8_ translation quality _/entity8_ than the _entity9_ statistical machine translation system _/entity9_ alone . _entity10_ Error analysis _/entity10_ suggests several key factors behind this surprising finding , including inherent limitations of current _entity11_ statistical MT architectures _/entity11_ .	NONE entity7 entity4
Automatic _entity1_ _P_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ _C_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ outputs _/entity17_ .	NONE entity1 entity4
We present a _entity1_ text mining method _/entity1_ for finding _entity2_ synonymous expressions _/entity2_ based on the _entity3_ distributional hypothesis _/entity3_ in a set of coherent _entity4_ corpora _/entity4_ . This paper proposes a new methodology to improve the _entity5_ accuracy _/entity5_ of a _entity6_ term aggregation system _/entity6_ using each author 's _entity7_ text _/entity7_ as a coherent _entity8_ corpus _/entity8_ . Our approach is based on the idea that one person tends to use one _entity9_ expression _/entity9_ for one _entity10_ meaning _/entity10_ . According to our assumption , most of the _entity11_ words _/entity11_ with _entity12_ similar context features _/entity12_ in each author 's _entity13_ corpus _/entity13_ tend not to be _entity14_ _P_ synonymous expressions _/entity14_ . Our proposed method improves the _entity15_ accuracy _/entity15_ of our _entity16_ _C_ term aggregation system _/entity16_ , showing that our approach is successful .	NONE entity14 entity16
This paper presents a _entity1_ critical discussion _/entity1_ of the various _entity2_ approaches _/entity2_ that have been used in the _entity3_ evaluation of Natural Language systems _/entity3_ . We conclude that previous _entity4_ approaches _/entity4_ have neglected to evaluate _entity5_ systems _/entity5_ in the context of their use , e.g . solving a _entity6_ task _/entity6_ requiring _entity7_ _P_ data retrieval _/entity7_ . This raises questions about the validity of such _entity8_ approaches _/entity8_ . In the second half of the paper , we report a _entity9_ _C_ laboratory study _/entity9_ using the _entity10_ Wizard of Oz technique _/entity10_ to identify _entity11_ NL requirements _/entity11_ for carrying out this _entity12_ task _/entity12_ . We evaluate the demands that _entity13_ task dialogues _/entity13_ collected using this _entity14_ technique _/entity14_ , place upon a _entity15_ prototype Natural Language system _/entity15_ . We identify three important requirements which arose from the _entity16_ task _/entity16_ that we gave our subjects : operators specific to the task of _entity17_ database access _/entity17_ , complex _entity18_ contextual reference _/entity18_ and reference to the _entity19_ structure _/entity19_ of the _entity20_ information source _/entity20_ . We discuss how these might be satisfied by future _entity21_ Natural Language systems _/entity21_ .	NONE entity7 entity9
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ _P_ utterances _/entity6_ are made in relation to one made previously , _entity7_ _C_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity6 entity7
We approximate _entity1_ Arabic 's rich morphology _/entity1_ by a _entity2_ model _/entity2_ that a _entity3_ word _/entity3_ consists of a sequence of _entity4_ morphemes _/entity4_ in the _entity5_ pattern _/entity5_ _entity6_ prefix*-stem-suffix* _/entity6_ ( * denotes zero or more occurrences of a _entity7_ morpheme _/entity7_ ) . Our method is seeded by a small _entity8_ manually segmented Arabic corpus _/entity8_ and uses it to bootstrap an _entity9_ unsupervised algorithm _/entity9_ to build the _entity10_ Arabic word segmenter _/entity10_ from a large _entity11_ unsegmented Arabic corpus _/entity11_ . The algorithm uses a _entity12_ trigram language model _/entity12_ to determine the most probable _entity13_ morpheme sequence _/entity13_ for a given _entity14_ _C_ input _/entity14_ . The _entity15_ language model _/entity15_ is initially estimated from a small _entity16_ _P_ manually segmented corpus _/entity16_ of about 110,000 _entity17_ words _/entity17_ . To improve the _entity18_ segmentation _/entity18_ _entity19_ accuracy _/entity19_ , we use an _entity20_ unsupervised algorithm _/entity20_ for automatically acquiring new _entity21_ stems _/entity21_ from a 155 million _entity22_ word _/entity22_ _entity23_ unsegmented corpus _/entity23_ , and re-estimate the _entity24_ model parameters _/entity24_ with the expanded _entity25_ vocabulary _/entity25_ and _entity26_ training corpus _/entity26_ . The resulting _entity27_ Arabic word segmentation system _/entity27_ achieves around 97 % _entity28_ exact match accuracy _/entity28_ on a _entity29_ test corpus _/entity29_ containing 28,449 _entity30_ word tokens _/entity30_ . We believe this is a state-of-the-art performance and the algorithm can be used for many _entity31_ highly inflected languages _/entity31_ provided that one can create a small _entity32_ manually segmented corpus _/entity32_ of the _entity33_ language _/entity33_ of interest .	NONE entity16 entity14
We present a _entity1_ tool _/entity1_ , called _entity2_ ILIMP _/entity2_ , which takes as input a _entity3_ raw text _/entity3_ in _entity4_ French _/entity4_ and produces as output the same _entity5_ text _/entity5_ in which every occurrence of the _entity6_ pronoun il _/entity6_ is tagged either with tag _entity7_ [ ANA ] _/entity7_ for _entity8_ anaphoric _/entity8_ or _entity9_ [ IMP ] _/entity9_ for _entity10_ _P_ impersonal _/entity10_ or _entity11_ expletive _/entity11_ . This _entity12_ tool _/entity12_ is therefore designed to distinguish between the _entity13_ _C_ anaphoric occurrences of il _/entity13_ , for which an _entity14_ anaphora resolution system _/entity14_ has to look for an antecedent , and the _entity15_ expletive occurrences _/entity15_ of this _entity16_ pronoun _/entity16_ , for which it does not make sense to look for an antecedent . The _entity17_ precision rate _/entity17_ for _entity18_ ILIMP _/entity18_ is 97,5 % . The few _entity19_ errors _/entity19_ are analyzed in detail . Other _entity20_ tasks _/entity20_ using the _entity21_ method _/entity21_ developed for _entity22_ ILIMP _/entity22_ are described briefly , as well as the use of _entity23_ ILIMP _/entity23_ in a modular _entity24_ syntactic analysis system _/entity24_ .	NONE entity10 entity13
Recently , we initiated a project to develop a _entity1_ phonetically-based spoken language understanding system _/entity1_ called _entity2_ _C_ SUMMIT _/entity2_ . In contrast to many of the past efforts that make use of _entity3_ heuristic rules _/entity3_ whose development requires intense _entity4_ knowledge engineering _/entity4_ , our approach attempts to express the _entity5_ _P_ speech knowledge _/entity5_ within a formal framework using well-defined mathematical tools . In our system , _entity6_ features _/entity6_ and _entity7_ decision strategies _/entity7_ are discovered and trained automatically , using a large body of _entity8_ speech data _/entity8_ . This paper describes the system , and documents its current performance .	NONE entity5 entity2
A novel _entity1_ bootstrapping approach _/entity1_ to _entity2_ Named Entity ( NE ) tagging _/entity2_ using _entity3_ concept-based seeds _/entity3_ and _entity4_ successive learners _/entity4_ is presented . This approach only requires a few _entity5_ common noun _/entity5_ or _entity6_ pronoun _/entity6_ _entity7_ seeds _/entity7_ that correspond to the _entity8_ concept _/entity8_ for the targeted _entity9_ NE _/entity9_ , e.g . he/she/man/woman for _entity10_ PERSON NE _/entity10_ . The _entity11_ bootstrapping procedure _/entity11_ is implemented as training two _entity12_ successive learners _/entity12_ . First , _entity13_ decision list _/entity13_ is used to learn the _entity14_ parsing-based NE rules _/entity14_ . Then , a _entity15_ _P_ Hidden Markov Model _/entity15_ is trained on a _entity16_ _C_ corpus _/entity16_ automatically tagged by the first _entity17_ learner _/entity17_ . The resulting _entity18_ NE system _/entity18_ approaches _entity19_ supervised NE _/entity19_ performance for some _entity20_ NE types _/entity20_ .	USAGE entity15 entity16
Automatic _entity1_ evaluation metrics _/entity1_ for _entity2_ Machine Translation ( MT ) systems _/entity2_ , such as _entity3_ BLEU _/entity3_ or _entity4_ NIST _/entity4_ , are now well established . Yet , they are scarcely used for the assessment of _entity5_ language pairs _/entity5_ like _entity6_ English-Chinese _/entity6_ or _entity7_ English-Japanese _/entity7_ , because of the _entity8_ word segmentation problem _/entity8_ . This study establishes the equivalence between the standard use of _entity9_ BLEU _/entity9_ in _entity10_ word n-grams _/entity10_ and its application at the _entity11_ character _/entity11_ level . The use of _entity12_ BLEU _/entity12_ at the _entity13_ character _/entity13_ level eliminates the _entity14_ _C_ word segmentation problem _/entity14_ : it makes it possible to directly compare commercial systems outputting _entity15_ unsegmented texts _/entity15_ with , for instance , _entity16_ statistical MT systems _/entity16_ which usually segment their _entity17_ _P_ outputs _/entity17_ .	NONE entity17 entity14
Currently several _entity1_ grammatical formalisms _/entity1_ converge towards being declarative and towards utilizing _entity2_ context-free phrase-structure grammar _/entity2_ as a backbone , e.g . _entity3_ LFG _/entity3_ and _entity4_ _P_ PATR-II _/entity4_ . Typically the processing of these formalisms is organized within a _entity5_ _C_ chart-parsing framework _/entity5_ . The declarative character of the _entity6_ formalisms _/entity6_ makes it important to decide upon an overall _entity7_ optimal control strategy _/entity7_ on the part of the processor . In particular , this brings the _entity8_ rule-invocation strategy _/entity8_ into critical focus : to gain maximal _entity9_ processing efficiency _/entity9_ , one has to determine the best way of putting the _entity10_ rules _/entity10_ to use . The aim of this paper is to provide a survey and a practical comparison of fundamental _entity11_ rule-invocation strategies _/entity11_ within _entity12_ context-free chart parsing _/entity12_ .	NONE entity4 entity5
We present a new HMM tagger that exploits _entity1_ context _/entity1_ on both sides of a _entity2_ word _/entity2_ to be tagged , and evaluate it in both the _entity3_ _C_ unsupervised and supervised case _/entity3_ . Along the way , we present the first comprehensive comparison of _entity4_ _P_ unsupervised methods for part-of-speech tagging _/entity4_ , noting that published results to date have not been comparable across _entity5_ corpora _/entity5_ or _entity6_ lexicons _/entity6_ . Observing that the _entity7_ quality _/entity7_ of the _entity8_ lexicon _/entity8_ greatly impacts the _entity9_ accuracy _/entity9_ that can be achieved by the _entity10_ algorithms _/entity10_ , we present a method of _entity11_ HMM training _/entity11_ that improves _entity12_ accuracy _/entity12_ when _entity13_ training _/entity13_ of _entity14_ lexical probabilities _/entity14_ is unstable . Finally , we show how this new tagger achieves state-of-the-art results in a _entity15_ supervised , non-training intensive framework _/entity15_ .	NONE entity4 entity3
The paper presents a method for _entity1_ _P_ word sense disambiguation _/entity1_ based on _entity2_ parallel corpora _/entity2_ . The method exploits recent advances in _entity3_ _C_ word alignment _/entity3_ and _entity4_ word clustering _/entity4_ based on _entity5_ automatic extraction _/entity5_ of _entity6_ translation equivalents _/entity6_ and being supported by available aligned _entity7_ wordnets _/entity7_ for the _entity8_ languages _/entity8_ in the _entity9_ corpus _/entity9_ . The _entity10_ wordnets _/entity10_ are aligned to the _entity11_ Princeton Wordnet _/entity11_ , according to the principles established by _entity12_ EuroWordNet _/entity12_ . The evaluation of the _entity13_ WSD system _/entity13_ , implementing the method described herein showed very encouraging results . The same system used in a validation mode , can be used to check and spot _entity14_ alignment errors _/entity14_ in _entity15_ multilingually aligned wordnets _/entity15_ as _entity16_ BalkaNet _/entity16_ and _entity17_ EuroWordNet _/entity17_ .	NONE entity1 entity3
In order to build robust _entity1_ automatic abstracting systems _/entity1_ , there is a need for better _entity2_ training resources _/entity2_ than are currently available . In this paper , we introduce an _entity3_ annotation scheme _/entity3_ for scientific articles which can be used to build such a _entity4_ resource _/entity4_ in a consistent way . The seven categories of the _entity5_ scheme _/entity5_ are based on _entity6_ rhetorical moves _/entity6_ of _entity7_ _P_ argumentation _/entity7_ . Our experimental results show that the _entity8_ _C_ scheme _/entity8_ is stable , reproducible and intuitive to use .	NONE entity7 entity8
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ _C_ machine-readable resources _/entity2_ for the construction of _entity3_ _P_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity3 entity2
_entity1_ Language resource quality _/entity1_ is crucial in _entity2_ NLP _/entity2_ . Many of the resources used are derived from data created by human beings out of an _entity3_ NLP _/entity3_ context , especially regarding _entity4_ MT _/entity4_ and _entity5_ reference translations _/entity5_ . Indeed , _entity6_ automatic evaluations _/entity6_ need _entity7_ high-quality data _/entity7_ that allow the comparison of both _entity8_ automatic and human translations _/entity8_ . The validation of these resources is widely recommended before being used . This paper describes the impact of using _entity9_ _C_ different-quality references _/entity9_ on _entity10_ _P_ evaluation _/entity10_ . Surprisingly enough , similar scores are obtained in many cases regardless of the quality . Thus , the limitations of the _entity11_ automatic metrics _/entity11_ used within _entity12_ MT _/entity12_ are also discussed in this regard .	NONE entity10 entity9
Recent years have seen increasing research on extracting and using temporal information in _entity1_ natural language applications _/entity1_ . However most of the works found in the literature have focused on identifying and understanding _entity2_ temporal expressions _/entity2_ in _entity3_ newswire texts _/entity3_ . In this paper we report our work on anchoring _entity4_ temporal expressions _/entity4_ in a novel _entity5_ genre _/entity5_ , emails . The highly under-specified nature of these _entity6_ expressions _/entity6_ fits well with our _entity7_ constraint-based representation _/entity7_ of time , _entity8_ _C_ Time Calculus for Natural Language ( TCNL ) _/entity8_ . We have developed and evaluated a _entity9_ Temporal Expression Anchoror ( TEA ) _/entity9_ , and the result shows that it performs significantly better than the _entity10_ _P_ baseline _/entity10_ , and compares favorably with some of the closely related work .	NONE entity10 entity8
This paper shows how _entity1_ dictionary word sense definitions _/entity1_ can be analysed by applying a hierarchy of _entity2_ phrasal patterns _/entity2_ . An experimental system embodying this mechanism has been implemented for processing _entity3_ _C_ definitions _/entity3_ from the _entity4_ Longman Dictionary of Contemporary English _/entity4_ . A property of this _entity5_ dictionary _/entity5_ , exploited by the system , is that it uses a _entity6_ _P_ restricted vocabulary _/entity6_ in its _entity7_ word sense definitions _/entity7_ . The structures generated by the experimental system are intended to be used for the _entity8_ classification _/entity8_ of new _entity9_ word senses _/entity9_ in terms of the _entity10_ senses _/entity10_ of _entity11_ words _/entity11_ in the _entity12_ restricted vocabulary _/entity12_ . Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed . The analysis process applies successively more specific _entity13_ phrasal analysis rules _/entity13_ as determined by a hierarchy of _entity14_ patterns _/entity14_ in which less specific _entity15_ patterns _/entity15_ dominate more specific ones . This ensures that reasonable incomplete analyses of the _entity16_ definitions _/entity16_ are produced when more complete analyses are not possible , resulting in a relatively robust _entity17_ analysis mechanism _/entity17_ . Thus the work reported addresses two _entity18_ robustness problems _/entity18_ faced by current experimental _entity19_ natural language processing systems _/entity19_ : coping with an incomplete _entity20_ lexicon _/entity20_ and with incomplete _entity21_ knowledge _/entity21_ of _entity22_ phrasal constructions _/entity22_ .	NONE entity6 entity3
We describe an implementation of data-driven selection of emphatic facial displays for an _entity1_ embodied conversational agent _/entity1_ in a _entity2_ dialogue system _/entity2_ . A _entity3_ corpus of sentences _/entity3_ in the domain of the _entity4_ _P_ target dialogue system _/entity4_ was recorded , and the facial displays used by the _entity5_ speaker _/entity5_ were annotated . The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of _entity6_ context _/entity6_ or choosing displays differently within a _entity7_ _C_ context _/entity7_ . The models were evaluated in two ways : by _entity8_ cross-validation _/entity8_ against the _entity9_ corpus _/entity9_ , and by asking users to rate the output . The predictions of the _entity10_ cross-validation _/entity10_ study differed from the actual user ratings . While the _entity11_ cross-validation _/entity11_ gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . This preference was especially strong among the female subjects .	NONE entity4 entity7
Empirical experience and observations have shown us when powerful and highly tunable _entity1_ classifiers _/entity1_ such as _entity2_ maximum entropy classifiers _/entity2_ , _entity3_ boosting _/entity3_ and _entity4_ SVMs _/entity4_ are applied to _entity5_ language processing tasks _/entity5_ , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point . To further improve performance , various _entity6_ error correction mechanisms _/entity6_ have been developed , but in practice , most of them can not be relied on to predictably improve performance on _entity7_ unseen data _/entity7_ ; indeed , depending upon the _entity8_ _C_ test set _/entity8_ , they are as likely to degrade accuracy as to improve it . This problem is especially severe if the _entity9_ _P_ base classifier _/entity9_ has already been finely tuned . In recent work , we introduced _entity10_ N-fold Templated Piped Correction , or NTPC ( `` nitpick '' ) _/entity10_ , an intriguing _entity11_ error corrector _/entity11_ that is designed to work in these extreme operating conditions . Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate _entity12_ base models _/entity12_ . This paper investigates some of the more surprising claims made by _entity13_ NTPC _/entity13_ , and presents experiments supporting an _entity14_ Occam 's Razor argument _/entity14_ that more complex models are damaging or unnecessary in practice .	NONE entity9 entity8
Valiant showed that _entity1_ Boolean matrix multiplication ( BMM ) _/entity1_ can be used for _entity2_ _C_ CFG parsing _/entity2_ . We prove a dual result : _entity3_ CFG parsers _/entity3_ running in _entity4_ _P_ time O ( |G||w|3-e ) _/entity4_ on a _entity5_ grammar G _/entity5_ and a _entity6_ string w _/entity6_ can be used to multiply _entity7_ m x m Boolean matrices _/entity7_ in _entity8_ time O ( m3-e/3 ) _/entity8_ . In the process we also provide a _entity9_ formal definition _/entity9_ of _entity10_ parsing _/entity10_ motivated by an informal notion due to Lang . Our result establishes one of the first limitations on general _entity11_ CFG parsing _/entity11_ : a fast , practical _entity12_ CFG parser _/entity12_ would yield a fast , practical _entity13_ BMM algorithm _/entity13_ , which is not believed to exist .	NONE entity4 entity2
This article considers approaches which rerank the output of an existing _entity1_ probabilistic parser _/entity1_ . The base _entity2_ parser _/entity2_ produces a set of _entity3_ candidate parses _/entity3_ for each input _entity4_ sentence _/entity4_ , with associated _entity5_ probabilities _/entity5_ that define an initial _entity6_ ranking _/entity6_ of these _entity7_ parses _/entity7_ . A second _entity8_ model _/entity8_ then attempts to improve upon this initial _entity9_ ranking _/entity9_ , using additional _entity10_ features _/entity10_ of the _entity11_ tree _/entity11_ as evidence . The strength of our approach is that it allows a _entity12_ tree _/entity12_ to be represented as an arbitrary set of _entity13_ features _/entity13_ , without concerns about how these _entity14_ features _/entity14_ interact or overlap and without the need to define a _entity15_ derivation _/entity15_ or a _entity16_ generative model _/entity16_ which takes these _entity17_ features _/entity17_ into account . We introduce a new method for the _entity18_ reranking task _/entity18_ , based on the _entity19_ boosting approach _/entity19_ to _entity20_ ranking problems _/entity20_ described in Freund et al . ( 1998 ) . We apply the _entity21_ _P_ boosting method _/entity21_ to _entity22_ parsing _/entity22_ the _entity23_ _C_ Wall Street Journal treebank _/entity23_ . The method combined the _entity24_ log-likelihood _/entity24_ under a _entity25_ baseline model _/entity25_ ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 _entity26_ features _/entity26_ over _entity27_ parse trees _/entity27_ that were not included in the original _entity28_ model _/entity28_ . The new _entity29_ model _/entity29_ achieved 89.75 % _entity30_ F-measure _/entity30_ , a 13 % relative decrease in _entity31_ F-measure _/entity31_ error over the _entity32_ baseline model 's score _/entity32_ of 88.2 % . The article also introduces a new algorithm for the _entity33_ boosting approach _/entity33_ which takes advantage of the _entity34_ sparsity of the feature space _/entity34_ in the _entity35_ parsing data _/entity35_ . Experiments show significant efficiency gains for the new algorithm over the obvious _entity36_ implementation _/entity36_ of the _entity37_ boosting approach _/entity37_ . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on _entity38_ feature selection methods _/entity38_ within _entity39_ log-linear ( maximum-entropy ) models _/entity39_ . Although the experiments in this article are on _entity40_ natural language parsing ( NLP ) _/entity40_ , the approach should be applicable to many other _entity41_ NLP problems _/entity41_ which are naturally framed as _entity42_ ranking tasks _/entity42_ , for example , _entity43_ speech recognition _/entity43_ , _entity44_ machine translation _/entity44_ , or _entity45_ natural language generation _/entity45_ .	NONE entity21 entity23
How to obtain _entity1_ hierarchical relations _/entity1_ ( e.g . _entity2_ superordinate -hyponym relation _/entity2_ , _entity3_ synonym relation _/entity3_ ) is one of the most important problems for _entity4_ thesaurus construction _/entity4_ . A pilot system for extracting these _entity5_ _P_ relations _/entity5_ automatically from an ordinary _entity6_ Japanese language dictionary _/entity6_ ( Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form ) is given . The features of the _entity7_ _C_ definition sentences _/entity7_ in the _entity8_ dictionary _/entity8_ , the mechanical extraction of the _entity9_ hierarchical relations _/entity9_ and the estimation of the results are discussed .	NONE entity5 entity7
We present an application of _entity1_ ambiguity packing and stochastic disambiguation techniques _/entity1_ for _entity2_ Lexical-Functional Grammars ( LFG ) _/entity2_ to the domain of _entity3_ sentence condensation _/entity3_ . Our system incorporates a _entity4_ linguistic parser/generator _/entity4_ for _entity5_ LFG _/entity5_ , a _entity6_ transfer component _/entity6_ for _entity7_ parse reduction _/entity7_ operating on _entity8_ packed parse forests _/entity8_ , and a _entity9_ maximum-entropy model _/entity9_ for _entity10_ stochastic output selection _/entity10_ . Furthermore , we propose the use of standard _entity11_ parser evaluation methods _/entity11_ for automatically evaluating the _entity12_ summarization _/entity12_ quality of _entity13_ _P_ sentence condensation systems _/entity13_ . An _entity14_ experimental evaluation _/entity14_ of _entity15_ _C_ summarization _/entity15_ quality shows a close correlation between the _entity16_ automatic parse-based evaluation _/entity16_ and a _entity17_ manual evaluation _/entity17_ of generated _entity18_ strings _/entity18_ . Overall _entity19_ summarization _/entity19_ quality of the proposed system is state-of-the-art , with guaranteed _entity20_ grammaticality _/entity20_ of the _entity21_ system output _/entity21_ due to the use of a _entity22_ constraint-based parser/generator _/entity22_ .	NONE entity13 entity15
The _entity1_ verb forms _/entity1_ are often claimed to convey two kinds of _entity2_ information _/entity2_ : 1. whether the _entity3_ event _/entity3_ described in a _entity4_ sentence _/entity4_ is _entity5_ present _/entity5_ , _entity6_ past _/entity6_ or _entity7_ _P_ future _/entity7_ ( = _entity8_ deictic information _/entity8_ ) 2. whether the _entity9_ _C_ event _/entity9_ described in a _entity10_ sentence _/entity10_ is presented as completed , going on , just starting or being finished ( = _entity11_ aspectual information _/entity11_ ) . It will be demonstrated in this paper that one has to add a third component to the analysis of _entity12_ verb form meanings _/entity12_ , namely whether or not they express _entity13_ habituality _/entity13_ . The framework of the analysis is _entity14_ model-theoretic semantics _/entity14_ .	NONE entity7 entity9
We describe a _entity1_ generative probabilistic model of natural language _/entity1_ , which we call _entity2_ HBG _/entity2_ , that takes advantage of detailed _entity3_ linguistic information _/entity3_ to resolve _entity4_ ambiguity _/entity4_ . _entity5_ HBG _/entity5_ incorporates _entity6_ lexical , syntactic , semantic , and structural information _/entity6_ from the _entity7_ parse tree _/entity7_ into the _entity8_ _C_ disambiguation process _/entity8_ in a novel way . We use a _entity9_ corpus of bracketed sentences _/entity9_ , called a _entity10_ _P_ Treebank _/entity10_ , in combination with _entity11_ decision tree building _/entity11_ to tease out the relevant aspects of a _entity12_ parse tree _/entity12_ that will determine the correct _entity13_ parse _/entity13_ of a _entity14_ sentence _/entity14_ . This stands in contrast to the usual approach of further _entity15_ grammar _/entity15_ tailoring via the usual _entity16_ linguistic introspection _/entity16_ in the hope of generating the correct _entity17_ parse _/entity17_ . In _entity18_ head-to-head tests _/entity18_ against one of the best existing robust _entity19_ probabilistic parsing models _/entity19_ , which we call _entity20_ P-CFG _/entity20_ , the _entity21_ HBG model _/entity21_ significantly outperforms _entity22_ P-CFG _/entity22_ , increasing the _entity23_ parsing accuracy _/entity23_ rate from 60 % to 75 % , a 37 % reduction in error .	NONE entity10 entity8
This paper presents a novel _entity1_ ensemble learning approach _/entity1_ to resolving _entity2_ German pronouns _/entity2_ . _entity3_ Boosting _/entity3_ , the method in question , combines the moderately accurate _entity4_ _P_ hypotheses _/entity4_ of several _entity5_ classifiers _/entity5_ to form a highly accurate one . Experiments show that this approach is superior to a single _entity6_ decision-tree classifier _/entity6_ . Furthermore , we present a _entity7_ _C_ standalone system _/entity7_ that resolves _entity8_ pronouns _/entity8_ in _entity9_ unannotated text _/entity9_ by using a fully automatic sequence of _entity10_ preprocessing modules _/entity10_ that mimics the _entity11_ manual annotation process _/entity11_ . Although the system performs well within a limited _entity12_ textual domain _/entity12_ , further research is needed to make it effective for _entity13_ open-domain question answering _/entity13_ and _entity14_ text summarisation _/entity14_ .	NONE entity4 entity7
This article introduces a _entity1_ bidirectional grammar generation system _/entity1_ called _entity2_ feature structure-directed generation _/entity2_ , developed for a _entity3_ dialogue translation system _/entity3_ . The system utilizes _entity4_ typed feature structures _/entity4_ to control the _entity5_ top-down derivation _/entity5_ in a declarative way . This _entity6_ generation system _/entity6_ also uses _entity7_ disjunctive feature structures _/entity7_ to reduce the number of copies of the _entity8_ derivation tree _/entity8_ . The _entity9_ _C_ grammar _/entity9_ for this _entity10_ _P_ generator _/entity10_ is designed to properly generate the _entity11_ speaker 's intention _/entity11_ in a _entity12_ telephone dialogue _/entity12_ .	NONE entity10 entity9
_entity1_ STRAND _/entity1_ ( Resnik , 1998 ) is a _entity2_ _C_ language-independent system _/entity2_ for _entity3_ automatic discovery of text _/entity3_ in _entity4_ _P_ parallel translation _/entity4_ on the World Wide Web . This paper extends the preliminary _entity5_ STRAND _/entity5_ results by adding _entity6_ automatic language identification _/entity6_ , scaling up by orders of magnitude , and formally evaluating performance . The most recent end-product is an _entity7_ automatically acquired parallel corpus _/entity7_ comprising 2491 _entity8_ English-French document pairs _/entity8_ , approximately 1.5 million _entity9_ words _/entity9_ per _entity10_ language _/entity10_ .	NONE entity4 entity2
This paper describes to what extent _entity1_ deep processing _/entity1_ may benefit from _entity2_ _C_ shallow techniques _/entity2_ and it presents a _entity3_ NLP system _/entity3_ which integrates a _entity4_ linguistic PoS tagger and chunker _/entity4_ as a preprocessing module of a _entity5_ _P_ broad coverage unification based grammar of Spanish _/entity5_ . Experiments show that the _entity6_ efficiency _/entity6_ of the overall analysis improves significantly and that our system also provides _entity7_ robustness _/entity7_ to the _entity8_ linguistic processing _/entity8_ while maintaining both the _entity9_ accuracy _/entity9_ and the _entity10_ precision _/entity10_ of the _entity11_ grammar _/entity11_ .	NONE entity5 entity2
_entity1_ Unification _/entity1_ is often the appropriate method for expressing _entity2_ relations _/entity2_ between _entity3_ _C_ representations _/entity3_ in the form of _entity4_ feature structures _/entity4_ ; however , there are circumstances in which a different approach is desirable . A _entity5_ declarative formalism _/entity5_ is presented which permits direct _entity6_ _P_ mappings _/entity6_ of one _entity7_ feature structure _/entity7_ into another , and illustrative examples are given of its application to areas of current interest .	NONE entity6 entity3
While _entity1_ sentence extraction _/entity1_ as an approach to _entity2_ summarization _/entity2_ has been shown to work in _entity3_ documents _/entity3_ of certain _entity4_ genres _/entity4_ , because of the conversational nature of _entity5_ email communication _/entity5_ where _entity6_ utterances _/entity6_ are made in relation to one made previously , _entity7_ sentence extraction _/entity7_ may not capture the necessary _entity8_ segments _/entity8_ of _entity9_ dialogue _/entity9_ that would make a _entity10_ _P_ summary _/entity10_ coherent . In this paper , we present our work on the detection of _entity11_ question-answer pairs _/entity11_ in an _entity12_ _C_ email conversation _/entity12_ for the task of _entity13_ email summarization _/entity13_ . We show that various _entity14_ features _/entity14_ based on the structure of email-threads can be used to improve upon _entity15_ lexical similarity _/entity15_ of _entity16_ discourse segments _/entity16_ for _entity17_ question-answer pairing _/entity17_ .	NONE entity10 entity12
The _entity1_ TIPSTER Architecture _/entity1_ has been designed to enable a variety of different _entity2_ text applications _/entity2_ to use a set of _entity3_ common text processing modules _/entity3_ . Since _entity4_ user interfaces _/entity4_ work best when customized for _entity5_ particular applications _/entity5_ , it is appropriator that no particular _entity6_ user interface styles or conventions _/entity6_ are described in the _entity7_ TIPSTER Architecture specification _/entity7_ . However , the _entity8_ Computing Research Laboratory ( CRL ) _/entity8_ has constructed several _entity9_ TIPSTER applications _/entity9_ that use a common set of configurable _entity10_ Graphical User Interface ( GUI ) functions _/entity10_ . These _entity11_ GUIs _/entity11_ were constructed using _entity12_ CRL 's TIPSTER User Interface Toolkit ( TUIT ) _/entity12_ . _entity13_ TUIT _/entity13_ is a _entity14_ software library _/entity14_ that can be used to construct _entity15_ multilingual TIPSTER user interfaces _/entity15_ for a set of common user tasks . _entity16_ _P_ CRL _/entity16_ developed _entity17_ TUIT _/entity17_ to support their work to integrate _entity18_ _C_ TIPSTER modules _/entity18_ for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects . This paper briefly describes _entity19_ TUIT _/entity19_ and its capabilities .	NONE entity16 entity18
_entity1_ Languages _/entity1_ differ in the _entity2_ concepts _/entity2_ and _entity3_ real-world entities _/entity3_ for which they have _entity4_ words _/entity4_ and _entity5_ grammatical constructs _/entity5_ . Therefore _entity6_ translation _/entity6_ must sometimes be a matter of approximating the _entity7_ meaning _/entity7_ of a _entity8_ source language text _/entity8_ rather than finding an exact counterpart in the _entity9_ target language _/entity9_ . We propose a _entity10_ translation framework _/entity10_ based on _entity11_ Situation Theory _/entity11_ . The basic ingredients are an _entity12_ information lattice _/entity12_ , a _entity13_ representation scheme _/entity13_ for _entity14_ _C_ utterances _/entity14_ embedded in _entity15_ contexts _/entity15_ , and a _entity16_ mismatch resolution scheme _/entity16_ defined in terms of _entity17_ _P_ information flow _/entity17_ . We motivate our approach with examples of _entity18_ translation _/entity18_ between _entity19_ English _/entity19_ and _entity20_ Japanese _/entity20_ .	NONE entity17 entity14
This paper presents necessary and sufficient conditions for the use of _entity1_ demonstrative expressions _/entity1_ in _entity2_ English _/entity2_ and discusses implications for current _entity3_ discourse processing algorithms _/entity3_ . We examine a broad range of _entity4_ texts _/entity4_ to show how the distribution of _entity5_ _C_ demonstrative forms and functions _/entity5_ is _entity6_ genre dependent _/entity6_ . This research is part of a larger study of _entity7_ anaphoric expressions _/entity7_ , the results of which will be incorporated into a _entity8_ _P_ natural language generation system _/entity8_ .	NONE entity8 entity5
_entity1_ Topical blog post retrieval _/entity1_ is the task of ranking _entity2_ blog posts _/entity2_ with respect to their _entity3_ relevance _/entity3_ for a given _entity4_ topic _/entity4_ . To improve _entity5_ topical blog post retrieval _/entity5_ we incorporate _entity6_ textual credibility indicators _/entity6_ in the _entity7_ _P_ retrieval process _/entity7_ . We consider two groups of _entity8_ _C_ indicators _/entity8_ : post level ( determined using information about individual _entity9_ blog posts _/entity9_ only ) and blog level ( determined using information from the underlying _entity10_ blogs _/entity10_ ) . We describe how to estimate these _entity11_ indicators _/entity11_ and how to integrate them into a _entity12_ retrieval approach _/entity12_ based on _entity13_ language models _/entity13_ . Experiments on the _entity14_ TREC Blog track test set _/entity14_ show that both groups of _entity15_ credibility indicators _/entity15_ significantly improve _entity16_ retrieval effectiveness _/entity16_ ; the best performance is achieved when combining them .	NONE entity7 entity8
A proposal to deal with _entity1_ French tenses _/entity1_ in the framework of _entity2_ Discourse Representation Theory _/entity2_ is presented , as it has been implemented for a fragment at the _entity3_ IMS _/entity3_ . It is based on the _entity4_ theory of tenses _/entity4_ of H. Kamp and Ch . Rohrer . Instead of using _entity5_ operators _/entity5_ to express the _entity6_ meaning _/entity6_ of the _entity7_ tenses _/entity7_ the Reichenbachian point of view is adopted and refined such that the impact of the _entity8_ tenses _/entity8_ with respect to the _entity9_ meaning _/entity9_ of the _entity10_ text _/entity10_ is understood as contribution to the integration of the _entity11_ _C_ events _/entity11_ of a _entity12_ sentence _/entity12_ in the _entity13_ _P_ event structure _/entity13_ of the preceeding _entity14_ text _/entity14_ . Thereby a _entity15_ system of relevant times _/entity15_ provided by the preceeding _entity16_ text _/entity16_ and by the _entity17_ temporal adverbials _/entity17_ of the _entity18_ sentence _/entity18_ being processed is used . This system consists of one or more _entity19_ reference times _/entity19_ and _entity20_ temporal perspective times _/entity20_ , the _entity21_ speech time _/entity21_ and the _entity22_ location time _/entity22_ . The special interest of our proposal is to establish a plausible choice of anchors for the _entity23_ new event _/entity23_ out of the _entity24_ system of relevant times _/entity24_ and to update this _entity25_ system of temporal coordinates _/entity25_ correctly . The problem of choice is largely neglected in the literature . In opposition to the approach of Kamp and Rohrer the exact _entity26_ meaning _/entity26_ of the _entity27_ tenses _/entity27_ is fixed by the _entity28_ resolution component _/entity28_ and not in the process of _entity29_ syntactic analysis _/entity29_ .	NONE entity13 entity11
We present a new _entity1_ part-of-speech tagger _/entity1_ that demonstrates the following ideas : ( i ) explicit use of both preceding and following _entity2_ tag contexts _/entity2_ via a _entity3_ dependency network representation _/entity3_ , ( ii ) broad use of _entity4_ lexical features _/entity4_ , including _entity5_ jointly conditioning on multiple consecutive words _/entity5_ , ( iii ) effective use of _entity6_ priors _/entity6_ in _entity7_ conditional loglinear models _/entity7_ , and ( iv ) fine-grained modeling of _entity8_ unknown word features _/entity8_ . Using these ideas together , the resulting _entity9_ tagger _/entity9_ gives a 97.24 % _entity10_ _P_ accuracy _/entity10_ on the _entity11_ _C_ Penn Treebank WSJ _/entity11_ , an _entity12_ error reduction _/entity12_ of 4.4 % on the best previous single automatically learned _entity13_ tagging _/entity13_ result .	NONE entity10 entity11
The major objective of this program is to develop and demonstrate robust , high performance _entity1_ continuous speech recognition ( CSR ) techniques _/entity1_ focussed on application in _entity2_ Spoken Language Systems ( SLS ) _/entity2_ which will enhance the effectiveness of _entity3_ military and civilian computer-based systems _/entity3_ . A key complementary objective is to define and develop applications of robust _entity4_ speech recognition and understanding systems _/entity4_ , and to help catalyze the transition of _entity5_ _C_ spoken language technology _/entity5_ into _entity6_ _P_ military and civilian systems _/entity6_ , with particular focus on application of robust _entity7_ CSR _/entity7_ to _entity8_ mobile military command and control _/entity8_ . The research effort focusses on developing advanced _entity9_ acoustic modelling _/entity9_ , rapid search , and _entity10_ recognition-time adaptation techniques _/entity10_ for robust _entity11_ large-vocabulary CSR _/entity11_ , and on applying these techniques to the new _entity12_ ARPA large-vocabulary CSR corpora _/entity12_ and to military application tasks .	NONE entity6 entity5
This paper proposes an automatic , essentially _entity1_ domain-independent means of evaluating Spoken Language Systems ( SLS ) _/entity1_ which combines _entity2_ software _/entity2_ we have developed for that purpose ( the `` _entity3_ Comparator _/entity3_ `` ) and a set of _entity4_ specifications _/entity4_ for _entity5_ answer expressions _/entity5_ ( the `` _entity6_ Common Answer Specification _/entity6_ `` , or _entity7_ CAS _/entity7_ ) . The _entity8_ Comparator _/entity8_ checks whether the answer provided by a _entity9_ SLS _/entity9_ accords with a _entity10_ canonical answer _/entity10_ , returning either true or false . The _entity11_ Common Answer Specification _/entity11_ determines the _entity12_ syntax _/entity12_ of _entity13_ answer expressions _/entity13_ , the minimal _entity14_ content _/entity14_ that must be included in them , the _entity15_ data _/entity15_ to be included in and excluded from _entity16_ test corpora _/entity16_ , and the _entity17_ procedures _/entity17_ used by the _entity18_ Comparator _/entity18_ . Though some details of the _entity19_ _P_ CAS _/entity19_ are particular to individual _entity20_ _C_ domains _/entity20_ , the _entity21_ Comparator software _/entity21_ is _entity22_ domain-independent _/entity22_ , as is the _entity23_ CAS approach _/entity23_ .	NONE entity19 entity20
An _entity1_ entity-oriented approach to restricted-domain parsing _/entity1_ is proposed . In this approach , the definitions of the _entity2_ _C_ structure _/entity2_ and _entity3_ _P_ surface representation _/entity3_ of _entity4_ domain entities _/entity4_ are grouped together . Like _entity5_ semantic grammar _/entity5_ , this allows easy exploitation of _entity6_ limited domain semantics _/entity6_ . In addition , it facilitates _entity7_ fragmentary recognition _/entity7_ and the use of _entity8_ multiple parsing strategies _/entity8_ , and so is particularly useful for robust _entity9_ recognition of extra-grammatical input _/entity9_ . Several advantages from the point of view of _entity10_ language definition _/entity10_ are also noted . Representative samples from an _entity11_ entity-oriented language definition _/entity11_ are presented , along with a _entity12_ control structure _/entity12_ for an _entity13_ entity-oriented parser _/entity13_ , some _entity14_ parsing strategies _/entity14_ that use the _entity15_ control structure _/entity15_ , and worked examples of _entity16_ parses _/entity16_ . A _entity17_ parser _/entity17_ incorporating the _entity18_ control structure _/entity18_ and the _entity19_ parsing strategies _/entity19_ is currently under _entity20_ implementation _/entity20_ .	NONE entity3 entity2
We describe a set of experiments to explore _entity1_ statistical techniques _/entity1_ for ranking and selecting the best _entity2_ translations _/entity2_ in a _entity3_ graph _/entity3_ of _entity4_ translation hypotheses _/entity4_ . In a previous paper ( Carl , 2007 ) we have described how the _entity5_ hypotheses graph _/entity5_ is generated through _entity6_ shallow mapping _/entity6_ and _entity7_ permutation rules _/entity7_ . We have given examples of its _entity8_ nodes _/entity8_ consisting of _entity9_ vectors representing morpho-syntactic properties _/entity9_ of _entity10_ words _/entity10_ and _entity11_ phrases _/entity11_ . This paper describes a number of methods for elaborating _entity12_ statistical feature functions _/entity12_ from some of the _entity13_ vector components _/entity13_ . The _entity14_ feature functions _/entity14_ are trained off-line on different types of _entity15_ text _/entity15_ and their _entity16_ log-linear combination _/entity16_ is then used to retrieve the best M _entity17_ translation paths _/entity17_ in the _entity18_ graph _/entity18_ . We compare two _entity19_ language modelling toolkits _/entity19_ , the _entity20_ CMU _/entity20_ and the _entity21_ SRI toolkit _/entity21_ and arrive at three results : 1 ) _entity22_ word-lemma based feature function models _/entity22_ produce better results than _entity23_ token-based models _/entity23_ , 2 ) adding a _entity24_ PoS-tag feature function _/entity24_ to the _entity25_ _C_ word-lemma model _/entity25_ improves the output and 3 ) _entity26_ _P_ weights _/entity26_ for _entity27_ lexical translations _/entity27_ are suitable if the _entity28_ training material _/entity28_ is similar to the _entity29_ texts _/entity29_ to be translated .	NONE entity26 entity25
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ _P_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ _C_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity9 entity11
Most large _entity1_ text-understanding systems _/entity1_ have been designed under the assumption that the input _entity2_ text _/entity2_ will be in reasonably neat form , e.g. , _entity3_ newspaper stories _/entity3_ and other _entity4_ edited texts _/entity4_ . However , a great deal of _entity5_ natural language texts _/entity5_ e.g. , _entity6_ memos _/entity6_ , rough _entity7_ drafts _/entity7_ , _entity8_ conversation transcripts _/entity8_ etc. , have features that differ significantly from _entity9_ neat texts _/entity9_ , posing special problems for readers , such as _entity10_ misspelled words _/entity10_ , _entity11_ missing words _/entity11_ , _entity12_ poor syntactic construction _/entity12_ , _entity13_ missing periods _/entity13_ , etc . Our solution to these problems is to make use of _entity14_ expectations _/entity14_ , based both on knowledge of _entity15_ surface English _/entity15_ and on _entity16_ world knowledge _/entity16_ of the situation being described . These _entity17_ syntactic and semantic expectations _/entity17_ can be used to figure out _entity18_ unknown words _/entity18_ from _entity19_ context _/entity19_ , constrain the possible _entity20_ word-senses _/entity20_ of _entity21_ words with multiple meanings _/entity21_ ( _entity22_ ambiguity _/entity22_ ) , fill in _entity23_ missing words _/entity23_ ( _entity24_ _P_ ellipsis _/entity24_ ) , and resolve _entity25_ referents _/entity25_ ( _entity26_ _C_ anaphora _/entity26_ ) . This method of using _entity27_ expectations _/entity27_ to aid the understanding of _entity28_ scruffy texts _/entity28_ has been incorporated into a working _entity29_ computer program _/entity29_ called _entity30_ NOMAD _/entity30_ , which understands _entity31_ scruffy texts _/entity31_ in the domain of Navy messages .	NONE entity24 entity26
This paper describes the understanding process of the _entity1_ spatial descriptions _/entity1_ in _entity2_ Japanese _/entity2_ . In order to understand the described _entity3_ world _/entity3_ , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space . It is done by an experimental _entity4_ computer program _/entity4_ _entity5_ _P_ SPRINT _/entity5_ , which takes _entity6_ _C_ natural language texts _/entity6_ and produces a _entity7_ model _/entity7_ of the described _entity8_ world _/entity8_ . To reconstruct the _entity9_ model _/entity9_ , the authors extract the _entity10_ qualitative spatial constraints _/entity10_ from the _entity11_ text _/entity11_ , and represent them as the _entity12_ numerical constraints _/entity12_ on the _entity13_ spatial attributes _/entity13_ of the _entity14_ entities _/entity14_ . This makes it possible to express the vagueness of the _entity15_ spatial concepts _/entity15_ and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints . The interpretation reflects the _entity16_ temporary belief _/entity16_ about the _entity17_ world _/entity17_ .	NONE entity5 entity6
The following describes recent work on the _entity1_ Lincoln CSR system _/entity1_ . Some new variations in _entity2_ semiphone modeling _/entity2_ have been tested . A very simple improved _entity3_ duration model _/entity3_ has reduced the _entity4_ error rate _/entity4_ by about 10 % in both _entity5_ _C_ triphone and semiphone systems _/entity5_ . A new _entity6_ training strategy _/entity6_ has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the _entity7_ _P_ recognizer _/entity7_ has been modified to use _entity8_ bigram back-off language models _/entity8_ . The system was then transferred from the _entity9_ RM task _/entity9_ to the _entity10_ ATIS CSR task _/entity10_ and a limited number of development tests performed . Evaluation test results are presented for both the _entity11_ RM and ATIS CSR tasks _/entity11_ .	NONE entity7 entity5
In this paper we present our recent work on harvesting _entity1_ English-Chinese bitexts _/entity1_ of the laws of Hong Kong from the _entity2_ _P_ Web _/entity2_ and aligning them to the _entity3_ _C_ subparagraph _/entity3_ level via utilizing the _entity4_ numbering system _/entity4_ in the _entity5_ legal text hierarchy _/entity5_ . Basic methodology and practical techniques are reported in detail . The resultant _entity6_ bilingual corpus _/entity6_ , 10.4M _entity7_ English words _/entity7_ and 18.3M _entity8_ Chinese characters _/entity8_ , is an authoritative and comprehensive _entity9_ text collection _/entity9_ covering the specific and special domain of HK laws . It is particularly valuable to _entity10_ empirical MT research _/entity10_ . This piece of work has also laid a foundation for exploring and harvesting _entity11_ English-Chinese bitexts _/entity11_ in a larger volume from the _entity12_ Web _/entity12_ .	NONE entity2 entity3
_entity1_ Syntax-based statistical machine translation ( MT ) _/entity1_ aims at applying _entity2_ statistical models _/entity2_ to _entity3_ structured data _/entity3_ . In this paper , we present a _entity4_ syntax-based statistical machine translation system _/entity4_ based on a _entity5_ probabilistic synchronous dependency insertion grammar _/entity5_ . _entity6_ Synchronous dependency insertion grammars _/entity6_ are a version of _entity7_ synchronous grammars _/entity7_ defined on _entity8_ dependency trees _/entity8_ . We first introduce our approach to inducing such a _entity9_ grammar _/entity9_ from _entity10_ parallel corpora _/entity10_ . Second , we describe the _entity11_ graphical model _/entity11_ for the _entity12_ machine translation task _/entity12_ , which can also be viewed as a _entity13_ stochastic tree-to-tree transducer _/entity13_ . We introduce a _entity14_ polynomial time decoding algorithm _/entity14_ for the _entity15_ model _/entity15_ . We evaluate the outputs of our _entity16_ _P_ MT system _/entity16_ using the _entity17_ NIST and Bleu automatic MT evaluation software _/entity17_ . The result shows that our system outperforms the _entity18_ _C_ baseline system _/entity18_ based on the _entity19_ IBM models _/entity19_ in both _entity20_ translation speed and quality _/entity20_ .	NONE entity16 entity18
The paper outlines a _entity1_ computational theory _/entity1_ of _entity2_ human plausible reasoning _/entity2_ constructed from analysis of people 's answers to everyday questions . Like _entity3_ _C_ logic _/entity3_ , the _entity4_ theory _/entity4_ is expressed in a _entity5_ _P_ content-independent formalism _/entity5_ . Unlike _entity6_ logic _/entity6_ , the _entity7_ theory _/entity7_ specifies how different information in _entity8_ memory _/entity8_ affects the certainty of the conclusions drawn . The _entity9_ theory _/entity9_ consists of a _entity10_ dimensionalized space _/entity10_ of different _entity11_ inference types _/entity11_ and their _entity12_ certainty conditions _/entity12_ , including a variety of _entity13_ meta-inference types _/entity13_ where the _entity14_ inference _/entity14_ depends on the person 's knowledge about his own knowledge . The protocols from people 's answers to questions are analyzed in terms of the different _entity15_ inference types _/entity15_ . The paper also discusses how _entity16_ memory _/entity16_ is structured in multiple ways to support the different _entity17_ inference types _/entity17_ , and how the information found in _entity18_ memory _/entity18_ determines which _entity19_ inference types _/entity19_ are triggered .	NONE entity5 entity3
This paper introduces a _entity1_ _P_ system for categorizing unknown words _/entity1_ . The _entity2_ _C_ system _/entity2_ is based on a _entity3_ multi-component architecture _/entity3_ where each _entity4_ component _/entity4_ is responsible for identifying one class of _entity5_ unknown words _/entity5_ . The focus of this paper is the _entity6_ components _/entity6_ that identify _entity7_ names _/entity7_ and _entity8_ spelling errors _/entity8_ . Each _entity9_ component _/entity9_ uses a _entity10_ decision tree architecture _/entity10_ to combine multiple types of _entity11_ evidence _/entity11_ about the _entity12_ unknown word _/entity12_ . The _entity13_ system _/entity13_ is evaluated using data from _entity14_ live closed captions _/entity14_ - a genre replete with a wide variety of _entity15_ unknown words _/entity15_ .	NONE entity1 entity2
We describe a novel approach to _entity1_ statistical machine translation _/entity1_ that combines _entity2_ syntactic information _/entity2_ in the _entity3_ source language _/entity3_ with recent advances in _entity4_ phrasal translation _/entity4_ . This method requires a _entity5_ source-language _/entity5_ _entity6_ dependency parser _/entity6_ , _entity7_ target language _/entity7_ _entity8_ word segmentation _/entity8_ and an _entity9_ _C_ unsupervised word alignment component _/entity9_ . We align a _entity10_ _P_ parallel corpus _/entity10_ , project the _entity11_ source dependency parse _/entity11_ onto the target _entity12_ sentence _/entity12_ , extract _entity13_ dependency treelet translation pairs _/entity13_ , and train a _entity14_ tree-based ordering model _/entity14_ . We describe an efficient _entity15_ decoder _/entity15_ and show that using these _entity16_ tree-based models _/entity16_ in combination with conventional _entity17_ SMT models _/entity17_ provides a promising approach that incorporates the power of _entity18_ phrasal SMT _/entity18_ with the linguistic generality available in a _entity19_ parser _/entity19_ .	NONE entity10 entity9
We present an implemented _entity1_ compilation algorithm _/entity1_ that translates _entity2_ HPSG _/entity2_ into _entity3_ lexicalized feature-based TAG _/entity3_ , relating concepts of the two _entity4_ theories _/entity4_ . While _entity5_ HPSG _/entity5_ has a more elaborated _entity6_ principle-based theory _/entity6_ of possible _entity7_ _C_ phrase structures _/entity7_ , _entity8_ TAG _/entity8_ provides the means to represent _entity9_ lexicalized structures _/entity9_ more explicitly . Our objectives are met by giving clear definitions that determine the _entity10_ _P_ projection of structures _/entity10_ from the _entity11_ lexicon _/entity11_ , and identify _entity12_ maximal projections _/entity12_ , _entity13_ auxiliary trees _/entity13_ and _entity14_ foot nodes _/entity14_ .	NONE entity10 entity7
This paper describes methods for relating ( threading ) multiple newspaper articles , and for visualizing various characteristics of them by using a _entity1_ directed graph _/entity1_ . A set of articles is represented by a set of _entity2_ word vectors _/entity2_ , and the _entity3_ similarity _/entity3_ between the _entity4_ vectors _/entity4_ is then calculated . The _entity5_ graph _/entity5_ is constructed from the _entity6_ similarity matrix _/entity6_ . By applying some _entity7_ constraints _/entity7_ on the chronological ordering of articles , an efficient _entity8_ threading algorithm _/entity8_ that runs in _entity9_ 0 ( n ) time _/entity9_ ( where n is the number of articles ) is obtained . The constructed _entity10_ graph _/entity10_ is visualized with _entity11_ words _/entity11_ that represent the _entity12_ _C_ topics _/entity12_ of the _entity13_ _P_ threads _/entity13_ , and _entity14_ words _/entity14_ that represent new _entity15_ information _/entity15_ in each article . The _entity16_ threading technique _/entity16_ is suitable for Webcasting ( push ) applications . A _entity17_ threading server _/entity17_ determines relationships among articles from various news sources , and creates files containing their _entity18_ threading information _/entity18_ . This information is represented in _entity19_ eXtended Markup Language ( XML ) _/entity19_ , and can be visualized on most Web browsers . The _entity20_ XML-based representation _/entity20_ and a current prototype are described in this paper .	NONE entity13 entity12
At MIT Lincoln Laboratory , we have been developing a _entity1_ Korean-to-English machine translation system _/entity1_ _entity2_ CCLINC ( Common Coalition Language System at Lincoln Laboratory ) _/entity2_ . The _entity3_ CCLINC Korean-to-English translation system _/entity3_ consists of two _entity4_ core modules _/entity4_ , _entity5_ language understanding and generation modules _/entity5_ mediated by a _entity6_ language neutral meaning representation _/entity6_ called a _entity7_ semantic frame _/entity7_ . The key features of the system include : ( i ) Robust efficient _entity8_ parsing _/entity8_ of _entity9_ Korean _/entity9_ ( a _entity10_ verb final language _/entity10_ with _entity11_ overt case markers _/entity11_ , relatively _entity12_ free word order _/entity12_ , and frequent omissions of _entity13_ arguments _/entity13_ ) . ( ii ) High quality _entity14_ translation _/entity14_ via _entity15_ word sense disambiguation _/entity15_ and accurate _entity16_ word order generation _/entity16_ of the _entity17_ target language _/entity17_ . ( iii ) _entity18_ Rapid system development _/entity18_ and porting to new _entity19_ _C_ domains _/entity19_ via _entity20_ _P_ knowledge-based automated acquisition of grammars _/entity20_ . Having been trained on _entity21_ Korean newspaper articles _/entity21_ on missiles and chemical biological warfare , the system produces the _entity22_ translation output _/entity22_ sufficient for content understanding of the _entity23_ original document _/entity23_ .	NONE entity20 entity19
When people use _entity1_ natural language _/entity1_ in natural settings , they often use it ungrammatically , missing out or repeating words , breaking-off and restarting , speaking in fragments , etc.. Their _entity2_ human listeners _/entity2_ are usually able to cope with these deviations with little difficulty . If a _entity3_ _P_ computer system _/entity3_ wishes to accept _entity4_ natural language input _/entity4_ from its _entity5_ users _/entity5_ on a routine basis , it must display a similar indifference . In this paper , we outline a set of _entity6_ _C_ parsing flexibilities _/entity6_ that such a system should provide . We go , on to describe _entity7_ FlexP _/entity7_ , a _entity8_ bottom-up pattern-matching parser _/entity8_ that we have designed and implemented to provide these flexibilities for _entity9_ restricted natural language _/entity9_ input to a limited-domain computer system .	NONE entity3 entity6
We examine the relationship between the two _entity1_ _C_ grammatical formalisms _/entity1_ : _entity2_ Tree Adjoining Grammars _/entity2_ and _entity3_ Head Grammars _/entity3_ . We briefly investigate the weak _entity4_ _P_ equivalence _/entity4_ of the two _entity5_ formalisms _/entity5_ . We then turn to a discussion comparing the _entity6_ linguistic expressiveness _/entity6_ of the two _entity7_ formalisms _/entity7_ .	NONE entity4 entity1
In the past the evaluation of _entity1_ machine translation systems _/entity1_ has focused on single system evaluations because there were only few systems available . But now there are several commercial systems for the same _entity2_ language pair _/entity2_ . This requires new methods of comparative evaluation . In the paper we propose a _entity3_ _C_ black-box method _/entity3_ for comparing the _entity4_ lexical coverage _/entity4_ of _entity5_ MT systems _/entity5_ . The method is based on lists of _entity6_ _P_ words _/entity6_ from different _entity7_ frequency classes _/entity7_ . It is shown how these _entity8_ word lists _/entity8_ can be compiled and used for testing . We also present the results of using our method on 6 _entity9_ MT systems _/entity9_ that translate between _entity10_ English _/entity10_ and _entity11_ German _/entity11_ .	NONE entity6 entity3
This paper proposes to use a _entity1_ convolution kernel _/entity1_ over _entity2_ parse trees _/entity2_ to model _entity3_ syntactic structure information _/entity3_ for _entity4_ relation extraction _/entity4_ . Our study reveals that the _entity5_ syntactic structure features _/entity5_ embedded in a _entity6_ _C_ parse tree _/entity6_ are very effective for _entity7_ relation extraction _/entity7_ and these features can be well captured by the _entity8_ convolution tree kernel _/entity8_ . Evaluation on the _entity9_ _P_ ACE 2003 corpus _/entity9_ shows that the _entity10_ convolution kernel _/entity10_ over _entity11_ parse trees _/entity11_ can achieve comparable performance with the previous best-reported feature-based methods on the 24 _entity12_ ACE relation subtypes _/entity12_ . It also shows that our method significantly outperforms the previous two _entity13_ dependency tree kernels _/entity13_ on the 5 _entity14_ ACE relation major types _/entity14_ .	NONE entity9 entity6
In this paper , we use the _entity1_ information redundancy _/entity1_ in _entity2_ multilingual input _/entity2_ to correct errors in _entity3_ machine translation _/entity3_ and thus improve the quality of _entity4_ multilingual summaries _/entity4_ . We consider the case of _entity5_ multi-document summarization _/entity5_ , where the input _entity6_ documents _/entity6_ are in _entity7_ Arabic _/entity7_ , and the output _entity8_ summary _/entity8_ is in _entity9_ English _/entity9_ . Typically , information that makes it to a _entity10_ summary _/entity10_ appears in many different _entity11_ lexical-syntactic forms _/entity11_ in the input _entity12_ documents _/entity12_ . Further , the use of multiple _entity13_ machine translation systems _/entity13_ provides yet more _entity14_ _C_ redundancy _/entity14_ , yielding different ways to realize that _entity15_ _P_ information _/entity15_ in _entity16_ English _/entity16_ . We demonstrate how errors in the _entity17_ machine translations _/entity17_ of the input _entity18_ Arabic documents _/entity18_ can be corrected by identifying and generating from such _entity19_ redundancy _/entity19_ , focusing on _entity20_ noun phrases _/entity20_ .	NONE entity15 entity14
This paper addresses the issue of _entity1_ word-sense ambiguity _/entity1_ in extraction from _entity2_ machine-readable resources _/entity2_ for the construction of _entity3_ large-scale knowledge sources _/entity3_ . We describe two experiments : one which ignored _entity4_ word-sense distinctions _/entity4_ , resulting in 6.3 % accuracy for _entity5_ semantic classification _/entity5_ of _entity6_ verbs _/entity6_ based on ( Levin , 1993 ) ; and one which exploited _entity7_ word-sense distinctions _/entity7_ , resulting in 97.9 % accuracy . These experiments were dual purpose : ( 1 ) to validate the central thesis of the work of ( Levin , 1993 ) , i.e. , that _entity8_ _P_ verb semantics _/entity8_ and _entity9_ syntactic behavior _/entity9_ are predictably related ; ( 2 ) to demonstrate that a 15-fold improvement can be achieved in deriving _entity10_ semantic information _/entity10_ from _entity11_ _C_ syntactic cues _/entity11_ if we first divide the _entity12_ syntactic cues _/entity12_ into distinct groupings that correlate with different _entity13_ word senses _/entity13_ . Finally , we show that we can provide effective acquisition techniques for novel _entity14_ word senses _/entity14_ using a combination of online sources .	NONE entity8 entity11
